{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32bc4fdd",
   "metadata": {
    "papermill": {
     "duration": 0.008006,
     "end_time": "2026-01-10T13:02:29.059696",
     "exception": false,
     "start_time": "2026-01-10T13:02:29.051690",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 1. Overview\n",
    "\n",
    "This is a notebook for training models to submit predictions to the \"Predicting Student Test Scores\" Kaggle competition ([playground-series-s6e1](https://www.kaggle.com/competitions/playground-series-s6e1)).\n",
    "\n",
    "Synthetic data is used for this playground competition, and the objective is to, for each student in the test set, predict a probability for the exam_score variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45109380",
   "metadata": {
    "papermill": {
     "duration": 0.006533,
     "end_time": "2026-01-10T13:02:29.072969",
     "exception": false,
     "start_time": "2026-01-10T13:02:29.066436",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 2. Setup\n",
    "\n",
    "## 2.1 Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee6ac70e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T13:02:29.087627Z",
     "iopub.status.busy": "2026-01-10T13:02:29.087363Z",
     "iopub.status.idle": "2026-01-10T13:02:42.092749Z",
     "shell.execute_reply": "2026-01-10T13:02:42.092151Z"
    },
    "papermill": {
     "duration": 13.015049,
     "end_time": "2026-01-10T13:02:42.094545",
     "exception": false,
     "start_time": "2026-01-10T13:02:29.079496",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/sqlalchemy/orm/query.py:195: SyntaxWarning: \"is not\" with 'tuple' literal. Did you mean \"!=\"?\n",
      "  if entities is not ():\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import copy\n",
    "import lightgbm as lgb\n",
    "import optuna\n",
    "import os\n",
    "import hashlib as hl # for StackingEstimator\n",
    "import inspect # for StackingEstimator\n",
    "import random\n",
    "import warnings\n",
    "from catboost import CatBoostRegressor\n",
    "from enum import Enum\n",
    "from pathlib import Path # for StackingPredictionsRetriever\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from types import FunctionType\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "pd.set_option('display.max_colwidth', None) # Display full column content\n",
    "pd.set_option('display.max_rows', None) # Display all rows\n",
    "pd.set_option('display.width', 1000) # Set larger display width"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1aea09c",
   "metadata": {
    "papermill": {
     "duration": 0.006822,
     "end_time": "2026-01-10T13:02:42.109369",
     "exception": false,
     "start_time": "2026-01-10T13:02:42.102547",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2.2 Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7299130",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T13:02:42.124624Z",
     "iopub.status.busy": "2026-01-10T13:02:42.123962Z",
     "iopub.status.idle": "2026-01-10T13:02:42.190693Z",
     "shell.execute_reply": "2026-01-10T13:02:42.189878Z"
    },
    "papermill": {
     "duration": 0.076465,
     "end_time": "2026-01-10T13:02:42.192462",
     "exception": false,
     "start_time": "2026-01-10T13:02:42.115997",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "RANDOM_SEEDS = [2, 11, 42, 99, 121]\n",
    "random.seed(RANDOM_SEEDS[0])\n",
    "np.random.seed(RANDOM_SEEDS[0])\n",
    "torch.manual_seed(RANDOM_SEEDS[0])\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(RANDOM_SEEDS[0])\n",
    "    torch.cuda.manual_seed_all(RANDOM_SEEDS[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacf7c59",
   "metadata": {
    "papermill": {
     "duration": 0.006586,
     "end_time": "2026-01-10T13:02:42.205959",
     "exception": false,
     "start_time": "2026-01-10T13:02:42.199373",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2.3 Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fba33f4f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T13:02:42.220036Z",
     "iopub.status.busy": "2026-01-10T13:02:42.219789Z",
     "iopub.status.idle": "2026-01-10T13:02:42.223471Z",
     "shell.execute_reply": "2026-01-10T13:02:42.222776Z"
    },
    "papermill": {
     "duration": 0.012423,
     "end_time": "2026-01-10T13:02:42.224822",
     "exception": false,
     "start_time": "2026-01-10T13:02:42.212399",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b8a863",
   "metadata": {
    "papermill": {
     "duration": 0.006485,
     "end_time": "2026-01-10T13:02:42.237865",
     "exception": false,
     "start_time": "2026-01-10T13:02:42.231380",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2.4 DataFrames\n",
    "\n",
    "Read the data provided for the competition into dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07fb6230",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T13:02:42.251814Z",
     "iopub.status.busy": "2026-01-10T13:02:42.251607Z",
     "iopub.status.idle": "2026-01-10T13:02:43.580265Z",
     "shell.execute_reply": "2026-01-10T13:02:43.579460Z"
    },
    "papermill": {
     "duration": 1.337724,
     "end_time": "2026-01-10T13:02:43.582015",
     "exception": false,
     "start_time": "2026-01-10T13:02:42.244291",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "INPUT_DIR = '/kaggle/input'\n",
    "orig_train_data = pd.read_csv(os.path.join(INPUT_DIR, 'playground-series-s6e1/train.csv'))\n",
    "orig_test_data = pd.read_csv(os.path.join(INPUT_DIR, 'playground-series-s6e1/test.csv'))\n",
    "\n",
    "# set index\n",
    "orig_train_data.set_index('id', inplace=True)\n",
    "orig_test_data.set_index('id', inplace=True)\n",
    "\n",
    "# target column\n",
    "target_col = \"exam_score\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a628de7b",
   "metadata": {
    "papermill": {
     "duration": 0.006619,
     "end_time": "2026-01-10T13:02:43.595563",
     "exception": false,
     "start_time": "2026-01-10T13:02:43.588944",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 3. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26acee40",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T13:02:43.610494Z",
     "iopub.status.busy": "2026-01-10T13:02:43.609996Z",
     "iopub.status.idle": "2026-01-10T13:02:43.613337Z",
     "shell.execute_reply": "2026-01-10T13:02:43.612739Z"
    },
    "papermill": {
     "duration": 0.012512,
     "end_time": "2026-01-10T13:02:43.615057",
     "exception": false,
     "start_time": "2026-01-10T13:02:43.602545",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# to skip the generation of plots (e.g. KDE) in this section that take time; set to False to generate the plots \n",
    "SKIP_PLOTS = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8aac46b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T13:02:43.629561Z",
     "iopub.status.busy": "2026-01-10T13:02:43.629342Z",
     "iopub.status.idle": "2026-01-10T13:02:43.756398Z",
     "shell.execute_reply": "2026-01-10T13:02:43.755621Z"
    },
    "papermill": {
     "duration": 0.13616,
     "end_time": "2026-01-10T13:02:43.757837",
     "exception": false,
     "start_time": "2026-01-10T13:02:43.621677",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>study_hours</th>\n",
       "      <th>class_attendance</th>\n",
       "      <th>sleep_hours</th>\n",
       "      <th>exam_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>630000.000000</td>\n",
       "      <td>630000.000000</td>\n",
       "      <td>630000.000000</td>\n",
       "      <td>630000.000000</td>\n",
       "      <td>630000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>20.545821</td>\n",
       "      <td>4.002337</td>\n",
       "      <td>71.987261</td>\n",
       "      <td>7.072758</td>\n",
       "      <td>62.506672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.260238</td>\n",
       "      <td>2.359880</td>\n",
       "      <td>17.430098</td>\n",
       "      <td>1.744811</td>\n",
       "      <td>18.916884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>17.000000</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>40.600000</td>\n",
       "      <td>4.100000</td>\n",
       "      <td>19.599000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>19.000000</td>\n",
       "      <td>1.970000</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>5.600000</td>\n",
       "      <td>48.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>21.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>72.600000</td>\n",
       "      <td>7.100000</td>\n",
       "      <td>62.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>23.000000</td>\n",
       "      <td>6.050000</td>\n",
       "      <td>87.200000</td>\n",
       "      <td>8.600000</td>\n",
       "      <td>76.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>24.000000</td>\n",
       "      <td>7.910000</td>\n",
       "      <td>99.400000</td>\n",
       "      <td>9.900000</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 age    study_hours  class_attendance    sleep_hours     exam_score\n",
       "count  630000.000000  630000.000000     630000.000000  630000.000000  630000.000000\n",
       "mean       20.545821       4.002337         71.987261       7.072758      62.506672\n",
       "std         2.260238       2.359880         17.430098       1.744811      18.916884\n",
       "min        17.000000       0.080000         40.600000       4.100000      19.599000\n",
       "25%        19.000000       1.970000         57.000000       5.600000      48.800000\n",
       "50%        21.000000       4.000000         72.600000       7.100000      62.600000\n",
       "75%        23.000000       6.050000         87.200000       8.600000      76.300000\n",
       "max        24.000000       7.910000         99.400000       9.900000     100.000000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig_train_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e38c05c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T13:02:43.773292Z",
     "iopub.status.busy": "2026-01-10T13:02:43.773011Z",
     "iopub.status.idle": "2026-01-10T13:02:43.817546Z",
     "shell.execute_reply": "2026-01-10T13:02:43.816885Z"
    },
    "papermill": {
     "duration": 0.053727,
     "end_time": "2026-01-10T13:02:43.819159",
     "exception": false,
     "start_time": "2026-01-10T13:02:43.765432",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>study_hours</th>\n",
       "      <th>class_attendance</th>\n",
       "      <th>sleep_hours</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>270000.000000</td>\n",
       "      <td>270000.000000</td>\n",
       "      <td>270000.000000</td>\n",
       "      <td>270000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>20.544137</td>\n",
       "      <td>4.003878</td>\n",
       "      <td>71.982509</td>\n",
       "      <td>7.072070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.260452</td>\n",
       "      <td>2.357741</td>\n",
       "      <td>17.414695</td>\n",
       "      <td>1.745513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>17.000000</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>40.600000</td>\n",
       "      <td>4.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>19.000000</td>\n",
       "      <td>1.980000</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>5.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>21.000000</td>\n",
       "      <td>4.010000</td>\n",
       "      <td>72.600000</td>\n",
       "      <td>7.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>23.000000</td>\n",
       "      <td>6.050000</td>\n",
       "      <td>87.200000</td>\n",
       "      <td>8.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>24.000000</td>\n",
       "      <td>7.910000</td>\n",
       "      <td>99.400000</td>\n",
       "      <td>9.900000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 age    study_hours  class_attendance    sleep_hours\n",
       "count  270000.000000  270000.000000     270000.000000  270000.000000\n",
       "mean       20.544137       4.003878         71.982509       7.072070\n",
       "std         2.260452       2.357741         17.414695       1.745513\n",
       "min        17.000000       0.080000         40.600000       4.100000\n",
       "25%        19.000000       1.980000         57.000000       5.600000\n",
       "50%        21.000000       4.010000         72.600000       7.100000\n",
       "75%        23.000000       6.050000         87.200000       8.600000\n",
       "max        24.000000       7.910000         99.400000       9.900000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig_test_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7cc2c7be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T13:02:43.834832Z",
     "iopub.status.busy": "2026-01-10T13:02:43.834592Z",
     "iopub.status.idle": "2026-01-10T13:02:44.003771Z",
     "shell.execute_reply": "2026-01-10T13:02:44.003157Z"
    },
    "papermill": {
     "duration": 0.178816,
     "end_time": "2026-01-10T13:02:44.005516",
     "exception": false,
     "start_time": "2026-01-10T13:02:43.826700",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "numeric_col_names = orig_train_data.select_dtypes(include='number').columns.to_series()\n",
    "categorical_col_names = orig_train_data.select_dtypes(include='object').columns.to_series()\n",
    "assert numeric_col_names.size + categorical_col_names.size == orig_train_data.shape[1]\n",
    "\n",
    "# drop target column from numeric column names\n",
    "numeric_col_names.drop(target_col, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f05a565e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T13:02:44.020922Z",
     "iopub.status.busy": "2026-01-10T13:02:44.020660Z",
     "iopub.status.idle": "2026-01-10T13:02:44.301403Z",
     "shell.execute_reply": "2026-01-10T13:02:44.300555Z"
    },
    "papermill": {
     "duration": 0.290045,
     "end_time": "2026-01-10T13:02:44.302899",
     "exception": false,
     "start_time": "2026-01-10T13:02:44.012854",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Train data missing values #####\n",
      "age                 0\n",
      "gender              0\n",
      "course              0\n",
      "study_hours         0\n",
      "class_attendance    0\n",
      "internet_access     0\n",
      "sleep_hours         0\n",
      "sleep_quality       0\n",
      "study_method        0\n",
      "facility_rating     0\n",
      "exam_difficulty     0\n",
      "exam_score          0\n",
      "dtype: int64\n",
      "\n",
      "##### Test data missing values #####\n",
      "age                 0\n",
      "gender              0\n",
      "course              0\n",
      "study_hours         0\n",
      "class_attendance    0\n",
      "internet_access     0\n",
      "sleep_hours         0\n",
      "sleep_quality       0\n",
      "study_method        0\n",
      "facility_rating     0\n",
      "exam_difficulty     0\n",
      "dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for (dataset_name, dataset) in [('Train data', orig_train_data), ('Test data', orig_test_data)]:\n",
    "    print(f\"##### {dataset_name} missing values #####\")\n",
    "    print(dataset.isnull().sum())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4468ac2e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T13:02:44.318578Z",
     "iopub.status.busy": "2026-01-10T13:02:44.318308Z",
     "iopub.status.idle": "2026-01-10T13:02:44.744918Z",
     "shell.execute_reply": "2026-01-10T13:02:44.743961Z"
    },
    "papermill": {
     "duration": 0.436362,
     "end_time": "2026-01-10T13:02:44.746592",
     "exception": false,
     "start_time": "2026-01-10T13:02:44.310230",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Train data categorical cols unique values #####\n",
      "\n",
      "[gender]\n",
      " other 33.51%\n",
      "  male 33.43%\n",
      "female 33.07%\n",
      "\n",
      "[course]\n",
      " b.tech 20.83%\n",
      "   b.sc 17.71%\n",
      "  b.com 17.61%\n",
      "    bca 14.08%\n",
      "    bba 12.01%\n",
      "     ba  9.84%\n",
      "diploma  7.92%\n",
      "\n",
      "[internet_access]\n",
      "yes 91.97%\n",
      " no  8.03%\n",
      "\n",
      "[sleep_quality]\n",
      "   poor 33.92%\n",
      "   good 33.82%\n",
      "average 32.26%\n",
      "\n",
      "[study_method]\n",
      "     coaching  20.9%\n",
      "   self-study 20.81%\n",
      "        mixed 19.54%\n",
      "  group study 19.53%\n",
      "online videos 19.22%\n",
      "\n",
      "[facility_rating]\n",
      "medium 33.98%\n",
      "   low 33.71%\n",
      "  high 32.31%\n",
      "\n",
      "[exam_difficulty]\n",
      "moderate 56.19%\n",
      "    easy 28.02%\n",
      "    hard 15.79%\n",
      "\n",
      "##### Test data categorical cols unique values #####\n",
      "\n",
      "[gender]\n",
      "  male 33.59%\n",
      " other 33.37%\n",
      "female 33.04%\n",
      "\n",
      "[course]\n",
      " b.tech 20.91%\n",
      "   b.sc 17.67%\n",
      "  b.com 17.53%\n",
      "    bca 14.11%\n",
      "    bba 11.89%\n",
      "     ba  9.95%\n",
      "diploma  7.94%\n",
      "\n",
      "[internet_access]\n",
      "yes 92.1%\n",
      " no  7.9%\n",
      "\n",
      "[sleep_quality]\n",
      "   good 33.98%\n",
      "   poor 33.96%\n",
      "average 32.06%\n",
      "\n",
      "[study_method]\n",
      "     coaching  21.0%\n",
      "   self-study 20.72%\n",
      "  group study 19.55%\n",
      "        mixed 19.52%\n",
      "online videos 19.22%\n",
      "\n",
      "[facility_rating]\n",
      "medium 34.01%\n",
      "   low 33.82%\n",
      "  high 32.17%\n",
      "\n",
      "[exam_difficulty]\n",
      "moderate 56.17%\n",
      "    easy 28.01%\n",
      "    hard 15.82%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for (dataset_name, dataset) in [('Train data', orig_train_data), ('Test data', orig_test_data)]:\n",
    "    print(f\"##### {dataset_name} categorical cols unique values #####\")\n",
    "    for categorical_col_name in categorical_col_names:\n",
    "        print()\n",
    "        print(f\"[{categorical_col_name}]\")\n",
    "        counts = dataset[categorical_col_name].value_counts(normalize=True).mul(100).round(2).astype(str).add('%')\n",
    "        print(counts.reset_index(name='counts').to_string(index=False, header=False))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "517b93ba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T13:02:44.762491Z",
     "iopub.status.busy": "2026-01-10T13:02:44.762246Z",
     "iopub.status.idle": "2026-01-10T13:02:44.766528Z",
     "shell.execute_reply": "2026-01-10T13:02:44.765959Z"
    },
    "papermill": {
     "duration": 0.013811,
     "end_time": "2026-01-10T13:02:44.767798",
     "exception": false,
     "start_time": "2026-01-10T13:02:44.753987",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# KDE plots of target variable and numerical features (train data)\n",
    "if not SKIP_PLOTS:\n",
    "    plt.figure(figsize=(12, 24))\n",
    "    kdeplot_col_names = [target_col]\n",
    "    kdeplot_col_names.extend(numeric_col_names)\n",
    "    for i, col in enumerate(kdeplot_col_names, start=1):\n",
    "        plt.subplot(10, 2, i)\n",
    "        sns.kdeplot(data=orig_train_data, x=col, fill=True)\n",
    "        plt.tight_layout()\n",
    "        plt.title(f\"KDE plot of {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f1028aff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T13:02:44.783570Z",
     "iopub.status.busy": "2026-01-10T13:02:44.783005Z",
     "iopub.status.idle": "2026-01-10T13:02:44.786988Z",
     "shell.execute_reply": "2026-01-10T13:02:44.786465Z"
    },
    "papermill": {
     "duration": 0.013293,
     "end_time": "2026-01-10T13:02:44.788258",
     "exception": false,
     "start_time": "2026-01-10T13:02:44.774965",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# KDE plots of numerical features (test data)\n",
    "if not SKIP_PLOTS:\n",
    "    plt.figure(figsize=(12, 24))\n",
    "    for i, col in enumerate(numeric_col_names, start=1):\n",
    "        plt.subplot(10, 2, i)\n",
    "        sns.kdeplot(data=orig_test_data, x=col, fill=True)\n",
    "        plt.tight_layout()\n",
    "        plt.title(f\"KDE plot of {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2a7178a5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T13:02:44.803190Z",
     "iopub.status.busy": "2026-01-10T13:02:44.802766Z",
     "iopub.status.idle": "2026-01-10T13:02:44.806804Z",
     "shell.execute_reply": "2026-01-10T13:02:44.806152Z"
    },
    "papermill": {
     "duration": 0.012951,
     "end_time": "2026-01-10T13:02:44.808170",
     "exception": false,
     "start_time": "2026-01-10T13:02:44.795219",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not SKIP_PLOTS:\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    corr_matrix_col_names = [target_col]\n",
    "    corr_matrix_col_names.extend(numeric_col_names)\n",
    "    sns.heatmap(\n",
    "        orig_train_data[corr_matrix_col_names].corr(),\n",
    "        cmap='Reds',\n",
    "        annot=True,\n",
    "        linewidths=2,\n",
    "        fmt='.2f',\n",
    "        vmin=-1,\n",
    "        vmax=1\n",
    "    )\n",
    "    plt.title('Correlation Matrix of Numeric Features and Target Variable', fontsize=18, pad=20)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4409b8",
   "metadata": {
    "papermill": {
     "duration": 0.007182,
     "end_time": "2026-01-10T13:02:44.822384",
     "exception": false,
     "start_time": "2026-01-10T13:02:44.815202",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 4. Data Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9a1ca6f9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T13:02:44.837672Z",
     "iopub.status.busy": "2026-01-10T13:02:44.837242Z",
     "iopub.status.idle": "2026-01-10T13:02:45.022071Z",
     "shell.execute_reply": "2026-01-10T13:02:45.021500Z"
    },
    "papermill": {
     "duration": 0.194241,
     "end_time": "2026-01-10T13:02:45.023820",
     "exception": false,
     "start_time": "2026-01-10T13:02:44.829579",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data = orig_train_data.copy()\n",
    "test_data = orig_test_data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4deac90d",
   "metadata": {
    "papermill": {
     "duration": 0.007014,
     "end_time": "2026-01-10T13:02:45.038497",
     "exception": false,
     "start_time": "2026-01-10T13:02:45.031483",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 4.1 Ordinal Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fcc888b9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T13:02:45.053470Z",
     "iopub.status.busy": "2026-01-10T13:02:45.053238Z",
     "iopub.status.idle": "2026-01-10T13:02:45.237288Z",
     "shell.execute_reply": "2026-01-10T13:02:45.236671Z"
    },
    "papermill": {
     "duration": 0.193584,
     "end_time": "2026-01-10T13:02:45.238986",
     "exception": false,
     "start_time": "2026-01-10T13:02:45.045402",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ordinal_mappings = {\n",
    "    'sleep_quality':   {'poor': 0, 'average': 1, 'good': 2},\n",
    "    'facility_rating': {'low': 0, 'medium': 1, 'high': 2},\n",
    "    'exam_difficulty': {'easy': 0, 'moderate': 1, 'hard': 2},\n",
    "    'internet_access': {'no': 0, 'yes': 1}\n",
    "}\n",
    "for col, mapping in ordinal_mappings.items():\n",
    "    train_data[col] = train_data[col].map(mapping)\n",
    "    test_data[col]  = test_data[col].map(mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b91d44",
   "metadata": {
    "papermill": {
     "duration": 0.006917,
     "end_time": "2026-01-10T13:02:45.253485",
     "exception": false,
     "start_time": "2026-01-10T13:02:45.246568",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 4.2 Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "574ef8aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T13:02:45.268836Z",
     "iopub.status.busy": "2026-01-10T13:02:45.268314Z",
     "iopub.status.idle": "2026-01-10T13:02:45.273726Z",
     "shell.execute_reply": "2026-01-10T13:02:45.273163Z"
    },
    "papermill": {
     "duration": 0.01453,
     "end_time": "2026-01-10T13:02:45.275002",
     "exception": false,
     "start_time": "2026-01-10T13:02:45.260472",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_generated_features(df):\n",
    "    # polynomial\n",
    "    for col in ['study_hours', 'class_attendance']:\n",
    "        df[f'{col}_sq'] = df[col] ** 2\n",
    "    df['sleep_parabola'] = (df['sleep_hours'] - 7.5) ** 2\n",
    "\n",
    "    # log\n",
    "    for col in ['study_hours']:\n",
    "        df[f'{col}_log'] = np.log1p(df[col])\n",
    "\n",
    "    # interactions\n",
    "    df['study_hours_x_class_attendance'] = df['study_hours'] * df['class_attendance'] # total dedication\n",
    "    df['study_hours_x_sleep_hours'] = df['study_hours'] * df['sleep_hours']\n",
    "    df['study_hours_x_sleep_quality'] = df['study_hours'] * df['sleep_quality']\n",
    "    df['study_hours_x_exam_difficulty'] = df['study_hours'] * df['exam_difficulty']\n",
    "    df['study_hours_x_internet_access'] = df['study_hours'] * df['internet_access']\n",
    "    df['sleep_hours_x_sleep_quality'] = df['sleep_hours'] * df['sleep_quality']\n",
    "    df['class_attendance_x_exam_difficulty'] = df['class_attendance'] * df['exam_difficulty']\n",
    "    df['class_attendance_x_facility_rating'] = df['class_attendance'] * df['facility_rating']\n",
    "\n",
    "    # ratios\n",
    "    df['study_hours_per_class_attendance'] = df['study_hours'] / (df['class_attendance'] + 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8ddb73c3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T13:02:45.290350Z",
     "iopub.status.busy": "2026-01-10T13:02:45.289857Z",
     "iopub.status.idle": "2026-01-10T13:02:45.356044Z",
     "shell.execute_reply": "2026-01-10T13:02:45.355448Z"
    },
    "papermill": {
     "duration": 0.075661,
     "end_time": "2026-01-10T13:02:45.357778",
     "exception": false,
     "start_time": "2026-01-10T13:02:45.282117",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# add generated features\n",
    "add_generated_features(train_data)\n",
    "add_generated_features(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3f0d46c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T13:02:45.374011Z",
     "iopub.status.busy": "2026-01-10T13:02:45.373446Z",
     "iopub.status.idle": "2026-01-10T13:02:45.378013Z",
     "shell.execute_reply": "2026-01-10T13:02:45.377364Z"
    },
    "papermill": {
     "duration": 0.014042,
     "end_time": "2026-01-10T13:02:45.379558",
     "exception": false,
     "start_time": "2026-01-10T13:02:45.365516",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['age', 'gender', 'course', 'study_hours', 'class_attendance', 'internet_access', 'sleep_hours', 'sleep_quality', 'study_method', 'facility_rating', 'exam_difficulty', 'exam_score', 'study_hours_sq', 'class_attendance_sq', 'sleep_parabola', 'study_hours_log', 'study_hours_x_class_attendance', 'study_hours_x_sleep_hours', 'study_hours_x_sleep_quality', 'study_hours_x_exam_difficulty', 'study_hours_x_internet_access', 'sleep_hours_x_sleep_quality', 'class_attendance_x_exam_difficulty', 'class_attendance_x_facility_rating', 'study_hours_per_class_attendance'], dtype='object')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "529f5607",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T13:02:45.395289Z",
     "iopub.status.busy": "2026-01-10T13:02:45.394796Z",
     "iopub.status.idle": "2026-01-10T13:02:45.399328Z",
     "shell.execute_reply": "2026-01-10T13:02:45.398647Z"
    },
    "papermill": {
     "duration": 0.013854,
     "end_time": "2026-01-10T13:02:45.400723",
     "exception": false,
     "start_time": "2026-01-10T13:02:45.386869",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['age', 'gender', 'course', 'study_hours', 'class_attendance', 'internet_access', 'sleep_hours', 'sleep_quality', 'study_method', 'facility_rating', 'exam_difficulty', 'study_hours_sq', 'class_attendance_sq', 'sleep_parabola', 'study_hours_log', 'study_hours_x_class_attendance', 'study_hours_x_sleep_hours', 'study_hours_x_sleep_quality', 'study_hours_x_exam_difficulty', 'study_hours_x_internet_access', 'sleep_hours_x_sleep_quality', 'class_attendance_x_exam_difficulty', 'class_attendance_x_facility_rating', 'study_hours_per_class_attendance'], dtype='object')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d896018",
   "metadata": {
    "papermill": {
     "duration": 0.006981,
     "end_time": "2026-01-10T13:02:45.414823",
     "exception": false,
     "start_time": "2026-01-10T13:02:45.407842",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 4.4 Remaining Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2c99b1a9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T13:02:45.430220Z",
     "iopub.status.busy": "2026-01-10T13:02:45.429602Z",
     "iopub.status.idle": "2026-01-10T13:02:45.781580Z",
     "shell.execute_reply": "2026-01-10T13:02:45.780878Z"
    },
    "papermill": {
     "duration": 0.361149,
     "end_time": "2026-01-10T13:02:45.782988",
     "exception": false,
     "start_time": "2026-01-10T13:02:45.421839",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gender', 'course', 'study_method']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_features = train_data.drop(target_col, axis=1).select_dtypes(include='object').columns.to_list()\n",
    "if len(cat_features) > 0:\n",
    "    for col in cat_features:\n",
    "        train_data[col] = train_data[col].astype('category')\n",
    "        test_data[col] = test_data[col].astype('category')\n",
    "cat_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03348edb",
   "metadata": {
    "papermill": {
     "duration": 0.007458,
     "end_time": "2026-01-10T13:02:45.798215",
     "exception": false,
     "start_time": "2026-01-10T13:02:45.790757",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 5. Stacking Initial Setup\n",
    "\n",
    "We'll use stacking, an [ensemble learning](https://en.wikipedia.org/wiki/Ensemble_learning) strategy, to generate the predictions. As we'll need to gather predictions from various base models (a.k.a. level-0 models) to feed as input features to a meta model (a.k.a. level-1 model), in order to streamline the process of experimenting with different combinations of base models, some helper classes will be defined in this section. These classes can also be found [here](https://github.com/chuo-v/machine-learning-utils/blob/master/ensemble-learning/stacking/stacking_predictions_retriever.py) at one of my GitHub repositories used to organize some utilities I implemented for machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "65cab399",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T13:02:45.814536Z",
     "iopub.status.busy": "2026-01-10T13:02:45.814089Z",
     "iopub.status.idle": "2026-01-10T13:02:45.841641Z",
     "shell.execute_reply": "2026-01-10T13:02:45.841084Z"
    },
    "papermill": {
     "duration": 0.037698,
     "end_time": "2026-01-10T13:02:45.843090",
     "exception": false,
     "start_time": "2026-01-10T13:02:45.805392",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class StackingEstimator:\n",
    "    \"\"\"\n",
    "    A class representing an estimator that will be used for stacking, an ensemble learning strategy.\n",
    "\n",
    "    Intended to be used in conjunction with the `StackingPredictionsRetriever` class, which helps\n",
    "    retrieve predictions for multiple instances of `StackingEstimator`; as the predictions are saved\n",
    "    in files, on subsequent requests to retrieve predictions, even as the set of estimators has been\n",
    "    modified, the `StackingPredictionsRetriever` class can determine the predictions of estimators\n",
    "    that are non-stale and available (if any) by using the `get_hash` method of the `StackingEstimator`\n",
    "    class to determine the relevance and staleness of any saved predictions.\n",
    "\n",
    "    Proper usage of this class requires one important condition to be satisfied: the predictions made\n",
    "    using the estimator are determinstic, i.e. they are exactly the same everytime the estimator is\n",
    "    run with the same inputs (`name`, `params_dict`, `feature_names`, `get_predictions`).\n",
    "    \"\"\"\n",
    "    name = \"\"\n",
    "    params_dict = {}\n",
    "    feature_names = []\n",
    "    get_predictions = lambda: None\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        name: str,\n",
    "        feature_names: [str],\n",
    "        params_dict: {},\n",
    "        get_preds: FunctionType\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializes a new instance of `StackingEstimator`.\n",
    "\n",
    "        :param name:\n",
    "            A string representing a name for the estimator. It is used for the column names of\n",
    "            the training and test predictions for each estimator, and is also used as an input\n",
    "            to calculate a hash value for the estimator. It is recommended to use a different\n",
    "            name from the names used for other estimators passed to `StackingPredictionsRetriever`.\n",
    "        :param feature_names:\n",
    "            A list of strings representing the names of the features that will be used for the\n",
    "            estimator. It will be passed as an argument to `get_preds`. Internally, it is only\n",
    "            used as an input to calculate a hash value for the estimator.\n",
    "        :param params_dict:\n",
    "            A dictionary of parameters that will be specified for the estimator. It will be\n",
    "            passed as an argument to `get_preds`. Internally, it is only used as an input\n",
    "            to calculate a hash value for the estimator.\n",
    "        :param get_preds:\n",
    "            A function for getting the predictions for the estimator. It should only take two\n",
    "            arguments: 'params_dict' and 'feature_names', and should return predictions for\n",
    "            the training and test data (in that order) as a tuple of two `pandas.Series`.\n",
    "        \"\"\"\n",
    "        # parameter check\n",
    "        if not isinstance(name, str):\n",
    "            raise ValueError(\"`name` argument should be of type `str`\")\n",
    "        if not isinstance(feature_names, list):\n",
    "            raise ValueError(f\"`feature_names` argument for estimator \\\"{name}\\\" should be of type `list`\")\n",
    "        elif not all(isinstance(feature_name, str) for feature_name in feature_names):\n",
    "            raise ValueError(f\"`feature_names` argument for estimator \\\"{name}\\\" should only contain instances of `str`\")\n",
    "        if not isinstance(params_dict, dict):\n",
    "            raise ValueError(f\"`params_dict` argument for estimator \\\"{name}\\\" should be of type `dict`\")\n",
    "        get_preds_params = inspect.signature(get_preds).parameters.values()\n",
    "        get_preds_param_names = [param.name for param in get_preds_params]\n",
    "        if len(get_preds_param_names) != 2:\n",
    "            raise ValueError(f\"`get_preds` function for estimator \\\"{name}\\\" should take two arguments\")\n",
    "        elif \"params_dict\" not in get_preds_param_names:\n",
    "            raise ValueError(f\"`get_preds` function for estimator \\\"{name}\\\" should take a \\\"params_dict\\\" argument\")\n",
    "        elif \"feature_names\" not in get_preds_param_names:\n",
    "            raise ValueError(f\"`get_preds` function for estimator \\\"{name}\\\" should take a \\\"feature_names\\\" argument\")\n",
    "\n",
    "        self.name = name\n",
    "        self.feature_names = feature_names\n",
    "        self.params_dict = params_dict\n",
    "        self.get_preds = get_preds\n",
    "\n",
    "    def get_hash_value(self):\n",
    "        \"\"\"\n",
    "        Calculates and returns a hash value for the estimator using\n",
    "        `name`, `feature_names` and `params_dict` as inputs.\n",
    "        \"\"\"\n",
    "        feature_names_str = \"_\".join(sorted(self.feature_names))\n",
    "        params_dict_str = \"_\".join(f\"{key}-{value}\" for (key, value) in sorted(self.params_dict.items()))\n",
    "        hash_input_str = \"_\".join([self.name, feature_names_str, params_dict_str])\n",
    "        md5_hash = hl.md5(hash_input_str.encode('utf-8')).hexdigest()\n",
    "        return md5_hash\n",
    "\n",
    "class StackingPredictionsRetriever:\n",
    "    \"\"\"\n",
    "    A class for streamlining stacking (an ensemble learning strategy) that saves predictions\n",
    "    from estimators to file so that when trying out different combinations of (base) estimators,\n",
    "    the predictions that are not stale can be reused, saving the time of having the estimators\n",
    "    make predictions again.\n",
    "\n",
    "    Intended to be used in conjunction with the `StackingEstimator` class. The `hash_value` of\n",
    "    `StackingEstimator` is used to determine the staleness and relevance of the predictions for\n",
    "    an estimator. The implementation for making predictions using an estimator needs to be\n",
    "    provided as a function to `get_preds` for `StackingEstimator`; when predictions need to be\n",
    "    made using an estimator, this class will call `get_preds` for the `StackingEstimator` instance.\n",
    "\n",
    "    Proper usage of this class requires one important condition to be satisfied: the predictions made\n",
    "    using the estimators are determinstic, i.e. they are exactly the same everytime a\n",
    "    `StackingEstimator` instance is run with the same inputs.\n",
    "    \"\"\"\n",
    "    estimators = []\n",
    "    working_dir_path = \"\"\n",
    "    train_preds_filename = \"\"\n",
    "    test_preds_filename = \"\"\n",
    "    preds_save_interval = 0\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        estimators: [StackingEstimator],\n",
    "        working_dir_path: str,\n",
    "        train_preds_filename: str = \"train_preds\",\n",
    "        test_preds_filename: str = \"test_preds\",\n",
    "        preds_save_interval: int = 5,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializes a new instance of `StackingPredictionsRetriever`.\n",
    "\n",
    "        :param estimators:\n",
    "            A list of `StackingEstimator` instances for which the class will retrieve predictions.\n",
    "        :param working_dir_path:\n",
    "            The path for the working directory where the files with predictions will be saved.\n",
    "        :param train_preds_filename:\n",
    "            The name of the file in which predictions for the training set will be stored.\n",
    "        :param test_preds_filename:\n",
    "            The name of the file in which predictions for the test set will be stored.\n",
    "        :param preds_save_interval:\n",
    "            An integer which specifies the interval at which predictions will be saved when\n",
    "            `get_preds` is called, corresponding to the number of estimators whose predictions\n",
    "            have been retrieved since the predictions were previously saved. Any estimators\n",
    "            whose predictions are not stale and therefore were not required to make predictions\n",
    "            again are not included in this number.\n",
    "        \"\"\"\n",
    "        # parameter check\n",
    "        if not isinstance(estimators, list):\n",
    "            raise ValueError(\"`estimators` must be passed as a list\")\n",
    "        if not all(isinstance(e, StackingEstimator) for e in estimators):\n",
    "            raise ValueError(\"`estimators` should only contain instances of `StackingEstimator`\")\n",
    "        if not isinstance(working_dir_path, str):\n",
    "            raise ValueError(\"`working_dir_path` argument should be of type `str`\")\n",
    "        if not isinstance(preds_save_interval, int):\n",
    "            raise ValueError(\"`preds_save_interval` argument should be of type `int`\")\n",
    "\n",
    "        self.estimators = estimators\n",
    "        self.working_dir_path = working_dir_path\n",
    "        self.train_preds_filename = train_preds_filename\n",
    "        self.test_preds_filename = test_preds_filename\n",
    "        self.preds_save_interval = preds_save_interval\n",
    "\n",
    "    def get_train_preds_file_path(self):\n",
    "        \"\"\"\n",
    "        Returns the file path for storing predictions for training data.\n",
    "        \"\"\"\n",
    "        return Path(f\"{self.working_dir_path}/{self.train_preds_filename}.csv\")\n",
    "\n",
    "    def get_test_preds_file_path(self):\n",
    "        \"\"\"\n",
    "        Returns the file path for storing predictions for test data.\n",
    "        \"\"\"\n",
    "        return Path(f\"{self.working_dir_path}/{self.test_preds_filename}.csv\")\n",
    "\n",
    "    def get_current_train_and_test_preds(self):\n",
    "        \"\"\"\n",
    "        Returns the current predictions for training and test data (in that order)\n",
    "        as a tuple of two `pandas.DataFrame`.\n",
    "\n",
    "        The predictions are attempted to be retrieved from the file paths returned\n",
    "        by `get_train_preds_file_path` and `get_test_preds_file_path`; if there are\n",
    "        any issues with doing so (e.g. file does not exist, dataframe is empty),\n",
    "        empty dataframes will be returned instead.\n",
    "        In the case an `pandas.errors.EmptyDataError` exception is raised when\n",
    "        reading from a file, the corresponding file will be removed.\n",
    "        \"\"\"\n",
    "        curr_train_preds = pd.DataFrame()\n",
    "        curr_test_preds = pd.DataFrame()\n",
    "        train_preds_file_path = self.get_train_preds_file_path()\n",
    "        test_preds_file_path = self.get_test_preds_file_path()\n",
    "\n",
    "        if train_preds_file_path.is_file():\n",
    "            try:\n",
    "                curr_train_preds = pd.read_csv(train_preds_file_path)\n",
    "            except pd.errors.EmptyDataError:\n",
    "                train_preds_file_path.unlink()\n",
    "        if test_preds_file_path.is_file():\n",
    "            try:\n",
    "                curr_test_preds = pd.read_csv(test_preds_file_path)\n",
    "            except pd.errors.EmptyDataError:\n",
    "                test_preds_file_path.unlink()\n",
    "\n",
    "        return curr_train_preds, curr_test_preds\n",
    "\n",
    "    def get_preds(self):\n",
    "        \"\"\"\n",
    "        Retrieves predictions from all estimators in `estimators`, storing them in\n",
    "        two files at the file paths specified by `working_dir_path`,\n",
    "        `train_preds_filename` and `test_preds_filename`.\n",
    "\n",
    "        If non-stale (relevant) predictions are found for an estimator, retrieval\n",
    "        of predictions by calling `get_preds` on the estimator will be skipped,\n",
    "        and the existing predictions for the estimator will be kept.\n",
    "        \"\"\"\n",
    "        print(\"[INFO] Getting predictions..\")\n",
    "        curr_train_preds, curr_test_preds = self.get_current_train_and_test_preds()\n",
    "\n",
    "        preds_retrieved_count = 0\n",
    "        num_preds_retrieved_but_not_yet_saved = 0\n",
    "        estimators_skipped = []\n",
    "\n",
    "        for estimator in self.estimators:\n",
    "            estimator_hash_value = estimator.get_hash_value()\n",
    "            estimator_name = f\"{estimator.name} ({estimator_hash_value})\"\n",
    "\n",
    "            # skip retrieving predictions for estimator if non-stale predictions are already available\n",
    "            train_preds_available = any(estimator_hash_value in col_name for col_name in curr_train_preds.columns)\n",
    "            test_preds_available = any(estimator_hash_value in col_name for col_name in curr_test_preds.columns)\n",
    "            if train_preds_available and test_preds_available:\n",
    "                estimators_skipped += [estimator_name]\n",
    "                continue\n",
    "\n",
    "            print(f\"[INFO] Getting predictions for estimator {estimator_name}\")\n",
    "            train_preds, test_preds = estimator.get_preds(estimator.params_dict, estimator.feature_names)\n",
    "            if not isinstance(train_preds, pd.core.series.Series):\n",
    "                raise ValueError(\"`train_preds` should be of type `pandas.Series`\")\n",
    "            if not isinstance(test_preds, pd.core.series.Series):\n",
    "                raise ValueError(\"`test_preds` should be of type `pandas.Series`\")\n",
    "            curr_train_preds[estimator_name] = train_preds\n",
    "            curr_test_preds[estimator_name] = test_preds\n",
    "            preds_retrieved_count += 1\n",
    "\n",
    "            # save predictions at an interval of `preds_save_interval`\n",
    "            if preds_retrieved_count % self.preds_save_interval == 0:\n",
    "                curr_train_preds.sort_index(axis=1).to_csv(self.get_train_preds_file_path(), index=False)\n",
    "                curr_test_preds.sort_index(axis=1).to_csv(self.get_test_preds_file_path(), index=False)\n",
    "                num_preds_retrieved_but_not_yet_saved = 0\n",
    "                print(\"[INFO] Saved predictions\")\n",
    "            else:\n",
    "                num_preds_retrieved_but_not_yet_saved += 1\n",
    "\n",
    "        if estimators_skipped:\n",
    "            estimators_skipped.sort()\n",
    "            formatted_estimators = \", \".join(estimators_skipped)\n",
    "            print(f\"[INFO] Skipped retrieving predictions for following estimators as their current ones are not stale:\\n{formatted_estimators}\")\n",
    "\n",
    "        if num_preds_retrieved_but_not_yet_saved != 0:\n",
    "            curr_train_preds.sort_index(axis=1).to_csv(self.get_train_preds_file_path(), index=False)\n",
    "            curr_test_preds.sort_index(axis=1).to_csv(self.get_test_preds_file_path(), index=False)\n",
    "            print(\"[INFO] Saved predictions\")\n",
    "\n",
    "        print(\"[INFO] Finished getting all predictions\")\n",
    "\n",
    "    def sync_preds(self):\n",
    "        \"\"\"\n",
    "        Syncs the predictions stored at the two file paths specified by\n",
    "        `working_dir_path`, `train_preds_filename` and `test_preds_filename` by\n",
    "        removing predictions for any estimator that is not currently in `estimators`.\n",
    "\n",
    "        Note that new predictions for estimators that do not currently have predictions\n",
    "        in the files will not be added; `get_preds` should be used for this purpose\n",
    "        instead.\n",
    "        \"\"\"\n",
    "        print(\"[INFO] Syncing predictions..\")\n",
    "        estimator_hash_values = [estimator.get_hash_value() for estimator in self.estimators]\n",
    "        should_remove_col = lambda col_name: not any(hash_value in col_name for hash_value in estimator_hash_values)\n",
    "\n",
    "        curr_train_preds, curr_test_preds = self.get_current_train_and_test_preds()\n",
    "\n",
    "        if not curr_train_preds.empty:\n",
    "            col_names_to_remove = [col_name for col_name in curr_train_preds.columns if should_remove_col(col_name)]\n",
    "            if col_names_to_remove:\n",
    "                print(f\"[INFO] Dropping columns for following estimators from training predictions:\\n{col_names_to_remove}\")\n",
    "                curr_train_preds.drop(columns=col_names_to_remove, inplace=True)\n",
    "                curr_train_preds.to_csv(self.get_train_preds_file_path(), index=False)\n",
    "            else:\n",
    "                print(f\"[INFO] No columns for training predictions were dropped\")\n",
    "        if not curr_test_preds.empty:\n",
    "            col_names_to_remove = [col_name for col_name in curr_test_preds.columns if should_remove_col(col_name)]\n",
    "            if col_names_to_remove:\n",
    "                print(f\"[INFO] Dropping columns for following estimators from test predictions:\\n{col_names_to_remove}\")\n",
    "                curr_test_preds.drop(columns=col_names_to_remove, inplace=True)\n",
    "                curr_test_preds.to_csv(self.get_test_preds_file_path(), index=False)\n",
    "            else:\n",
    "                print(f\"[INFO] No columns for test predictions were dropped\")\n",
    "\n",
    "        print(\"[INFO] Finished syncing predictions\")\n",
    "\n",
    "    def import_preds(self, input_dir_path):\n",
    "        \"\"\"\n",
    "        Imports predictions stored at the two file paths at `input_dir_path` with\n",
    "        `train_preds_filename` and `test_preds_filename` as their filenames. If no\n",
    "        such files are found, no predictions will be imported.\n",
    "\n",
    "        Only predictions for estimators specified in `estimators` will be imported.\n",
    "        Any predictions for estimators that were already available will be overwritten\n",
    "        with predictions for the same estimators found in the files at `input_dir_path`.\n",
    "\n",
    "        :param input_dir_path:\n",
    "            The path to the directory for the training and test predictions files.\n",
    "            The file names are expected to be the same as `train_preds_filename`\n",
    "            and `test_preds_filename`\n",
    "        \"\"\"\n",
    "        print(\"[INFO] Importing predictions..\")\n",
    "        curr_train_preds, curr_test_preds = self.get_current_train_and_test_preds()\n",
    "        input_train_preds = pd.DataFrame()\n",
    "        input_test_preds = pd.DataFrame()\n",
    "\n",
    "        input_train_preds_path = Path(f\"{input_dir_path}/{self.train_preds_filename}.csv\")\n",
    "        input_test_preds_path = Path(f\"{input_dir_path}/{self.test_preds_filename}.csv\")\n",
    "        if input_train_preds_path.is_file():\n",
    "            try:\n",
    "                input_train_preds = pd.read_csv(input_train_preds_path)\n",
    "            except: pass\n",
    "        if input_test_preds_path.is_file():\n",
    "            try:\n",
    "                input_test_preds = pd.read_csv(input_test_preds_path)\n",
    "            except: pass\n",
    "\n",
    "        estimators_with_imported_train_preds = []\n",
    "        estimators_with_imported_test_preds = []\n",
    "        for estimator in self.estimators:\n",
    "            estimator_hash_value = estimator.get_hash_value()\n",
    "            estimator_name = f\"{estimator.name} ({estimator_hash_value})\"\n",
    "            train_preds_available = any(estimator_hash_value in col_name for col_name in input_train_preds.columns)\n",
    "            test_preds_available = any(estimator_hash_value in col_name for col_name in input_test_preds.columns)\n",
    "\n",
    "            if train_preds_available:\n",
    "                curr_train_preds[estimator_name] = input_train_preds[estimator_name]\n",
    "                estimators_with_imported_train_preds += [estimator_name]\n",
    "            if test_preds_available:\n",
    "                curr_test_preds[estimator_name] = input_test_preds[estimator_name]\n",
    "                estimators_with_imported_test_preds += [estimator_name]\n",
    "\n",
    "        if not estimators_with_imported_train_preds:\n",
    "            print(\"[INFO] No train predictions were imported\")\n",
    "        else:\n",
    "            curr_train_preds.sort_index(axis=1).to_csv(self.get_train_preds_file_path(), index=False)\n",
    "            formatted_estimators = \", \".join(estimators_with_imported_train_preds)\n",
    "            print(f\"[INFO] {len(estimators_with_imported_train_preds)} train predictions were imported:\\n{formatted_estimators}\")\n",
    "        if not estimators_with_imported_test_preds:\n",
    "            print(\"[INFO] No test predictions were imported\")\n",
    "        else:\n",
    "            curr_test_preds.sort_index(axis=1).to_csv(self.get_test_preds_file_path(), index=False)\n",
    "            formatted_estimators = \", \".join(estimators_with_imported_test_preds)\n",
    "            print(f\"[INFO] {len(estimators_with_imported_test_preds)} test predictions were imported:\\n{formatted_estimators}\")\n",
    "        \n",
    "        print(\"[INFO] Finished importing predictions\")\n",
    "\n",
    "    def clear_preds(self):\n",
    "        \"\"\"\n",
    "        Removes all stored predictions by deleting the two files at filepaths specified\n",
    "        by `working_dir_path`, `train_preds_filename` and `test_preds_filename`.\n",
    "        \"\"\"\n",
    "        train_preds_file_path = self.get_train_preds_file_path()\n",
    "        test_preds_file_path = self.get_test_preds_file_path()\n",
    "\n",
    "        if train_preds_file_path.is_file():\n",
    "            train_preds_file_path.unlink()\n",
    "        if test_preds_file_path.is_file():\n",
    "            test_preds_file_path.unlink()\n",
    "\n",
    "        print(\"[INFO] Finished clearing predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c819ee",
   "metadata": {
    "papermill": {
     "duration": 0.007267,
     "end_time": "2026-01-10T13:02:45.857998",
     "exception": false,
     "start_time": "2026-01-10T13:02:45.850731",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Next, we'll create a variable for storing the estimators (`StackingEstimator` instances) that we'll pass to the `StackingPredictionsRetriever` class for getting all the predictions from our base models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1484378e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T13:02:45.873946Z",
     "iopub.status.busy": "2026-01-10T13:02:45.873491Z",
     "iopub.status.idle": "2026-01-10T13:02:45.876984Z",
     "shell.execute_reply": "2026-01-10T13:02:45.876324Z"
    },
    "papermill": {
     "duration": 0.013274,
     "end_time": "2026-01-10T13:02:45.878475",
     "exception": false,
     "start_time": "2026-01-10T13:02:45.865201",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "estimators = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bee7b3b",
   "metadata": {
    "papermill": {
     "duration": 0.00731,
     "end_time": "2026-01-10T13:02:45.893269",
     "exception": false,
     "start_time": "2026-01-10T13:02:45.885959",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 6. Feature Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "84441327",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T13:02:45.909353Z",
     "iopub.status.busy": "2026-01-10T13:02:45.908691Z",
     "iopub.status.idle": "2026-01-10T13:02:45.912815Z",
     "shell.execute_reply": "2026-01-10T13:02:45.912141Z"
    },
    "papermill": {
     "duration": 0.013844,
     "end_time": "2026-01-10T13:02:45.914247",
     "exception": false,
     "start_time": "2026-01-10T13:02:45.900403",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "FEATURE_SET_1 = [\n",
    "    'gender', 'course', 'study_hours',\n",
    "    'class_attendance', 'internet_access', 'sleep_hours',\n",
    "    'sleep_quality', 'study_method', 'facility_rating',\n",
    "    'exam_difficulty', 'study_hours_sq', 'class_attendance_sq',\n",
    "    'sleep_parabola', 'study_hours_log', 'study_hours_x_class_attendance',\n",
    "    'study_hours_x_sleep_hours', 'study_hours_x_sleep_quality', 'study_hours_x_exam_difficulty',\n",
    "    'study_hours_x_internet_access', 'sleep_hours_x_sleep_quality', 'class_attendance_x_exam_difficulty',\n",
    "    'class_attendance_x_facility_rating', 'study_hours_per_class_attendance',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9ca8f7",
   "metadata": {
    "papermill": {
     "duration": 0.007277,
     "end_time": "2026-01-10T13:02:45.928647",
     "exception": false,
     "start_time": "2026-01-10T13:02:45.921370",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 7. Base Model Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "13bfeefd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T13:02:45.944183Z",
     "iopub.status.busy": "2026-01-10T13:02:45.943904Z",
     "iopub.status.idle": "2026-01-10T13:02:45.947319Z",
     "shell.execute_reply": "2026-01-10T13:02:45.946650Z"
    },
    "papermill": {
     "duration": 0.012775,
     "end_time": "2026-01-10T13:02:45.948619",
     "exception": false,
     "start_time": "2026-01-10T13:02:45.935844",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# to skip hyperparameter tuning when it's not needed; set to `False` to do the tuning\n",
    "SKIP_BASE_MODEL_HYPERPARAMETER_TUNING = True\n",
    "\n",
    "# value set for early stopping for base models that support it; this value will be used for actual model training as well\n",
    "BASE_MODEL_EARLY_STOPPING_ROUNDS = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "faaf976c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T13:02:45.964369Z",
     "iopub.status.busy": "2026-01-10T13:02:45.964141Z",
     "iopub.status.idle": "2026-01-10T13:02:45.967616Z",
     "shell.execute_reply": "2026-01-10T13:02:45.967082Z"
    },
    "papermill": {
     "duration": 0.012766,
     "end_time": "2026-01-10T13:02:45.968913",
     "exception": false,
     "start_time": "2026-01-10T13:02:45.956147",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BaseModelOptunaStudyEstimator(Enum):\n",
    "    CATBOOSTREGRESSOR = 'CatBoostRegressor'\n",
    "    LGBMREGRESSOR = 'LGBMRegressor'\n",
    "    XGBREGRESSOR = 'XGBRegressor'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1c2be695",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T13:02:45.984673Z",
     "iopub.status.busy": "2026-01-10T13:02:45.984172Z",
     "iopub.status.idle": "2026-01-10T13:02:45.987676Z",
     "shell.execute_reply": "2026-01-10T13:02:45.987002Z"
    },
    "papermill": {
     "duration": 0.012743,
     "end_time": "2026-01-10T13:02:45.989015",
     "exception": false,
     "start_time": "2026-01-10T13:02:45.976272",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# estimator to use for Optuna study\n",
    "BASE_MODEL_OPTUNA_STUDY_ESTIMATOR = BaseModelOptunaStudyEstimator.XGBREGRESSOR\n",
    "\n",
    "# feature set to use for Optuna study\n",
    "BASE_MODEL_OPTUNA_STUDY_FEATURE_SET = FEATURE_SET_1\n",
    "\n",
    "# maximum number of trials Optuna will conduct for the optimization\n",
    "BASE_MODEL_OPTUNA_STUDY_NUM_TRIALS = 500\n",
    "\n",
    "# number of splits to use for K-Fold Cross-Validation for Optuna study\n",
    "BASE_MODEL_OPTUNA_STUDY_KFOLD_NUM_SPLITS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e48a7648",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T13:02:46.004672Z",
     "iopub.status.busy": "2026-01-10T13:02:46.004455Z",
     "iopub.status.idle": "2026-01-10T13:02:46.019160Z",
     "shell.execute_reply": "2026-01-10T13:02:46.018500Z"
    },
    "papermill": {
     "duration": 0.024135,
     "end_time": "2026-01-10T13:02:46.020470",
     "exception": false,
     "start_time": "2026-01-10T13:02:45.996335",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_base_model_optuna_params(trial, study_estimator):\n",
    "    if study_estimator == BaseModelOptunaStudyEstimator.CATBOOSTREGRESSOR:\n",
    "        if BASE_MODEL_OPTUNA_STUDY_FEATURE_SET == FEATURE_SET_1:\n",
    "            return {\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 0.010, 0.015),\n",
    "                'depth': trial.suggest_categorical('depth', [6]),\n",
    "                'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 2.0, 4.0, log=True),\n",
    "                'random_strength': trial.suggest_float('random_strength', 1.0, 2.5),\n",
    "                'colsample_bylevel': trial.suggest_float('colsample_bylevel', 0.2, 0.3),\n",
    "                'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 85, 105),\n",
    "                'bagging_temperature': trial.suggest_float('bagging_temperature', 0.0, 0.2),\n",
    "            }\n",
    "        else:\n",
    "            raise ValueError(f\"Search space for feature set for Optuna study not yet specified for CatBoostRegressor.\")\n",
    "    elif study_estimator == BaseModelOptunaStudyEstimator.LGBMREGRESSOR:\n",
    "        if BASE_MODEL_OPTUNA_STUDY_FEATURE_SET == FEATURE_SET_1:\n",
    "            return {\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 0.002, 0.0035),\n",
    "                'num_leaves': trial.suggest_int('num_leaves', 90, 120),\n",
    "                'min_child_samples': trial.suggest_int('min_child_samples', 5, 20),\n",
    "                'min_split_gain': trial.suggest_float('min_split_gain', 1e-3, 0.1, log=True),\n",
    "                'subsample': trial.suggest_float('subsample', 0.85, 1.0),\n",
    "                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.15, 0.25),\n",
    "                'reg_alpha': trial.suggest_float('reg_alpha', 1e-4, 0.1, log=True),\n",
    "                'reg_lambda': trial.suggest_float('reg_lambda', 10.0, 30.0, log=True),\n",
    "            }\n",
    "        else:\n",
    "            raise ValueError(f\"Search space for feature set for Optuna study not yet specified for LGBMRegressor.\")\n",
    "    elif study_estimator == BaseModelOptunaStudyEstimator.XGBREGRESSOR:\n",
    "        if BASE_MODEL_OPTUNA_STUDY_FEATURE_SET == FEATURE_SET_1:\n",
    "            return {\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 0.011, 0.014),\n",
    "                'max_depth': trial.suggest_categorical('max_depth', [6]),\n",
    "                'subsample': trial.suggest_float('subsample', 0.94, 1.0),\n",
    "                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.22, 0.27),\n",
    "                'alpha': trial.suggest_float('alpha', 1.0, 3.0, log=True),\n",
    "                'lambda': trial.suggest_float('lambda', 1e-3, 1.0, log=True),\n",
    "                'gamma': trial.suggest_float('gamma', 1e-4, 0.2, log=True),\n",
    "                'min_child_weight': trial.suggest_int('min_child_weight', 28, 38),\n",
    "            }\n",
    "        else:\n",
    "            raise ValueError(f\"Search space for feature set for Optuna study not yet specified for XGBRegressor.\")\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported optuna study estimator\")\n",
    "\n",
    "def get_base_model_predictions(study_estimator, trial_params, X_train_fold, y_train_fold, X_validation_fold, y_validation_fold):\n",
    "    if study_estimator == BaseModelOptunaStudyEstimator.CATBOOSTREGRESSOR:\n",
    "        model = CatBoostRegressor(\n",
    "            **trial_params,\n",
    "            iterations=30000,\n",
    "            use_best_model=True,\n",
    "            cat_features=cat_features,\n",
    "            bootstrap_type='Bayesian',\n",
    "            loss_function='RMSE',\n",
    "            eval_metric='RMSE',\n",
    "            task_type='GPU' if torch.cuda.is_available() else 'CPU',\n",
    "            devices='0',\n",
    "            random_seed=RANDOM_SEEDS[0],\n",
    "            verbose=False,\n",
    "            allow_writing_files=False\n",
    "        )\n",
    "        model.fit(\n",
    "            X_train_fold, y_train_fold,\n",
    "            eval_set=(X_validation_fold, y_validation_fold),\n",
    "            early_stopping_rounds=BASE_MODEL_EARLY_STOPPING_ROUNDS\n",
    "        )\n",
    "        return model.predict(X_validation_fold)\n",
    "    elif study_estimator == BaseModelOptunaStudyEstimator.LGBMREGRESSOR:\n",
    "        model = lgb.LGBMRegressor(\n",
    "            **trial_params,\n",
    "            n_estimators=30000,\n",
    "            objective='regression',\n",
    "            metric='rmse',\n",
    "            bagging_freq=1,\n",
    "            verbose=-1,\n",
    "            n_jobs=-1,\n",
    "            random_state=RANDOM_SEEDS[0]\n",
    "        )\n",
    "        model.fit(\n",
    "            X_train_fold, y_train_fold,\n",
    "            eval_set=[(X_validation_fold, y_validation_fold)],\n",
    "            callbacks=[lgb.early_stopping(stopping_rounds=BASE_MODEL_EARLY_STOPPING_ROUNDS, verbose=0)]\n",
    "        )\n",
    "        return model.predict(X_validation_fold)\n",
    "    elif study_estimator == BaseModelOptunaStudyEstimator.XGBREGRESSOR:\n",
    "        model = XGBRegressor(\n",
    "            **trial_params,\n",
    "            n_estimators=30000,\n",
    "            tree_method='hist' if torch.cuda.is_available() else 'auto',\n",
    "            device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "            enable_categorical=True,\n",
    "            objective='reg:squarederror',\n",
    "            eval_metric='rmse',\n",
    "            early_stopping_rounds=BASE_MODEL_EARLY_STOPPING_ROUNDS,\n",
    "            n_jobs=-1,\n",
    "            random_state=RANDOM_SEEDS[0],\n",
    "            verbosity=0\n",
    "        )\n",
    "        model.fit(\n",
    "            X_train_fold, y_train_fold,\n",
    "            eval_set=[(X_validation_fold, y_validation_fold)],\n",
    "            verbose=False\n",
    "        )\n",
    "        return model.predict(X_validation_fold)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported optuna study estimator\")\n",
    "\n",
    "def base_model_optuna_study_objective(trial):\n",
    "    base_model_params = get_base_model_optuna_params(trial, BASE_MODEL_OPTUNA_STUDY_ESTIMATOR)\n",
    "\n",
    "    X_train = train_data[BASE_MODEL_OPTUNA_STUDY_FEATURE_SET]\n",
    "    y_train = train_data[target_col]\n",
    "\n",
    "    base_model_optuna_study_kf = KFold(n_splits=BASE_MODEL_OPTUNA_STUDY_KFOLD_NUM_SPLITS, shuffle=True, random_state=RANDOM_SEEDS[0])\n",
    "    base_model_optuna_study_kf_splits = base_model_optuna_study_kf.split(X_train, y_train)\n",
    "    base_model_optuna_study_kf_enumeration = enumerate(base_model_optuna_study_kf_splits)\n",
    "\n",
    "    total_rmse = 0\n",
    "\n",
    "    for fold, (train_indices, validation_indices) in base_model_optuna_study_kf_enumeration:\n",
    "        X_train_fold = X_train.iloc[train_indices]\n",
    "        X_validation_fold = X_train.iloc[validation_indices]\n",
    "        y_train_fold = y_train.iloc[train_indices]\n",
    "        y_validation_fold = y_train.iloc[validation_indices]\n",
    "\n",
    "        y_validation_pred = get_base_model_predictions(\n",
    "            BASE_MODEL_OPTUNA_STUDY_ESTIMATOR,\n",
    "            base_model_params,\n",
    "            X_train_fold, y_train_fold,\n",
    "            X_validation_fold, y_validation_fold\n",
    "        )\n",
    "\n",
    "        y_validation_pred = np.clip(y_validation_pred, 0, 100)\n",
    "\n",
    "        rmse_fold = np.sqrt(mean_squared_error(y_validation_fold, y_validation_pred))\n",
    "        total_rmse += rmse_fold\n",
    "\n",
    "        trial.report(rmse_fold, step=fold)\n",
    "        if trial.should_prune():\n",
    "            raise optuna.TrialPruned()\n",
    "\n",
    "    average_rmse = total_rmse / BASE_MODEL_OPTUNA_STUDY_KFOLD_NUM_SPLITS\n",
    "    return average_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1da54ff7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T13:02:46.035810Z",
     "iopub.status.busy": "2026-01-10T13:02:46.035594Z",
     "iopub.status.idle": "2026-01-10T13:02:46.040471Z",
     "shell.execute_reply": "2026-01-10T13:02:46.039683Z"
    },
    "papermill": {
     "duration": 0.014141,
     "end_time": "2026-01-10T13:02:46.041824",
     "exception": false,
     "start_time": "2026-01-10T13:02:46.027683",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped base model hyperparameter tuning\n"
     ]
    }
   ],
   "source": [
    "if SKIP_BASE_MODEL_HYPERPARAMETER_TUNING:\n",
    "    print(\"Skipped base model hyperparameter tuning\")\n",
    "else:\n",
    "    print(f\"Started base model hyperparameter tuning for {BASE_MODEL_OPTUNA_STUDY_ESTIMATOR.value}\")\n",
    "    sampler = optuna.samplers.TPESampler(n_ei_candidates=50, multivariate=True)\n",
    "    study = optuna.create_study(sampler=sampler, direction='minimize', study_name='base_model_study')\n",
    "    study.optimize(base_model_optuna_study_objective, n_trials=BASE_MODEL_OPTUNA_STUDY_NUM_TRIALS)\n",
    "    \n",
    "    print(f\"# trials finished: {len(study.trials)}\")\n",
    "    trial = study.best_trial\n",
    "    print(f\"Best trial AUC: {trial.value}\")\n",
    "    print(f\"Best trial params:\")\n",
    "    for param_key, param_value in trial.params.items():\n",
    "        print(f\"- {param_key}: {param_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0acf8d5a",
   "metadata": {
    "papermill": {
     "duration": 0.007268,
     "end_time": "2026-01-10T13:02:46.056454",
     "exception": false,
     "start_time": "2026-01-10T13:02:46.049186",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 8. Base Model Candidates\n",
    "\n",
    "The base models that are candidates for the ensemble are specified. Instead of only relying on the meta-model to filter out unhelpful base model predictions, Optuna studies will also be conducted to find the feature set (set of base model predictions) that should be used for the meta-model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9969d609",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T13:02:46.072021Z",
     "iopub.status.busy": "2026-01-10T13:02:46.071801Z",
     "iopub.status.idle": "2026-01-10T13:02:46.074775Z",
     "shell.execute_reply": "2026-01-10T13:02:46.074237Z"
    },
    "papermill": {
     "duration": 0.012201,
     "end_time": "2026-01-10T13:02:46.075994",
     "exception": false,
     "start_time": "2026-01-10T13:02:46.063793",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# number of splits to use for K-Fold Cross-Validation for base models\n",
    "BASE_MODEL_KFOLD_NUM_SPLITS = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b91789f",
   "metadata": {
    "papermill": {
     "duration": 0.007177,
     "end_time": "2026-01-10T13:02:46.090330",
     "exception": false,
     "start_time": "2026-01-10T13:02:46.083153",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 8.1 CatBoostRegressor\n",
    "\n",
    "### 8.1.1 Helper Methods (CatBoostRegressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4abc3aca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T13:02:46.105831Z",
     "iopub.status.busy": "2026-01-10T13:02:46.105637Z",
     "iopub.status.idle": "2026-01-10T13:02:46.112544Z",
     "shell.execute_reply": "2026-01-10T13:02:46.112114Z"
    },
    "papermill": {
     "duration": 0.01647,
     "end_time": "2026-01-10T13:02:46.113908",
     "exception": false,
     "start_time": "2026-01-10T13:02:46.097438",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_catboostregressor_preds(params_dict, feature_names):\n",
    "    oof_preds_accumulator = np.zeros(len(train_data))\n",
    "    test_preds_accumulator = np.zeros(len(test_data))\n",
    "\n",
    "    X_train = train_data[feature_names]\n",
    "    y_train = train_data[target_col]\n",
    "\n",
    "    for random_seed in RANDOM_SEEDS:\n",
    "        kf = KFold(n_splits=BASE_MODEL_KFOLD_NUM_SPLITS, shuffle=True, random_state=random_seed)\n",
    "        kf_splits = kf.split(X_train, y_train)\n",
    "        kf_enumeration = enumerate(kf_splits)\n",
    "\n",
    "        seed_oof_preds = np.zeros(len(train_data))\n",
    "        \n",
    "        for fold, (train_indices, validation_indices) in kf_enumeration:\n",
    "            X_train_fold = X_train.iloc[train_indices]\n",
    "            X_validation_fold = X_train.iloc[validation_indices]\n",
    "            y_train_fold = y_train.iloc[train_indices]\n",
    "            y_validation_fold = y_train.iloc[validation_indices]\n",
    "\n",
    "            model = CatBoostRegressor(\n",
    "                **params_dict,\n",
    "                iterations=30000,\n",
    "                use_best_model=True,\n",
    "                cat_features=cat_features,\n",
    "                bootstrap_type='Bayesian',\n",
    "                loss_function='RMSE',\n",
    "                eval_metric='RMSE',\n",
    "                task_type='GPU' if torch.cuda.is_available() else 'CPU',\n",
    "                devices='0',\n",
    "                random_seed=random_seed,\n",
    "                verbose=False,\n",
    "                allow_writing_files=False\n",
    "            )\n",
    "            \n",
    "            model.fit(\n",
    "                X_train_fold, y_train_fold,\n",
    "                eval_set=(X_validation_fold, y_validation_fold),\n",
    "                early_stopping_rounds=BASE_MODEL_EARLY_STOPPING_ROUNDS\n",
    "            )\n",
    "\n",
    "            y_validation_pred = model.predict(X_validation_fold)\n",
    "            y_test_pred = model.predict(test_data[feature_names])\n",
    "\n",
    "            y_validation_pred = np.clip(y_validation_pred, 0, 100)\n",
    "            y_test_pred = np.clip(y_test_pred, 0, 100)\n",
    "\n",
    "            seed_oof_preds[validation_indices] = y_validation_pred\n",
    "            test_preds_accumulator += y_test_pred\n",
    "\n",
    "        oof_preds_accumulator += seed_oof_preds\n",
    "\n",
    "    final_oof_preds = oof_preds_accumulator / len(RANDOM_SEEDS)\n",
    "    final_test_preds = test_preds_accumulator / (BASE_MODEL_KFOLD_NUM_SPLITS * len(RANDOM_SEEDS))\n",
    "    \n",
    "    return pd.Series(final_oof_preds), pd.Series(final_test_preds)\n",
    "\n",
    "def get_catboostregressor_stacking_estimator(index, params_dict, feature_names):\n",
    "    return StackingEstimator(\n",
    "        name=f\"CatBoostRegressor_{index}\",\n",
    "        params_dict=params_dict,\n",
    "        feature_names=feature_names,\n",
    "        get_preds=get_catboostregressor_preds\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3f872f",
   "metadata": {
    "papermill": {
     "duration": 0.007372,
     "end_time": "2026-01-10T13:02:46.128719",
     "exception": false,
     "start_time": "2026-01-10T13:02:46.121347",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 8.1.2 Add Estimators (CatBoostRegressor)\n",
    "\n",
    "Add CatBoostRegressor estimators to list that StackingPredictionsRetriever will process. Hyperparameters were found using Optuna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "33465a21",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T13:02:46.144202Z",
     "iopub.status.busy": "2026-01-10T13:02:46.143955Z",
     "iopub.status.idle": "2026-01-10T13:02:46.147510Z",
     "shell.execute_reply": "2026-01-10T13:02:46.146809Z"
    },
    "papermill": {
     "duration": 0.012781,
     "end_time": "2026-01-10T13:02:46.148819",
     "exception": false,
     "start_time": "2026-01-10T13:02:46.136038",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# CatBoostRegressor base models using FEATURE_SET_1\n",
    "estimators += [\n",
    "    #   get_catboostregressor_stacking_estimator(\n",
    "    #     index=1,\n",
    "    #     params_dict={ # Optuna study RMSE: 8.732405143185524\n",
    "    #         'learning_rate': 0.012512877480828674,\n",
    "    #         'depth': 6,\n",
    "    #         'l2_leaf_reg': 2.785033525504829,\n",
    "    #         'random_strength': 1.9972412072333539,\n",
    "    #         'colsample_bylevel': 0.2578536380324921,\n",
    "    #         'min_data_in_leaf': 93,\n",
    "    #         'bagging_temperature': 0.1409287678742852,\n",
    "    #     },\n",
    "    #     feature_names=FEATURE_SET_1\n",
    "    # ),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb93862",
   "metadata": {
    "papermill": {
     "duration": 0.007363,
     "end_time": "2026-01-10T13:02:46.163362",
     "exception": false,
     "start_time": "2026-01-10T13:02:46.155999",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 8.2 LGBMRegressor\n",
    "\n",
    "### 8.2.1 Helper Methods (LGBMRegressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d939d962",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T13:02:46.179200Z",
     "iopub.status.busy": "2026-01-10T13:02:46.178950Z",
     "iopub.status.idle": "2026-01-10T13:02:46.185945Z",
     "shell.execute_reply": "2026-01-10T13:02:46.185404Z"
    },
    "papermill": {
     "duration": 0.016399,
     "end_time": "2026-01-10T13:02:46.187229",
     "exception": false,
     "start_time": "2026-01-10T13:02:46.170830",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_lgbmregressor_preds(params_dict, feature_names):\n",
    "    oof_preds_accumulator = np.zeros(len(train_data))\n",
    "    test_preds_accumulator = np.zeros(len(test_data))\n",
    "\n",
    "    X_train = train_data[feature_names]\n",
    "    y_train = train_data[target_col]\n",
    "\n",
    "    for random_seed in RANDOM_SEEDS:\n",
    "        kf = KFold(n_splits=BASE_MODEL_KFOLD_NUM_SPLITS, shuffle=True, random_state=random_seed)\n",
    "        kf_splits = kf.split(X_train, y_train)\n",
    "        kf_enumeration = enumerate(kf_splits)\n",
    "\n",
    "        seed_oof_preds = np.zeros(len(train_data))\n",
    "\n",
    "        for fold, (train_indices, validation_indices) in kf_enumeration:\n",
    "            X_train_fold = X_train.iloc[train_indices]\n",
    "            X_validation_fold = X_train.iloc[validation_indices]\n",
    "            y_train_fold = y_train.iloc[train_indices]\n",
    "            y_validation_fold = y_train.iloc[validation_indices]\n",
    "\n",
    "            model = lgb.LGBMRegressor(\n",
    "                **params_dict,\n",
    "                n_estimators=30000,\n",
    "                objective='regression',\n",
    "                metric='rmse',\n",
    "                bagging_freq=1,\n",
    "                verbose=-1,\n",
    "                n_jobs=-1,\n",
    "                random_state=random_seed\n",
    "            )\n",
    "            \n",
    "            model.fit(\n",
    "                X_train_fold, y_train_fold,\n",
    "                eval_set=[(X_validation_fold, y_validation_fold)],\n",
    "                callbacks=[lgb.early_stopping(stopping_rounds=BASE_MODEL_EARLY_STOPPING_ROUNDS, verbose=0)]\n",
    "            )\n",
    "\n",
    "            y_validation_pred = model.predict(X_validation_fold)\n",
    "            y_test_pred = model.predict(test_data[feature_names])\n",
    "\n",
    "            y_validation_pred = np.clip(y_validation_pred, 0, 100)\n",
    "            y_test_pred = np.clip(y_test_pred, 0, 100)\n",
    "\n",
    "            seed_oof_preds[validation_indices] = y_validation_pred\n",
    "            test_preds_accumulator += y_test_pred\n",
    "\n",
    "        oof_preds_accumulator += seed_oof_preds\n",
    "\n",
    "    final_oof_preds = oof_preds_accumulator / len(RANDOM_SEEDS)\n",
    "    final_test_preds = test_preds_accumulator / (BASE_MODEL_KFOLD_NUM_SPLITS * len(RANDOM_SEEDS))\n",
    "    \n",
    "    return pd.Series(final_oof_preds), pd.Series(final_test_preds)\n",
    "\n",
    "def get_lgbmregressor_stacking_estimator(index, params_dict, feature_names):\n",
    "    return StackingEstimator(\n",
    "        name=f\"LGBMRegressor_{index}\",\n",
    "        params_dict=params_dict,\n",
    "        feature_names=feature_names,\n",
    "        get_preds=get_lgbmregressor_preds\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410acb1c",
   "metadata": {
    "papermill": {
     "duration": 0.007321,
     "end_time": "2026-01-10T13:02:46.201753",
     "exception": false,
     "start_time": "2026-01-10T13:02:46.194432",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 8.2.2 Add Estimators (LGBMRegressor)\n",
    "\n",
    "Add LGBMRegressor estimators to list that StackingPredictionsRetriever will process. Hyperparameters were found using Optuna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ea1ab640",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T13:02:46.217347Z",
     "iopub.status.busy": "2026-01-10T13:02:46.217116Z",
     "iopub.status.idle": "2026-01-10T13:02:46.221115Z",
     "shell.execute_reply": "2026-01-10T13:02:46.220567Z"
    },
    "papermill": {
     "duration": 0.01331,
     "end_time": "2026-01-10T13:02:46.222470",
     "exception": false,
     "start_time": "2026-01-10T13:02:46.209160",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# LGBMRegressor base models using FEATURE_SET_1\n",
    "estimators += [\n",
    "    get_lgbmregressor_stacking_estimator(\n",
    "        index=1,\n",
    "        params_dict={ # Optuna study RMSE: 8.731615849391675\n",
    "            'learning_rate': 0.00560062116243721,\n",
    "            'num_leaves': 56,\n",
    "            'min_child_samples': 26,\n",
    "            'min_split_gain': 0.04953245371789207,\n",
    "            'subsample': 0.8713341159266043,\n",
    "            'colsample_bytree': 0.1954098975745725,\n",
    "            'reg_alpha': 25.4080280093216,\n",
    "            'reg_lambda': 7.3219797767089885,\n",
    "        },\n",
    "        feature_names=FEATURE_SET_1\n",
    "    ),\n",
    "    #  get_lgbmregressor_stacking_estimator(\n",
    "    #     index=2,\n",
    "    #     params_dict={ # Optuna study RMSE: 8.731139395008947\n",
    "    #         'learning_rate': 0.0023343061793223165,\n",
    "    #         'num_leaves': 106,\n",
    "    #         'min_child_samples': 6,\n",
    "    #         'min_split_gain': 0.015685494632883334,\n",
    "    #         'subsample': 0.8698884920139696,\n",
    "    #         'colsample_bytree': 0.1739375351290053,\n",
    "    #         'reg_alpha': 0.00014704017752338418,\n",
    "    #         'reg_lambda': 15.047782845063596,\n",
    "    #     },\n",
    "    #     feature_names=FEATURE_SET_1\n",
    "    # ),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7b5e8d",
   "metadata": {
    "papermill": {
     "duration": 0.007103,
     "end_time": "2026-01-10T13:02:46.236782",
     "exception": false,
     "start_time": "2026-01-10T13:02:46.229679",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 8.3 XGBRegressor\n",
    "\n",
    "### 8.3.1 Helper Methods (XGBRegressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9a8454df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T13:02:46.252337Z",
     "iopub.status.busy": "2026-01-10T13:02:46.252110Z",
     "iopub.status.idle": "2026-01-10T13:02:46.259646Z",
     "shell.execute_reply": "2026-01-10T13:02:46.259120Z"
    },
    "papermill": {
     "duration": 0.016825,
     "end_time": "2026-01-10T13:02:46.260871",
     "exception": false,
     "start_time": "2026-01-10T13:02:46.244046",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_xgbregressor_preds(params_dict, feature_names):\n",
    "    oof_preds_accumulator = np.zeros(len(train_data))\n",
    "    test_preds_accumulator = np.zeros(len(test_data))\n",
    "\n",
    "    X_train = train_data[feature_names]\n",
    "    y_train = train_data[target_col]\n",
    "\n",
    "    for random_seed in RANDOM_SEEDS:\n",
    "        kf = KFold(n_splits=BASE_MODEL_KFOLD_NUM_SPLITS, shuffle=True, random_state=random_seed)\n",
    "        kf_splits = kf.split(X_train, y_train)\n",
    "        kf_enumeration = enumerate(kf_splits)\n",
    "\n",
    "        seed_oof_preds = np.zeros(len(train_data))\n",
    "\n",
    "        for fold, (train_indices, validation_indices) in kf_enumeration:\n",
    "            X_train_fold = X_train.iloc[train_indices]\n",
    "            X_validation_fold = X_train.iloc[validation_indices]\n",
    "            y_train_fold = y_train.iloc[train_indices]\n",
    "            y_validation_fold = y_train.iloc[validation_indices]\n",
    "\n",
    "            model = XGBRegressor(\n",
    "                **params_dict,\n",
    "                n_estimators=30000,\n",
    "                tree_method='hist' if torch.cuda.is_available() else 'auto',\n",
    "                device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "                enable_categorical=True,\n",
    "                objective='reg:squarederror',\n",
    "                eval_metric='rmse',\n",
    "                early_stopping_rounds=BASE_MODEL_EARLY_STOPPING_ROUNDS,\n",
    "                n_jobs=-1,\n",
    "                random_state=random_seed,\n",
    "                verbosity=0\n",
    "            )\n",
    "            \n",
    "            model.fit(\n",
    "                X_train_fold, y_train_fold,\n",
    "                eval_set=[(X_validation_fold, y_validation_fold)],\n",
    "                verbose=False\n",
    "            )\n",
    "\n",
    "            y_validation_pred = model.predict(X_validation_fold)\n",
    "            y_test_pred = model.predict(test_data[feature_names])\n",
    "\n",
    "            y_validation_pred = np.clip(y_validation_pred, 0, 100)\n",
    "            y_test_pred = np.clip(y_test_pred, 0, 100)\n",
    "\n",
    "            seed_oof_preds[validation_indices] = y_validation_pred\n",
    "            test_preds_accumulator += y_test_pred\n",
    "\n",
    "        oof_preds_accumulator += seed_oof_preds\n",
    "\n",
    "    final_oof_preds = oof_preds_accumulator / len(RANDOM_SEEDS)\n",
    "    final_test_preds = test_preds_accumulator / (BASE_MODEL_KFOLD_NUM_SPLITS * len(RANDOM_SEEDS))\n",
    "    \n",
    "    return pd.Series(final_oof_preds), pd.Series(final_test_preds)\n",
    "\n",
    "def get_xgbregressor_stacking_estimator(index, params_dict, feature_names):\n",
    "    return StackingEstimator(\n",
    "        name=f\"XGBRegressor_{index}\",\n",
    "        params_dict=params_dict,\n",
    "        feature_names=feature_names,\n",
    "        get_preds=get_xgbregressor_preds\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e151a71c",
   "metadata": {
    "papermill": {
     "duration": 0.007241,
     "end_time": "2026-01-10T13:02:46.275398",
     "exception": false,
     "start_time": "2026-01-10T13:02:46.268157",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 8.3.2 Add Estimators (XGBRegressor)\n",
    "\n",
    "Add XGBRegressor estimators to list that StackingPredictionsRetriever will process. Hyperparameters were found using Optuna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5ca4de13",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T13:02:46.291481Z",
     "iopub.status.busy": "2026-01-10T13:02:46.291243Z",
     "iopub.status.idle": "2026-01-10T13:02:46.297621Z",
     "shell.execute_reply": "2026-01-10T13:02:46.297088Z"
    },
    "papermill": {
     "duration": 0.016182,
     "end_time": "2026-01-10T13:02:46.298854",
     "exception": false,
     "start_time": "2026-01-10T13:02:46.282672",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# XGBRegressor base models using FEATURE_SET_1\n",
    "estimators += [\n",
    "    get_xgbregressor_stacking_estimator(\n",
    "        index=1,\n",
    "        params_dict={ # Optuna study RMSE: 8.726746380604618\n",
    "            'learning_rate': 0.013587597098853266,\n",
    "            'max_depth': 6,\n",
    "            'subsample': 0.9572735014614386,\n",
    "            'colsample_bytree': 0.23913914702804767,\n",
    "            'alpha': 1.4405674568559026,\n",
    "            'lambda': 0.039839278291000785,\n",
    "            'gamma': 0.041457934625352785,\n",
    "            'min_child_weight': 25,\n",
    "        },\n",
    "        feature_names=FEATURE_SET_1\n",
    "    ),\n",
    "    get_xgbregressor_stacking_estimator(\n",
    "        index=2,\n",
    "        params_dict={ # Optuna study RMSE: 8.72559295676817\n",
    "            'learning_rate': 0.011042630421555327,\n",
    "            'max_depth': 6,\n",
    "            'subsample': 0.9599956732090699,\n",
    "            'colsample_bytree': 0.25445172888708584,\n",
    "            'alpha': 0.6887980139572322,\n",
    "            'lambda': 0.05097488457198369,\n",
    "            'gamma': 0.07390409174758057,\n",
    "            'min_child_weight': 38,\n",
    "        },\n",
    "        feature_names=FEATURE_SET_1\n",
    "    ),\n",
    "    get_xgbregressor_stacking_estimator(\n",
    "        index=3,\n",
    "        params_dict={ # Optuna study RMSE: 8.725078961771361\n",
    "            'learning_rate': 0.008078244339287369,\n",
    "            'max_depth': 6,\n",
    "            'subsample': 0.9163689142226277,\n",
    "            'colsample_bytree': 0.2296982964930553,\n",
    "            'alpha': 0.19395772739611541,\n",
    "            'lambda': 0.04919117142485752,\n",
    "            'gamma': 0.16665026636261474,\n",
    "            'min_child_weight': 43,\n",
    "        },\n",
    "        feature_names=FEATURE_SET_1\n",
    "    ),\n",
    "    get_xgbregressor_stacking_estimator(\n",
    "        index=4,\n",
    "        params_dict={ # Optuna study RMSE: 8.725332819931655\n",
    "            'learning_rate': 0.009581863710458615,\n",
    "            'max_depth': 6,\n",
    "            'subsample': 0.9731807527392785,\n",
    "            'colsample_bytree': 0.22833370318746365,\n",
    "            'alpha': 0.48389973296874966,\n",
    "            'lambda': 0.7147942393033277,\n",
    "            'gamma': 0.020976364139759063,\n",
    "            'min_child_weight': 38,\n",
    "        },\n",
    "        feature_names=FEATURE_SET_1\n",
    "    ),\n",
    "    get_xgbregressor_stacking_estimator(\n",
    "        index=5,\n",
    "        params_dict={ # Optuna study RMSE: 8.725728226968462\n",
    "            'learning_rate': 0.01361992613682343,\n",
    "            'max_depth': 6,\n",
    "            'subsample': 0.9596640337178189,\n",
    "            'colsample_bytree': 0.24585494407648067,\n",
    "            'alpha': 1.2816205155984357,\n",
    "            'lambda': 0.10010880106322106,\n",
    "            'gamma': 0.017351186148235078,\n",
    "            'min_child_weight': 38,\n",
    "        },\n",
    "        feature_names=FEATURE_SET_1\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e357068",
   "metadata": {
    "papermill": {
     "duration": 0.007059,
     "end_time": "2026-01-10T13:02:46.313312",
     "exception": false,
     "start_time": "2026-01-10T13:02:46.306253",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 8.4 Number of Base Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "84231b65",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T13:02:46.328565Z",
     "iopub.status.busy": "2026-01-10T13:02:46.328340Z",
     "iopub.status.idle": "2026-01-10T13:02:46.332185Z",
     "shell.execute_reply": "2026-01-10T13:02:46.331460Z"
    },
    "papermill": {
     "duration": 0.013418,
     "end_time": "2026-01-10T13:02:46.333883",
     "exception": false,
     "start_time": "2026-01-10T13:02:46.320465",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of base models: 6\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total number of base models: {len(estimators)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a2cbc3",
   "metadata": {
    "papermill": {
     "duration": 0.008494,
     "end_time": "2026-01-10T13:02:46.351962",
     "exception": false,
     "start_time": "2026-01-10T13:02:46.343468",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 9. Base Model Predictions\n",
    "\n",
    "## 9.1 Get Base Model Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a043d2b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T13:02:46.367809Z",
     "iopub.status.busy": "2026-01-10T13:02:46.367580Z",
     "iopub.status.idle": "2026-01-10T13:34:37.118054Z",
     "shell.execute_reply": "2026-01-10T13:34:37.117243Z"
    },
    "papermill": {
     "duration": 1910.760671,
     "end_time": "2026-01-10T13:34:37.120008",
     "exception": false,
     "start_time": "2026-01-10T13:02:46.359337",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Finished clearing predictions\n",
      "[INFO] Importing predictions..\n",
      "[INFO] 5 train predictions were imported:\n",
      "LGBMRegressor_1 (bc61f24c868323d36835d4301ce2690f), XGBRegressor_1 (d5b33f920572395ea591f85395372949), XGBRegressor_2 (1651e6cc0bc873c5f37ae6eebceb2002), XGBRegressor_3 (7c033486efe2592e85d22287e98391fd), XGBRegressor_4 (7afd382667da316172eb1fc14749b8b8)\n",
      "[INFO] 5 test predictions were imported:\n",
      "LGBMRegressor_1 (bc61f24c868323d36835d4301ce2690f), XGBRegressor_1 (d5b33f920572395ea591f85395372949), XGBRegressor_2 (1651e6cc0bc873c5f37ae6eebceb2002), XGBRegressor_3 (7c033486efe2592e85d22287e98391fd), XGBRegressor_4 (7afd382667da316172eb1fc14749b8b8)\n",
      "[INFO] Finished importing predictions\n",
      "[INFO] Getting predictions..\n",
      "[INFO] Getting predictions for estimator XGBRegressor_5 (9f5cff03ab7df1a857f0e8efa792a24f)\n",
      "[INFO] Saved predictions\n",
      "[INFO] Skipped retrieving predictions for following estimators as their current ones are not stale:\n",
      "LGBMRegressor_1 (bc61f24c868323d36835d4301ce2690f), XGBRegressor_1 (d5b33f920572395ea591f85395372949), XGBRegressor_2 (1651e6cc0bc873c5f37ae6eebceb2002), XGBRegressor_3 (7c033486efe2592e85d22287e98391fd), XGBRegressor_4 (7afd382667da316172eb1fc14749b8b8)\n",
      "[INFO] Finished getting all predictions\n"
     ]
    }
   ],
   "source": [
    "stacking_preds_retriever = StackingPredictionsRetriever(\n",
    "    estimators=estimators,\n",
    "    working_dir_path=\"/kaggle/working/\",\n",
    "    train_preds_filename=\"base_models_train_preds\",\n",
    "    test_preds_filename=\"base_models_test_preds\",\n",
    "    preds_save_interval=1\n",
    ")\n",
    "stacking_preds_retriever.clear_preds()\n",
    "stacking_preds_retriever.import_preds(\"/kaggle/input/predicting-student-test-scores-base-model-preds/\")\n",
    "stacking_preds_retriever.get_preds()\n",
    "\n",
    "base_model_train_preds, base_model_test_preds = stacking_preds_retriever.get_current_train_and_test_preds()\n",
    "base_model_train_preds.sort_index(axis=1, inplace=True, key=lambda index: index.map(lambda col_name: (col_name.split(\"_\")[0], int(col_name.split()[0].split(\"_\")[-1]))))\n",
    "base_model_test_preds.sort_index(axis=1, inplace=True, key=lambda index: index.map(lambda col_name: (col_name.split(\"_\")[0], int(col_name.split()[0].split(\"_\")[-1]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f9d67f",
   "metadata": {
    "papermill": {
     "duration": 0.007518,
     "end_time": "2026-01-10T13:34:37.135416",
     "exception": false,
     "start_time": "2026-01-10T13:34:37.127898",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 9.2 Base Models RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c1dfc09a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T13:34:37.152014Z",
     "iopub.status.busy": "2026-01-10T13:34:37.151336Z",
     "iopub.status.idle": "2026-01-10T13:34:37.179191Z",
     "shell.execute_reply": "2026-01-10T13:34:37.178434Z"
    },
    "papermill": {
     "duration": 0.037947,
     "end_time": "2026-01-10T13:34:37.180762",
     "exception": false,
     "start_time": "2026-01-10T13:34:37.142815",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor_2 (1651e6cc0bc873c5f37ae6eebceb2002)     8.706541\n",
       "XGBRegressor_4 (7afd382667da316172eb1fc14749b8b8)     8.706637\n",
       "XGBRegressor_5 (9f5cff03ab7df1a857f0e8efa792a24f)     8.706923\n",
       "XGBRegressor_1 (d5b33f920572395ea591f85395372949)     8.707001\n",
       "XGBRegressor_3 (7c033486efe2592e85d22287e98391fd)     8.707424\n",
       "LGBMRegressor_1 (bc61f24c868323d36835d4301ce2690f)    8.715844\n",
       "dtype: float64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model_rmse = pd.Series()\n",
    "for estimator in base_model_train_preds.columns:\n",
    "    base_model_rmse[estimator] = np.sqrt(mean_squared_error(train_data[target_col], base_model_train_preds[estimator]))\n",
    "base_model_rmse.sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b3af57",
   "metadata": {
    "papermill": {
     "duration": 0.007439,
     "end_time": "2026-01-10T13:34:37.195925",
     "exception": false,
     "start_time": "2026-01-10T13:34:37.188486",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 10. Submission (Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f33f196a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T13:34:37.214581Z",
     "iopub.status.busy": "2026-01-10T13:34:37.214311Z",
     "iopub.status.idle": "2026-01-10T13:34:37.694481Z",
     "shell.execute_reply": "2026-01-10T13:34:37.693609Z"
    },
    "papermill": {
     "duration": 0.491378,
     "end_time": "2026-01-10T13:34:37.695880",
     "exception": false,
     "start_time": "2026-01-10T13:34:37.204502",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission file prepared.\n"
     ]
    }
   ],
   "source": [
    "# prepare submission\n",
    "submission = pd.DataFrame({'id': test_data.index, target_col: base_model_test_preds['XGBRegressor_5 (9f5cff03ab7df1a857f0e8efa792a24f)']})\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print('Submission file prepared.')"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 14993753,
     "sourceId": 119082,
     "sourceType": "competition"
    },
    {
     "databundleVersionId": 15275523,
     "datasetId": 9179059,
     "sourceId": 14455088,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31234,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1933.722684,
   "end_time": "2026-01-10T13:34:40.309824",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2026-01-10T13:02:26.587140",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
