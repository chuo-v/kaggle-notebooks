{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ffa239f",
   "metadata": {
    "papermill": {
     "duration": 0.008072,
     "end_time": "2026-01-13T04:01:16.238171",
     "exception": false,
     "start_time": "2026-01-13T04:01:16.230099",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 1. Overview\n",
    "\n",
    "This is a notebook for training models to submit predictions to the \"Predicting Student Test Scores\" Kaggle competition ([playground-series-s6e1](https://www.kaggle.com/competitions/playground-series-s6e1)).\n",
    "\n",
    "Synthetic data is used for this playground competition, and the objective is to, for each student in the test set, predict a probability for the exam_score variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25269785",
   "metadata": {
    "papermill": {
     "duration": 0.00657,
     "end_time": "2026-01-13T04:01:16.251491",
     "exception": false,
     "start_time": "2026-01-13T04:01:16.244921",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 2. Setup\n",
    "\n",
    "## 2.1 Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2eda7b12",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-13T04:01:16.266203Z",
     "iopub.status.busy": "2026-01-13T04:01:16.265936Z",
     "iopub.status.idle": "2026-01-13T04:01:29.648640Z",
     "shell.execute_reply": "2026-01-13T04:01:29.647848Z"
    },
    "papermill": {
     "duration": 13.392493,
     "end_time": "2026-01-13T04:01:29.650530",
     "exception": false,
     "start_time": "2026-01-13T04:01:16.258037",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/sqlalchemy/orm/query.py:195: SyntaxWarning: \"is not\" with 'tuple' literal. Did you mean \"!=\"?\n",
      "  if entities is not ():\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import copy\n",
    "import lightgbm as lgb\n",
    "import optuna\n",
    "import os\n",
    "import hashlib as hl # for StackingEstimator\n",
    "import inspect # for StackingEstimator\n",
    "import random\n",
    "import warnings\n",
    "from catboost import CatBoostRegressor\n",
    "from enum import Enum\n",
    "from pathlib import Path # for StackingPredictionsRetriever\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from types import FunctionType\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "pd.set_option('display.max_colwidth', None) # Display full column content\n",
    "pd.set_option('display.max_rows', None) # Display all rows\n",
    "pd.set_option('display.width', 1000) # Set larger display width"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e7927d",
   "metadata": {
    "papermill": {
     "duration": 0.00693,
     "end_time": "2026-01-13T04:01:29.665483",
     "exception": false,
     "start_time": "2026-01-13T04:01:29.658553",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2.2 Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73c201f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-13T04:01:29.680580Z",
     "iopub.status.busy": "2026-01-13T04:01:29.679935Z",
     "iopub.status.idle": "2026-01-13T04:01:29.746170Z",
     "shell.execute_reply": "2026-01-13T04:01:29.745558Z"
    },
    "papermill": {
     "duration": 0.075703,
     "end_time": "2026-01-13T04:01:29.747854",
     "exception": false,
     "start_time": "2026-01-13T04:01:29.672151",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "RANDOM_SEEDS = [2, 11, 42, 99, 121]\n",
    "random.seed(RANDOM_SEEDS[0])\n",
    "np.random.seed(RANDOM_SEEDS[0])\n",
    "torch.manual_seed(RANDOM_SEEDS[0])\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(RANDOM_SEEDS[0])\n",
    "    torch.cuda.manual_seed_all(RANDOM_SEEDS[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b2eb8c",
   "metadata": {
    "papermill": {
     "duration": 0.0066,
     "end_time": "2026-01-13T04:01:29.761537",
     "exception": false,
     "start_time": "2026-01-13T04:01:29.754937",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2.3 Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5af5e23e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-13T04:01:29.775882Z",
     "iopub.status.busy": "2026-01-13T04:01:29.775585Z",
     "iopub.status.idle": "2026-01-13T04:01:29.779273Z",
     "shell.execute_reply": "2026-01-13T04:01:29.778731Z"
    },
    "papermill": {
     "duration": 0.012839,
     "end_time": "2026-01-13T04:01:29.780921",
     "exception": false,
     "start_time": "2026-01-13T04:01:29.768082",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85c085f",
   "metadata": {
    "papermill": {
     "duration": 0.006622,
     "end_time": "2026-01-13T04:01:29.794193",
     "exception": false,
     "start_time": "2026-01-13T04:01:29.787571",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2.4 DataFrames\n",
    "\n",
    "Read the data provided for the competition into dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df5c90b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-13T04:01:29.808620Z",
     "iopub.status.busy": "2026-01-13T04:01:29.808374Z",
     "iopub.status.idle": "2026-01-13T04:01:31.248445Z",
     "shell.execute_reply": "2026-01-13T04:01:31.247848Z"
    },
    "papermill": {
     "duration": 1.4494,
     "end_time": "2026-01-13T04:01:31.250215",
     "exception": false,
     "start_time": "2026-01-13T04:01:29.800815",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "INPUT_DIR = '/kaggle/input'\n",
    "orig_train_data = pd.read_csv(os.path.join(INPUT_DIR, 'playground-series-s6e1/train.csv'))\n",
    "orig_test_data = pd.read_csv(os.path.join(INPUT_DIR, 'playground-series-s6e1/test.csv'))\n",
    "\n",
    "# set index\n",
    "orig_train_data.set_index('id', inplace=True)\n",
    "orig_test_data.set_index('id', inplace=True)\n",
    "\n",
    "# target column\n",
    "target_col = \"exam_score\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3008dc0d",
   "metadata": {
    "papermill": {
     "duration": 0.007038,
     "end_time": "2026-01-13T04:01:31.264525",
     "exception": false,
     "start_time": "2026-01-13T04:01:31.257487",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 3. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2a2c3c7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-13T04:01:31.279431Z",
     "iopub.status.busy": "2026-01-13T04:01:31.278961Z",
     "iopub.status.idle": "2026-01-13T04:01:31.282264Z",
     "shell.execute_reply": "2026-01-13T04:01:31.281659Z"
    },
    "papermill": {
     "duration": 0.012386,
     "end_time": "2026-01-13T04:01:31.283599",
     "exception": false,
     "start_time": "2026-01-13T04:01:31.271213",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# to skip the generation of plots (e.g. KDE) in this section that take time; set to False to generate the plots \n",
    "SKIP_PLOTS = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2f5e522",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-13T04:01:31.298437Z",
     "iopub.status.busy": "2026-01-13T04:01:31.298000Z",
     "iopub.status.idle": "2026-01-13T04:01:31.431053Z",
     "shell.execute_reply": "2026-01-13T04:01:31.430196Z"
    },
    "papermill": {
     "duration": 0.14218,
     "end_time": "2026-01-13T04:01:31.432657",
     "exception": false,
     "start_time": "2026-01-13T04:01:31.290477",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>study_hours</th>\n",
       "      <th>class_attendance</th>\n",
       "      <th>sleep_hours</th>\n",
       "      <th>exam_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>630000.000000</td>\n",
       "      <td>630000.000000</td>\n",
       "      <td>630000.000000</td>\n",
       "      <td>630000.000000</td>\n",
       "      <td>630000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>20.545821</td>\n",
       "      <td>4.002337</td>\n",
       "      <td>71.987261</td>\n",
       "      <td>7.072758</td>\n",
       "      <td>62.506672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.260238</td>\n",
       "      <td>2.359880</td>\n",
       "      <td>17.430098</td>\n",
       "      <td>1.744811</td>\n",
       "      <td>18.916884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>17.000000</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>40.600000</td>\n",
       "      <td>4.100000</td>\n",
       "      <td>19.599000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>19.000000</td>\n",
       "      <td>1.970000</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>5.600000</td>\n",
       "      <td>48.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>21.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>72.600000</td>\n",
       "      <td>7.100000</td>\n",
       "      <td>62.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>23.000000</td>\n",
       "      <td>6.050000</td>\n",
       "      <td>87.200000</td>\n",
       "      <td>8.600000</td>\n",
       "      <td>76.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>24.000000</td>\n",
       "      <td>7.910000</td>\n",
       "      <td>99.400000</td>\n",
       "      <td>9.900000</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 age    study_hours  class_attendance    sleep_hours     exam_score\n",
       "count  630000.000000  630000.000000     630000.000000  630000.000000  630000.000000\n",
       "mean       20.545821       4.002337         71.987261       7.072758      62.506672\n",
       "std         2.260238       2.359880         17.430098       1.744811      18.916884\n",
       "min        17.000000       0.080000         40.600000       4.100000      19.599000\n",
       "25%        19.000000       1.970000         57.000000       5.600000      48.800000\n",
       "50%        21.000000       4.000000         72.600000       7.100000      62.600000\n",
       "75%        23.000000       6.050000         87.200000       8.600000      76.300000\n",
       "max        24.000000       7.910000         99.400000       9.900000     100.000000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig_train_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "591616c2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-13T04:01:31.448239Z",
     "iopub.status.busy": "2026-01-13T04:01:31.448009Z",
     "iopub.status.idle": "2026-01-13T04:01:31.493529Z",
     "shell.execute_reply": "2026-01-13T04:01:31.492856Z"
    },
    "papermill": {
     "duration": 0.054687,
     "end_time": "2026-01-13T04:01:31.494990",
     "exception": false,
     "start_time": "2026-01-13T04:01:31.440303",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>study_hours</th>\n",
       "      <th>class_attendance</th>\n",
       "      <th>sleep_hours</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>270000.000000</td>\n",
       "      <td>270000.000000</td>\n",
       "      <td>270000.000000</td>\n",
       "      <td>270000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>20.544137</td>\n",
       "      <td>4.003878</td>\n",
       "      <td>71.982509</td>\n",
       "      <td>7.072070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.260452</td>\n",
       "      <td>2.357741</td>\n",
       "      <td>17.414695</td>\n",
       "      <td>1.745513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>17.000000</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>40.600000</td>\n",
       "      <td>4.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>19.000000</td>\n",
       "      <td>1.980000</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>5.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>21.000000</td>\n",
       "      <td>4.010000</td>\n",
       "      <td>72.600000</td>\n",
       "      <td>7.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>23.000000</td>\n",
       "      <td>6.050000</td>\n",
       "      <td>87.200000</td>\n",
       "      <td>8.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>24.000000</td>\n",
       "      <td>7.910000</td>\n",
       "      <td>99.400000</td>\n",
       "      <td>9.900000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 age    study_hours  class_attendance    sleep_hours\n",
       "count  270000.000000  270000.000000     270000.000000  270000.000000\n",
       "mean       20.544137       4.003878         71.982509       7.072070\n",
       "std         2.260452       2.357741         17.414695       1.745513\n",
       "min        17.000000       0.080000         40.600000       4.100000\n",
       "25%        19.000000       1.980000         57.000000       5.600000\n",
       "50%        21.000000       4.010000         72.600000       7.100000\n",
       "75%        23.000000       6.050000         87.200000       8.600000\n",
       "max        24.000000       7.910000         99.400000       9.900000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig_test_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17d1e8ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-13T04:01:31.511844Z",
     "iopub.status.busy": "2026-01-13T04:01:31.511203Z",
     "iopub.status.idle": "2026-01-13T04:01:31.680357Z",
     "shell.execute_reply": "2026-01-13T04:01:31.679742Z"
    },
    "papermill": {
     "duration": 0.179233,
     "end_time": "2026-01-13T04:01:31.682168",
     "exception": false,
     "start_time": "2026-01-13T04:01:31.502935",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "numeric_col_names = orig_train_data.select_dtypes(include='number').columns.to_series()\n",
    "categorical_col_names = orig_train_data.select_dtypes(include='object').columns.to_series()\n",
    "assert numeric_col_names.size + categorical_col_names.size == orig_train_data.shape[1]\n",
    "\n",
    "# drop target column from numeric column names\n",
    "numeric_col_names.drop(target_col, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96aa78e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-13T04:01:31.698215Z",
     "iopub.status.busy": "2026-01-13T04:01:31.697603Z",
     "iopub.status.idle": "2026-01-13T04:01:31.976738Z",
     "shell.execute_reply": "2026-01-13T04:01:31.975841Z"
    },
    "papermill": {
     "duration": 0.288679,
     "end_time": "2026-01-13T04:01:31.978274",
     "exception": false,
     "start_time": "2026-01-13T04:01:31.689595",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Train data missing values #####\n",
      "age                 0\n",
      "gender              0\n",
      "course              0\n",
      "study_hours         0\n",
      "class_attendance    0\n",
      "internet_access     0\n",
      "sleep_hours         0\n",
      "sleep_quality       0\n",
      "study_method        0\n",
      "facility_rating     0\n",
      "exam_difficulty     0\n",
      "exam_score          0\n",
      "dtype: int64\n",
      "\n",
      "##### Test data missing values #####\n",
      "age                 0\n",
      "gender              0\n",
      "course              0\n",
      "study_hours         0\n",
      "class_attendance    0\n",
      "internet_access     0\n",
      "sleep_hours         0\n",
      "sleep_quality       0\n",
      "study_method        0\n",
      "facility_rating     0\n",
      "exam_difficulty     0\n",
      "dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for (dataset_name, dataset) in [('Train data', orig_train_data), ('Test data', orig_test_data)]:\n",
    "    print(f\"##### {dataset_name} missing values #####\")\n",
    "    print(dataset.isnull().sum())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6efd566",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-13T04:01:31.994532Z",
     "iopub.status.busy": "2026-01-13T04:01:31.994008Z",
     "iopub.status.idle": "2026-01-13T04:01:32.411016Z",
     "shell.execute_reply": "2026-01-13T04:01:32.410234Z"
    },
    "papermill": {
     "duration": 0.426644,
     "end_time": "2026-01-13T04:01:32.412422",
     "exception": false,
     "start_time": "2026-01-13T04:01:31.985778",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Train data categorical cols unique values #####\n",
      "\n",
      "[gender]\n",
      " other 33.51%\n",
      "  male 33.43%\n",
      "female 33.07%\n",
      "\n",
      "[course]\n",
      " b.tech 20.83%\n",
      "   b.sc 17.71%\n",
      "  b.com 17.61%\n",
      "    bca 14.08%\n",
      "    bba 12.01%\n",
      "     ba  9.84%\n",
      "diploma  7.92%\n",
      "\n",
      "[internet_access]\n",
      "yes 91.97%\n",
      " no  8.03%\n",
      "\n",
      "[sleep_quality]\n",
      "   poor 33.92%\n",
      "   good 33.82%\n",
      "average 32.26%\n",
      "\n",
      "[study_method]\n",
      "     coaching  20.9%\n",
      "   self-study 20.81%\n",
      "        mixed 19.54%\n",
      "  group study 19.53%\n",
      "online videos 19.22%\n",
      "\n",
      "[facility_rating]\n",
      "medium 33.98%\n",
      "   low 33.71%\n",
      "  high 32.31%\n",
      "\n",
      "[exam_difficulty]\n",
      "moderate 56.19%\n",
      "    easy 28.02%\n",
      "    hard 15.79%\n",
      "\n",
      "##### Test data categorical cols unique values #####\n",
      "\n",
      "[gender]\n",
      "  male 33.59%\n",
      " other 33.37%\n",
      "female 33.04%\n",
      "\n",
      "[course]\n",
      " b.tech 20.91%\n",
      "   b.sc 17.67%\n",
      "  b.com 17.53%\n",
      "    bca 14.11%\n",
      "    bba 11.89%\n",
      "     ba  9.95%\n",
      "diploma  7.94%\n",
      "\n",
      "[internet_access]\n",
      "yes 92.1%\n",
      " no  7.9%\n",
      "\n",
      "[sleep_quality]\n",
      "   good 33.98%\n",
      "   poor 33.96%\n",
      "average 32.06%\n",
      "\n",
      "[study_method]\n",
      "     coaching  21.0%\n",
      "   self-study 20.72%\n",
      "  group study 19.55%\n",
      "        mixed 19.52%\n",
      "online videos 19.22%\n",
      "\n",
      "[facility_rating]\n",
      "medium 34.01%\n",
      "   low 33.82%\n",
      "  high 32.17%\n",
      "\n",
      "[exam_difficulty]\n",
      "moderate 56.17%\n",
      "    easy 28.01%\n",
      "    hard 15.82%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for (dataset_name, dataset) in [('Train data', orig_train_data), ('Test data', orig_test_data)]:\n",
    "    print(f\"##### {dataset_name} categorical cols unique values #####\")\n",
    "    for categorical_col_name in categorical_col_names:\n",
    "        print()\n",
    "        print(f\"[{categorical_col_name}]\")\n",
    "        counts = dataset[categorical_col_name].value_counts(normalize=True).mul(100).round(2).astype(str).add('%')\n",
    "        print(counts.reset_index(name='counts').to_string(index=False, header=False))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "947ab9af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-13T04:01:32.428551Z",
     "iopub.status.busy": "2026-01-13T04:01:32.427971Z",
     "iopub.status.idle": "2026-01-13T04:01:32.432269Z",
     "shell.execute_reply": "2026-01-13T04:01:32.431730Z"
    },
    "papermill": {
     "duration": 0.013786,
     "end_time": "2026-01-13T04:01:32.433644",
     "exception": false,
     "start_time": "2026-01-13T04:01:32.419858",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# KDE plots of target variable and numerical features (train data)\n",
    "if not SKIP_PLOTS:\n",
    "    plt.figure(figsize=(12, 24))\n",
    "    kdeplot_col_names = [target_col]\n",
    "    kdeplot_col_names.extend(numeric_col_names)\n",
    "    for i, col in enumerate(kdeplot_col_names, start=1):\n",
    "        plt.subplot(10, 2, i)\n",
    "        sns.kdeplot(data=orig_train_data, x=col, fill=True)\n",
    "        plt.tight_layout()\n",
    "        plt.title(f\"KDE plot of {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2933273f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-13T04:01:32.449657Z",
     "iopub.status.busy": "2026-01-13T04:01:32.449039Z",
     "iopub.status.idle": "2026-01-13T04:01:32.453285Z",
     "shell.execute_reply": "2026-01-13T04:01:32.452562Z"
    },
    "papermill": {
     "duration": 0.013762,
     "end_time": "2026-01-13T04:01:32.454763",
     "exception": false,
     "start_time": "2026-01-13T04:01:32.441001",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# KDE plots of numerical features (test data)\n",
    "if not SKIP_PLOTS:\n",
    "    plt.figure(figsize=(12, 24))\n",
    "    for i, col in enumerate(numeric_col_names, start=1):\n",
    "        plt.subplot(10, 2, i)\n",
    "        sns.kdeplot(data=orig_test_data, x=col, fill=True)\n",
    "        plt.tight_layout()\n",
    "        plt.title(f\"KDE plot of {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "98bbd057",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-13T04:01:32.471275Z",
     "iopub.status.busy": "2026-01-13T04:01:32.470606Z",
     "iopub.status.idle": "2026-01-13T04:01:32.475297Z",
     "shell.execute_reply": "2026-01-13T04:01:32.474754Z"
    },
    "papermill": {
     "duration": 0.014376,
     "end_time": "2026-01-13T04:01:32.476573",
     "exception": false,
     "start_time": "2026-01-13T04:01:32.462197",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not SKIP_PLOTS:\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    corr_matrix_col_names = [target_col]\n",
    "    corr_matrix_col_names.extend(numeric_col_names)\n",
    "    sns.heatmap(\n",
    "        orig_train_data[corr_matrix_col_names].corr(),\n",
    "        cmap='Reds',\n",
    "        annot=True,\n",
    "        linewidths=2,\n",
    "        fmt='.2f',\n",
    "        vmin=-1,\n",
    "        vmax=1\n",
    "    )\n",
    "    plt.title('Correlation Matrix of Numeric Features and Target Variable', fontsize=18, pad=20)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d42cfbb",
   "metadata": {
    "papermill": {
     "duration": 0.007147,
     "end_time": "2026-01-13T04:01:32.491071",
     "exception": false,
     "start_time": "2026-01-13T04:01:32.483924",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 4. Data Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e0261020",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-13T04:01:32.507061Z",
     "iopub.status.busy": "2026-01-13T04:01:32.506801Z",
     "iopub.status.idle": "2026-01-13T04:01:32.699650Z",
     "shell.execute_reply": "2026-01-13T04:01:32.698841Z"
    },
    "papermill": {
     "duration": 0.203142,
     "end_time": "2026-01-13T04:01:32.701416",
     "exception": false,
     "start_time": "2026-01-13T04:01:32.498274",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data = orig_train_data.copy()\n",
    "test_data = orig_test_data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db195c97",
   "metadata": {
    "papermill": {
     "duration": 0.007452,
     "end_time": "2026-01-13T04:01:32.716715",
     "exception": false,
     "start_time": "2026-01-13T04:01:32.709263",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 4.1 Ordinal Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "63690dc6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-13T04:01:32.733181Z",
     "iopub.status.busy": "2026-01-13T04:01:32.732563Z",
     "iopub.status.idle": "2026-01-13T04:01:32.914598Z",
     "shell.execute_reply": "2026-01-13T04:01:32.913966Z"
    },
    "papermill": {
     "duration": 0.192278,
     "end_time": "2026-01-13T04:01:32.916409",
     "exception": false,
     "start_time": "2026-01-13T04:01:32.724131",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ordinal_mappings = {\n",
    "    'sleep_quality':   {'poor': 0, 'average': 1, 'good': 2},\n",
    "    'facility_rating': {'low': 0, 'medium': 1, 'high': 2},\n",
    "    'exam_difficulty': {'easy': 0, 'moderate': 1, 'hard': 2},\n",
    "    'internet_access': {'no': 0, 'yes': 1}\n",
    "}\n",
    "for col, mapping in ordinal_mappings.items():\n",
    "    train_data[col] = train_data[col].map(mapping)\n",
    "    test_data[col]  = test_data[col].map(mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956a0222",
   "metadata": {
    "papermill": {
     "duration": 0.00714,
     "end_time": "2026-01-13T04:01:32.931216",
     "exception": false,
     "start_time": "2026-01-13T04:01:32.924076",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 4.2 Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8710c0b9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-13T04:01:32.947097Z",
     "iopub.status.busy": "2026-01-13T04:01:32.946800Z",
     "iopub.status.idle": "2026-01-13T04:01:32.952575Z",
     "shell.execute_reply": "2026-01-13T04:01:32.951963Z"
    },
    "papermill": {
     "duration": 0.015624,
     "end_time": "2026-01-13T04:01:32.953996",
     "exception": false,
     "start_time": "2026-01-13T04:01:32.938372",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_generated_features(df):\n",
    "    # polynomial\n",
    "    for col in ['study_hours', 'class_attendance']:\n",
    "        df[f'{col}_sq'] = df[col] ** 2\n",
    "    df['sleep_parabola'] = (df['sleep_hours'] - 7.5) ** 2\n",
    "\n",
    "    # log\n",
    "    for col in ['study_hours']:\n",
    "        df[f'{col}_log'] = np.log1p(df[col])\n",
    "\n",
    "    # interactions\n",
    "    df['study_hours_x_class_attendance'] = df['study_hours'] * df['class_attendance'] # total dedication\n",
    "    df['study_hours_x_sleep_hours'] = df['study_hours'] * df['sleep_hours']\n",
    "    df['study_hours_x_sleep_quality'] = df['study_hours'] * df['sleep_quality']\n",
    "    df['study_hours_x_exam_difficulty'] = df['study_hours'] * df['exam_difficulty']\n",
    "    df['study_hours_x_internet_access'] = df['study_hours'] * df['internet_access']\n",
    "    df['sleep_hours_x_sleep_quality'] = df['sleep_hours'] * df['sleep_quality']\n",
    "    df['class_attendance_x_exam_difficulty'] = df['class_attendance'] * df['exam_difficulty']\n",
    "    df['class_attendance_x_facility_rating'] = df['class_attendance'] * df['facility_rating']\n",
    "\n",
    "    # ratios\n",
    "    df['study_hours_per_class_attendance'] = df['study_hours'] / (df['class_attendance'] + 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "399899c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-13T04:01:32.970014Z",
     "iopub.status.busy": "2026-01-13T04:01:32.969713Z",
     "iopub.status.idle": "2026-01-13T04:01:33.037085Z",
     "shell.execute_reply": "2026-01-13T04:01:33.036486Z"
    },
    "papermill": {
     "duration": 0.077451,
     "end_time": "2026-01-13T04:01:33.038816",
     "exception": false,
     "start_time": "2026-01-13T04:01:32.961365",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# add generated features\n",
    "add_generated_features(train_data)\n",
    "add_generated_features(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c6601b15",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-13T04:01:33.055438Z",
     "iopub.status.busy": "2026-01-13T04:01:33.055172Z",
     "iopub.status.idle": "2026-01-13T04:01:33.059859Z",
     "shell.execute_reply": "2026-01-13T04:01:33.059220Z"
    },
    "papermill": {
     "duration": 0.014219,
     "end_time": "2026-01-13T04:01:33.061178",
     "exception": false,
     "start_time": "2026-01-13T04:01:33.046959",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['age', 'gender', 'course', 'study_hours', 'class_attendance', 'internet_access', 'sleep_hours', 'sleep_quality', 'study_method', 'facility_rating', 'exam_difficulty', 'exam_score', 'study_hours_sq', 'class_attendance_sq', 'sleep_parabola', 'study_hours_log', 'study_hours_x_class_attendance', 'study_hours_x_sleep_hours', 'study_hours_x_sleep_quality', 'study_hours_x_exam_difficulty', 'study_hours_x_internet_access', 'sleep_hours_x_sleep_quality', 'class_attendance_x_exam_difficulty', 'class_attendance_x_facility_rating', 'study_hours_per_class_attendance'], dtype='object')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d5c73bf3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-13T04:01:33.076560Z",
     "iopub.status.busy": "2026-01-13T04:01:33.076320Z",
     "iopub.status.idle": "2026-01-13T04:01:33.081454Z",
     "shell.execute_reply": "2026-01-13T04:01:33.080923Z"
    },
    "papermill": {
     "duration": 0.014368,
     "end_time": "2026-01-13T04:01:33.082740",
     "exception": false,
     "start_time": "2026-01-13T04:01:33.068372",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['age', 'gender', 'course', 'study_hours', 'class_attendance', 'internet_access', 'sleep_hours', 'sleep_quality', 'study_method', 'facility_rating', 'exam_difficulty', 'study_hours_sq', 'class_attendance_sq', 'sleep_parabola', 'study_hours_log', 'study_hours_x_class_attendance', 'study_hours_x_sleep_hours', 'study_hours_x_sleep_quality', 'study_hours_x_exam_difficulty', 'study_hours_x_internet_access', 'sleep_hours_x_sleep_quality', 'class_attendance_x_exam_difficulty', 'class_attendance_x_facility_rating', 'study_hours_per_class_attendance'], dtype='object')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e4365d",
   "metadata": {
    "papermill": {
     "duration": 0.011449,
     "end_time": "2026-01-13T04:01:33.102238",
     "exception": false,
     "start_time": "2026-01-13T04:01:33.090789",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 4.4 Remaining Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "52f26b07",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-13T04:01:33.120722Z",
     "iopub.status.busy": "2026-01-13T04:01:33.120088Z",
     "iopub.status.idle": "2026-01-13T04:01:33.470967Z",
     "shell.execute_reply": "2026-01-13T04:01:33.470220Z"
    },
    "papermill": {
     "duration": 0.362094,
     "end_time": "2026-01-13T04:01:33.472449",
     "exception": false,
     "start_time": "2026-01-13T04:01:33.110355",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gender', 'course', 'study_method']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_features = train_data.drop(target_col, axis=1).select_dtypes(include='object').columns.to_list()\n",
    "if len(cat_features) > 0:\n",
    "    for col in cat_features:\n",
    "        train_data[col] = train_data[col].astype('category')\n",
    "        test_data[col] = test_data[col].astype('category')\n",
    "cat_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8567b408",
   "metadata": {
    "papermill": {
     "duration": 0.00739,
     "end_time": "2026-01-13T04:01:33.487426",
     "exception": false,
     "start_time": "2026-01-13T04:01:33.480036",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 5. Stacking Initial Setup\n",
    "\n",
    "We'll use stacking, an [ensemble learning](https://en.wikipedia.org/wiki/Ensemble_learning) strategy, to generate the predictions. As we'll need to gather predictions from various base models (a.k.a. level-0 models) to feed as input features to a meta model (a.k.a. level-1 model), in order to streamline the process of experimenting with different combinations of base models, some helper classes will be defined in this section. These classes can also be found [here](https://github.com/chuo-v/machine-learning-utils/blob/master/ensemble-learning/stacking/stacking_predictions_retriever.py) at one of my GitHub repositories used to organize some utilities I implemented for machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7719be8b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-13T04:01:33.504021Z",
     "iopub.status.busy": "2026-01-13T04:01:33.503735Z",
     "iopub.status.idle": "2026-01-13T04:01:33.530563Z",
     "shell.execute_reply": "2026-01-13T04:01:33.529835Z"
    },
    "papermill": {
     "duration": 0.037378,
     "end_time": "2026-01-13T04:01:33.532060",
     "exception": false,
     "start_time": "2026-01-13T04:01:33.494682",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class StackingEstimator:\n",
    "    \"\"\"\n",
    "    A class representing an estimator that will be used for stacking, an ensemble learning strategy.\n",
    "\n",
    "    Intended to be used in conjunction with the `StackingPredictionsRetriever` class, which helps\n",
    "    retrieve predictions for multiple instances of `StackingEstimator`; as the predictions are saved\n",
    "    in files, on subsequent requests to retrieve predictions, even as the set of estimators has been\n",
    "    modified, the `StackingPredictionsRetriever` class can determine the predictions of estimators\n",
    "    that are non-stale and available (if any) by using the `get_hash` method of the `StackingEstimator`\n",
    "    class to determine the relevance and staleness of any saved predictions.\n",
    "\n",
    "    Proper usage of this class requires one important condition to be satisfied: the predictions made\n",
    "    using the estimator are determinstic, i.e. they are exactly the same everytime the estimator is\n",
    "    run with the same inputs (`name`, `params_dict`, `feature_names`, `get_predictions`).\n",
    "    \"\"\"\n",
    "    name = \"\"\n",
    "    params_dict = {}\n",
    "    feature_names = []\n",
    "    get_predictions = lambda: None\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        name: str,\n",
    "        feature_names: [str],\n",
    "        params_dict: {},\n",
    "        get_preds: FunctionType\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializes a new instance of `StackingEstimator`.\n",
    "\n",
    "        :param name:\n",
    "            A string representing a name for the estimator. It is used for the column names of\n",
    "            the training and test predictions for each estimator, and is also used as an input\n",
    "            to calculate a hash value for the estimator. It is recommended to use a different\n",
    "            name from the names used for other estimators passed to `StackingPredictionsRetriever`.\n",
    "        :param feature_names:\n",
    "            A list of strings representing the names of the features that will be used for the\n",
    "            estimator. It will be passed as an argument to `get_preds`. Internally, it is only\n",
    "            used as an input to calculate a hash value for the estimator.\n",
    "        :param params_dict:\n",
    "            A dictionary of parameters that will be specified for the estimator. It will be\n",
    "            passed as an argument to `get_preds`. Internally, it is only used as an input\n",
    "            to calculate a hash value for the estimator.\n",
    "        :param get_preds:\n",
    "            A function for getting the predictions for the estimator. It should only take two\n",
    "            arguments: 'params_dict' and 'feature_names', and should return predictions for\n",
    "            the training and test data (in that order) as a tuple of two `pandas.Series`.\n",
    "        \"\"\"\n",
    "        # parameter check\n",
    "        if not isinstance(name, str):\n",
    "            raise ValueError(\"`name` argument should be of type `str`\")\n",
    "        if not isinstance(feature_names, list):\n",
    "            raise ValueError(f\"`feature_names` argument for estimator \\\"{name}\\\" should be of type `list`\")\n",
    "        elif not all(isinstance(feature_name, str) for feature_name in feature_names):\n",
    "            raise ValueError(f\"`feature_names` argument for estimator \\\"{name}\\\" should only contain instances of `str`\")\n",
    "        if not isinstance(params_dict, dict):\n",
    "            raise ValueError(f\"`params_dict` argument for estimator \\\"{name}\\\" should be of type `dict`\")\n",
    "        get_preds_params = inspect.signature(get_preds).parameters.values()\n",
    "        get_preds_param_names = [param.name for param in get_preds_params]\n",
    "        if len(get_preds_param_names) != 2:\n",
    "            raise ValueError(f\"`get_preds` function for estimator \\\"{name}\\\" should take two arguments\")\n",
    "        elif \"params_dict\" not in get_preds_param_names:\n",
    "            raise ValueError(f\"`get_preds` function for estimator \\\"{name}\\\" should take a \\\"params_dict\\\" argument\")\n",
    "        elif \"feature_names\" not in get_preds_param_names:\n",
    "            raise ValueError(f\"`get_preds` function for estimator \\\"{name}\\\" should take a \\\"feature_names\\\" argument\")\n",
    "\n",
    "        self.name = name\n",
    "        self.feature_names = feature_names\n",
    "        self.params_dict = params_dict\n",
    "        self.get_preds = get_preds\n",
    "\n",
    "    def get_hash_value(self):\n",
    "        \"\"\"\n",
    "        Calculates and returns a hash value for the estimator using\n",
    "        `name`, `feature_names` and `params_dict` as inputs.\n",
    "        \"\"\"\n",
    "        feature_names_str = \"_\".join(sorted(self.feature_names))\n",
    "        params_dict_str = \"_\".join(f\"{key}-{value}\" for (key, value) in sorted(self.params_dict.items()))\n",
    "        hash_input_str = \"_\".join([self.name, feature_names_str, params_dict_str])\n",
    "        md5_hash = hl.md5(hash_input_str.encode('utf-8')).hexdigest()\n",
    "        return md5_hash\n",
    "\n",
    "class StackingPredictionsRetriever:\n",
    "    \"\"\"\n",
    "    A class for streamlining stacking (an ensemble learning strategy) that saves predictions\n",
    "    from estimators to file so that when trying out different combinations of (base) estimators,\n",
    "    the predictions that are not stale can be reused, saving the time of having the estimators\n",
    "    make predictions again.\n",
    "\n",
    "    Intended to be used in conjunction with the `StackingEstimator` class. The `hash_value` of\n",
    "    `StackingEstimator` is used to determine the staleness and relevance of the predictions for\n",
    "    an estimator. The implementation for making predictions using an estimator needs to be\n",
    "    provided as a function to `get_preds` for `StackingEstimator`; when predictions need to be\n",
    "    made using an estimator, this class will call `get_preds` for the `StackingEstimator` instance.\n",
    "\n",
    "    Proper usage of this class requires one important condition to be satisfied: the predictions made\n",
    "    using the estimators are determinstic, i.e. they are exactly the same everytime a\n",
    "    `StackingEstimator` instance is run with the same inputs.\n",
    "    \"\"\"\n",
    "    estimators = []\n",
    "    working_dir_path = \"\"\n",
    "    train_preds_filename = \"\"\n",
    "    test_preds_filename = \"\"\n",
    "    preds_save_interval = 0\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        estimators: [StackingEstimator],\n",
    "        working_dir_path: str,\n",
    "        train_preds_filename: str = \"train_preds\",\n",
    "        test_preds_filename: str = \"test_preds\",\n",
    "        preds_save_interval: int = 5,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializes a new instance of `StackingPredictionsRetriever`.\n",
    "\n",
    "        :param estimators:\n",
    "            A list of `StackingEstimator` instances for which the class will retrieve predictions.\n",
    "        :param working_dir_path:\n",
    "            The path for the working directory where the files with predictions will be saved.\n",
    "        :param train_preds_filename:\n",
    "            The name of the file in which predictions for the training set will be stored.\n",
    "        :param test_preds_filename:\n",
    "            The name of the file in which predictions for the test set will be stored.\n",
    "        :param preds_save_interval:\n",
    "            An integer which specifies the interval at which predictions will be saved when\n",
    "            `get_preds` is called, corresponding to the number of estimators whose predictions\n",
    "            have been retrieved since the predictions were previously saved. Any estimators\n",
    "            whose predictions are not stale and therefore were not required to make predictions\n",
    "            again are not included in this number.\n",
    "        \"\"\"\n",
    "        # parameter check\n",
    "        if not isinstance(estimators, list):\n",
    "            raise ValueError(\"`estimators` must be passed as a list\")\n",
    "        if not all(isinstance(e, StackingEstimator) for e in estimators):\n",
    "            raise ValueError(\"`estimators` should only contain instances of `StackingEstimator`\")\n",
    "        if not isinstance(working_dir_path, str):\n",
    "            raise ValueError(\"`working_dir_path` argument should be of type `str`\")\n",
    "        if not isinstance(preds_save_interval, int):\n",
    "            raise ValueError(\"`preds_save_interval` argument should be of type `int`\")\n",
    "\n",
    "        self.estimators = estimators\n",
    "        self.working_dir_path = working_dir_path\n",
    "        self.train_preds_filename = train_preds_filename\n",
    "        self.test_preds_filename = test_preds_filename\n",
    "        self.preds_save_interval = preds_save_interval\n",
    "\n",
    "    def get_train_preds_file_path(self):\n",
    "        \"\"\"\n",
    "        Returns the file path for storing predictions for training data.\n",
    "        \"\"\"\n",
    "        return Path(f\"{self.working_dir_path}/{self.train_preds_filename}.csv\")\n",
    "\n",
    "    def get_test_preds_file_path(self):\n",
    "        \"\"\"\n",
    "        Returns the file path for storing predictions for test data.\n",
    "        \"\"\"\n",
    "        return Path(f\"{self.working_dir_path}/{self.test_preds_filename}.csv\")\n",
    "\n",
    "    def get_current_train_and_test_preds(self):\n",
    "        \"\"\"\n",
    "        Returns the current predictions for training and test data (in that order)\n",
    "        as a tuple of two `pandas.DataFrame`.\n",
    "\n",
    "        The predictions are attempted to be retrieved from the file paths returned\n",
    "        by `get_train_preds_file_path` and `get_test_preds_file_path`; if there are\n",
    "        any issues with doing so (e.g. file does not exist, dataframe is empty),\n",
    "        empty dataframes will be returned instead.\n",
    "        In the case an `pandas.errors.EmptyDataError` exception is raised when\n",
    "        reading from a file, the corresponding file will be removed.\n",
    "        \"\"\"\n",
    "        curr_train_preds = pd.DataFrame()\n",
    "        curr_test_preds = pd.DataFrame()\n",
    "        train_preds_file_path = self.get_train_preds_file_path()\n",
    "        test_preds_file_path = self.get_test_preds_file_path()\n",
    "\n",
    "        if train_preds_file_path.is_file():\n",
    "            try:\n",
    "                curr_train_preds = pd.read_csv(train_preds_file_path)\n",
    "            except pd.errors.EmptyDataError:\n",
    "                train_preds_file_path.unlink()\n",
    "        if test_preds_file_path.is_file():\n",
    "            try:\n",
    "                curr_test_preds = pd.read_csv(test_preds_file_path)\n",
    "            except pd.errors.EmptyDataError:\n",
    "                test_preds_file_path.unlink()\n",
    "\n",
    "        return curr_train_preds, curr_test_preds\n",
    "\n",
    "    def get_preds(self):\n",
    "        \"\"\"\n",
    "        Retrieves predictions from all estimators in `estimators`, storing them in\n",
    "        two files at the file paths specified by `working_dir_path`,\n",
    "        `train_preds_filename` and `test_preds_filename`.\n",
    "\n",
    "        If non-stale (relevant) predictions are found for an estimator, retrieval\n",
    "        of predictions by calling `get_preds` on the estimator will be skipped,\n",
    "        and the existing predictions for the estimator will be kept.\n",
    "        \"\"\"\n",
    "        print(\"[INFO] Getting predictions..\")\n",
    "        curr_train_preds, curr_test_preds = self.get_current_train_and_test_preds()\n",
    "\n",
    "        preds_retrieved_count = 0\n",
    "        num_preds_retrieved_but_not_yet_saved = 0\n",
    "        estimators_skipped = []\n",
    "\n",
    "        for estimator in self.estimators:\n",
    "            estimator_hash_value = estimator.get_hash_value()\n",
    "            estimator_name = f\"{estimator.name} ({estimator_hash_value})\"\n",
    "\n",
    "            # skip retrieving predictions for estimator if non-stale predictions are already available\n",
    "            train_preds_available = any(estimator_hash_value in col_name for col_name in curr_train_preds.columns)\n",
    "            test_preds_available = any(estimator_hash_value in col_name for col_name in curr_test_preds.columns)\n",
    "            if train_preds_available and test_preds_available:\n",
    "                estimators_skipped += [estimator_name]\n",
    "                continue\n",
    "\n",
    "            print(f\"[INFO] Getting predictions for estimator {estimator_name}\")\n",
    "            train_preds, test_preds = estimator.get_preds(estimator.params_dict, estimator.feature_names)\n",
    "            if not isinstance(train_preds, pd.core.series.Series):\n",
    "                raise ValueError(\"`train_preds` should be of type `pandas.Series`\")\n",
    "            if not isinstance(test_preds, pd.core.series.Series):\n",
    "                raise ValueError(\"`test_preds` should be of type `pandas.Series`\")\n",
    "            curr_train_preds[estimator_name] = train_preds\n",
    "            curr_test_preds[estimator_name] = test_preds\n",
    "            preds_retrieved_count += 1\n",
    "\n",
    "            # save predictions at an interval of `preds_save_interval`\n",
    "            if preds_retrieved_count % self.preds_save_interval == 0:\n",
    "                curr_train_preds.sort_index(axis=1).to_csv(self.get_train_preds_file_path(), index=False)\n",
    "                curr_test_preds.sort_index(axis=1).to_csv(self.get_test_preds_file_path(), index=False)\n",
    "                num_preds_retrieved_but_not_yet_saved = 0\n",
    "                print(\"[INFO] Saved predictions\")\n",
    "            else:\n",
    "                num_preds_retrieved_but_not_yet_saved += 1\n",
    "\n",
    "        if estimators_skipped:\n",
    "            estimators_skipped.sort()\n",
    "            formatted_estimators = \", \".join(estimators_skipped)\n",
    "            print(f\"[INFO] Skipped retrieving predictions for following estimators as their current ones are not stale:\\n{formatted_estimators}\")\n",
    "\n",
    "        if num_preds_retrieved_but_not_yet_saved != 0:\n",
    "            curr_train_preds.sort_index(axis=1).to_csv(self.get_train_preds_file_path(), index=False)\n",
    "            curr_test_preds.sort_index(axis=1).to_csv(self.get_test_preds_file_path(), index=False)\n",
    "            print(\"[INFO] Saved predictions\")\n",
    "\n",
    "        print(\"[INFO] Finished getting all predictions\")\n",
    "\n",
    "    def sync_preds(self):\n",
    "        \"\"\"\n",
    "        Syncs the predictions stored at the two file paths specified by\n",
    "        `working_dir_path`, `train_preds_filename` and `test_preds_filename` by\n",
    "        removing predictions for any estimator that is not currently in `estimators`.\n",
    "\n",
    "        Note that new predictions for estimators that do not currently have predictions\n",
    "        in the files will not be added; `get_preds` should be used for this purpose\n",
    "        instead.\n",
    "        \"\"\"\n",
    "        print(\"[INFO] Syncing predictions..\")\n",
    "        estimator_hash_values = [estimator.get_hash_value() for estimator in self.estimators]\n",
    "        should_remove_col = lambda col_name: not any(hash_value in col_name for hash_value in estimator_hash_values)\n",
    "\n",
    "        curr_train_preds, curr_test_preds = self.get_current_train_and_test_preds()\n",
    "\n",
    "        if not curr_train_preds.empty:\n",
    "            col_names_to_remove = [col_name for col_name in curr_train_preds.columns if should_remove_col(col_name)]\n",
    "            if col_names_to_remove:\n",
    "                print(f\"[INFO] Dropping columns for following estimators from training predictions:\\n{col_names_to_remove}\")\n",
    "                curr_train_preds.drop(columns=col_names_to_remove, inplace=True)\n",
    "                curr_train_preds.to_csv(self.get_train_preds_file_path(), index=False)\n",
    "            else:\n",
    "                print(f\"[INFO] No columns for training predictions were dropped\")\n",
    "        if not curr_test_preds.empty:\n",
    "            col_names_to_remove = [col_name for col_name in curr_test_preds.columns if should_remove_col(col_name)]\n",
    "            if col_names_to_remove:\n",
    "                print(f\"[INFO] Dropping columns for following estimators from test predictions:\\n{col_names_to_remove}\")\n",
    "                curr_test_preds.drop(columns=col_names_to_remove, inplace=True)\n",
    "                curr_test_preds.to_csv(self.get_test_preds_file_path(), index=False)\n",
    "            else:\n",
    "                print(f\"[INFO] No columns for test predictions were dropped\")\n",
    "\n",
    "        print(\"[INFO] Finished syncing predictions\")\n",
    "\n",
    "    def import_preds(self, input_dir_path):\n",
    "        \"\"\"\n",
    "        Imports predictions stored at the two file paths at `input_dir_path` with\n",
    "        `train_preds_filename` and `test_preds_filename` as their filenames. If no\n",
    "        such files are found, no predictions will be imported.\n",
    "\n",
    "        Only predictions for estimators specified in `estimators` will be imported.\n",
    "        Any predictions for estimators that were already available will be overwritten\n",
    "        with predictions for the same estimators found in the files at `input_dir_path`.\n",
    "\n",
    "        :param input_dir_path:\n",
    "            The path to the directory for the training and test predictions files.\n",
    "            The file names are expected to be the same as `train_preds_filename`\n",
    "            and `test_preds_filename`\n",
    "        \"\"\"\n",
    "        print(\"[INFO] Importing predictions..\")\n",
    "        curr_train_preds, curr_test_preds = self.get_current_train_and_test_preds()\n",
    "        input_train_preds = pd.DataFrame()\n",
    "        input_test_preds = pd.DataFrame()\n",
    "\n",
    "        input_train_preds_path = Path(f\"{input_dir_path}/{self.train_preds_filename}.csv\")\n",
    "        input_test_preds_path = Path(f\"{input_dir_path}/{self.test_preds_filename}.csv\")\n",
    "        if input_train_preds_path.is_file():\n",
    "            try:\n",
    "                input_train_preds = pd.read_csv(input_train_preds_path)\n",
    "            except: pass\n",
    "        if input_test_preds_path.is_file():\n",
    "            try:\n",
    "                input_test_preds = pd.read_csv(input_test_preds_path)\n",
    "            except: pass\n",
    "\n",
    "        estimators_with_imported_train_preds = []\n",
    "        estimators_with_imported_test_preds = []\n",
    "        for estimator in self.estimators:\n",
    "            estimator_hash_value = estimator.get_hash_value()\n",
    "            estimator_name = f\"{estimator.name} ({estimator_hash_value})\"\n",
    "            train_preds_available = any(estimator_hash_value in col_name for col_name in input_train_preds.columns)\n",
    "            test_preds_available = any(estimator_hash_value in col_name for col_name in input_test_preds.columns)\n",
    "\n",
    "            if train_preds_available:\n",
    "                curr_train_preds[estimator_name] = input_train_preds[estimator_name]\n",
    "                estimators_with_imported_train_preds += [estimator_name]\n",
    "            if test_preds_available:\n",
    "                curr_test_preds[estimator_name] = input_test_preds[estimator_name]\n",
    "                estimators_with_imported_test_preds += [estimator_name]\n",
    "\n",
    "        if not estimators_with_imported_train_preds:\n",
    "            print(\"[INFO] No train predictions were imported\")\n",
    "        else:\n",
    "            curr_train_preds.sort_index(axis=1).to_csv(self.get_train_preds_file_path(), index=False)\n",
    "            formatted_estimators = \", \".join(estimators_with_imported_train_preds)\n",
    "            print(f\"[INFO] {len(estimators_with_imported_train_preds)} train predictions were imported:\\n{formatted_estimators}\")\n",
    "        if not estimators_with_imported_test_preds:\n",
    "            print(\"[INFO] No test predictions were imported\")\n",
    "        else:\n",
    "            curr_test_preds.sort_index(axis=1).to_csv(self.get_test_preds_file_path(), index=False)\n",
    "            formatted_estimators = \", \".join(estimators_with_imported_test_preds)\n",
    "            print(f\"[INFO] {len(estimators_with_imported_test_preds)} test predictions were imported:\\n{formatted_estimators}\")\n",
    "        \n",
    "        print(\"[INFO] Finished importing predictions\")\n",
    "\n",
    "    def clear_preds(self):\n",
    "        \"\"\"\n",
    "        Removes all stored predictions by deleting the two files at filepaths specified\n",
    "        by `working_dir_path`, `train_preds_filename` and `test_preds_filename`.\n",
    "        \"\"\"\n",
    "        train_preds_file_path = self.get_train_preds_file_path()\n",
    "        test_preds_file_path = self.get_test_preds_file_path()\n",
    "\n",
    "        if train_preds_file_path.is_file():\n",
    "            train_preds_file_path.unlink()\n",
    "        if test_preds_file_path.is_file():\n",
    "            test_preds_file_path.unlink()\n",
    "\n",
    "        print(\"[INFO] Finished clearing predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2584488b",
   "metadata": {
    "papermill": {
     "duration": 0.007263,
     "end_time": "2026-01-13T04:01:33.546998",
     "exception": false,
     "start_time": "2026-01-13T04:01:33.539735",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Next, we'll create a variable for storing the estimators (`StackingEstimator` instances) that we'll pass to the `StackingPredictionsRetriever` class for getting all the predictions from our base models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "df1df0ec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-13T04:01:33.562983Z",
     "iopub.status.busy": "2026-01-13T04:01:33.562665Z",
     "iopub.status.idle": "2026-01-13T04:01:33.565917Z",
     "shell.execute_reply": "2026-01-13T04:01:33.565313Z"
    },
    "papermill": {
     "duration": 0.012941,
     "end_time": "2026-01-13T04:01:33.567267",
     "exception": false,
     "start_time": "2026-01-13T04:01:33.554326",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "estimators = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2481ffc",
   "metadata": {
    "papermill": {
     "duration": 0.007846,
     "end_time": "2026-01-13T04:01:33.582523",
     "exception": false,
     "start_time": "2026-01-13T04:01:33.574677",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 6. Feature Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b63a777a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-13T04:01:33.598684Z",
     "iopub.status.busy": "2026-01-13T04:01:33.598394Z",
     "iopub.status.idle": "2026-01-13T04:01:33.602657Z",
     "shell.execute_reply": "2026-01-13T04:01:33.601966Z"
    },
    "papermill": {
     "duration": 0.014268,
     "end_time": "2026-01-13T04:01:33.604194",
     "exception": false,
     "start_time": "2026-01-13T04:01:33.589926",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "FEATURE_SET_1 = [\n",
    "    'gender', 'course', 'study_hours',\n",
    "    'class_attendance', 'internet_access', 'sleep_hours',\n",
    "    'sleep_quality', 'study_method', 'facility_rating',\n",
    "    'exam_difficulty', 'study_hours_sq', 'class_attendance_sq',\n",
    "    'sleep_parabola', 'study_hours_log', 'study_hours_x_class_attendance',\n",
    "    'study_hours_x_sleep_hours', 'study_hours_x_sleep_quality', 'study_hours_x_exam_difficulty',\n",
    "    'study_hours_x_internet_access', 'sleep_hours_x_sleep_quality', 'class_attendance_x_exam_difficulty',\n",
    "    'class_attendance_x_facility_rating', 'study_hours_per_class_attendance',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e7adac",
   "metadata": {
    "papermill": {
     "duration": 0.007196,
     "end_time": "2026-01-13T04:01:33.618674",
     "exception": false,
     "start_time": "2026-01-13T04:01:33.611478",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 7. Base Model Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0f9cf864",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-13T04:01:33.634439Z",
     "iopub.status.busy": "2026-01-13T04:01:33.634167Z",
     "iopub.status.idle": "2026-01-13T04:01:33.637596Z",
     "shell.execute_reply": "2026-01-13T04:01:33.637052Z"
    },
    "papermill": {
     "duration": 0.013098,
     "end_time": "2026-01-13T04:01:33.639031",
     "exception": false,
     "start_time": "2026-01-13T04:01:33.625933",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# to skip hyperparameter tuning when it's not needed; set to `False` to do the tuning\n",
    "SKIP_BASE_MODEL_HYPERPARAMETER_TUNING = False\n",
    "\n",
    "# value set for early stopping for base models that support it; this value will be used for actual model training as well\n",
    "BASE_MODEL_EARLY_STOPPING_ROUNDS = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6d80ae17",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-13T04:01:33.655486Z",
     "iopub.status.busy": "2026-01-13T04:01:33.655242Z",
     "iopub.status.idle": "2026-01-13T04:01:33.658845Z",
     "shell.execute_reply": "2026-01-13T04:01:33.658277Z"
    },
    "papermill": {
     "duration": 0.01391,
     "end_time": "2026-01-13T04:01:33.660258",
     "exception": false,
     "start_time": "2026-01-13T04:01:33.646348",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BaseModelOptunaStudyEstimator(Enum):\n",
    "    CATBOOSTREGRESSOR = 'CatBoostRegressor'\n",
    "    LGBMREGRESSOR = 'LGBMRegressor'\n",
    "    XGBREGRESSOR = 'XGBRegressor'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "65ae4f66",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-13T04:01:33.676610Z",
     "iopub.status.busy": "2026-01-13T04:01:33.676310Z",
     "iopub.status.idle": "2026-01-13T04:01:33.680088Z",
     "shell.execute_reply": "2026-01-13T04:01:33.679503Z"
    },
    "papermill": {
     "duration": 0.013638,
     "end_time": "2026-01-13T04:01:33.681428",
     "exception": false,
     "start_time": "2026-01-13T04:01:33.667790",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# estimator to use for Optuna study\n",
    "BASE_MODEL_OPTUNA_STUDY_ESTIMATOR = BaseModelOptunaStudyEstimator.XGBREGRESSOR\n",
    "\n",
    "# feature set to use for Optuna study\n",
    "BASE_MODEL_OPTUNA_STUDY_FEATURE_SET = FEATURE_SET_1\n",
    "\n",
    "# maximum number of trials Optuna will conduct for the optimization\n",
    "BASE_MODEL_OPTUNA_STUDY_NUM_TRIALS = 200\n",
    "\n",
    "# number of splits to use for K-Fold Cross-Validation for Optuna study\n",
    "BASE_MODEL_OPTUNA_STUDY_KFOLD_NUM_SPLITS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "129b0bfa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-13T04:01:33.698476Z",
     "iopub.status.busy": "2026-01-13T04:01:33.698167Z",
     "iopub.status.idle": "2026-01-13T04:01:33.714222Z",
     "shell.execute_reply": "2026-01-13T04:01:33.713534Z"
    },
    "papermill": {
     "duration": 0.02657,
     "end_time": "2026-01-13T04:01:33.715590",
     "exception": false,
     "start_time": "2026-01-13T04:01:33.689020",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_base_model_optuna_params(trial, study_estimator):\n",
    "    if study_estimator == BaseModelOptunaStudyEstimator.CATBOOSTREGRESSOR:\n",
    "        if BASE_MODEL_OPTUNA_STUDY_FEATURE_SET == FEATURE_SET_1:\n",
    "            return {\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 0.010, 0.015),\n",
    "                'depth': trial.suggest_categorical('depth', [6]),\n",
    "                'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 2.0, 4.0, log=True),\n",
    "                'random_strength': trial.suggest_float('random_strength', 1.0, 2.5),\n",
    "                'colsample_bylevel': trial.suggest_float('colsample_bylevel', 0.2, 0.3),\n",
    "                'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 85, 105),\n",
    "                'bagging_temperature': trial.suggest_float('bagging_temperature', 0.0, 0.2),\n",
    "            }\n",
    "        else:\n",
    "            raise ValueError(f\"Search space for feature set for Optuna study not yet specified for CatBoostRegressor.\")\n",
    "    elif study_estimator == BaseModelOptunaStudyEstimator.LGBMREGRESSOR:\n",
    "        if BASE_MODEL_OPTUNA_STUDY_FEATURE_SET == FEATURE_SET_1:\n",
    "            return {\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 0.0045, 0.0065),\n",
    "                'num_leaves': trial.suggest_int('num_leaves', 85, 105),\n",
    "                'min_child_samples': trial.suggest_int('min_child_samples', 5, 15),\n",
    "                'min_child_weight': trial.suggest_float('min_child_weight', 1e-3, 10.0, log=True),\n",
    "                'min_split_gain': trial.suggest_float('min_split_gain', 1e-3, 0.1, log=True),\n",
    "                'subsample': trial.suggest_float('subsample', 0.85, 1.0),\n",
    "                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.15, 0.25),\n",
    "                'reg_alpha': trial.suggest_float('reg_alpha', 1.0, 10.0, log=True),\n",
    "                'reg_lambda': trial.suggest_float('reg_lambda', 5.0, 40.0, log=True),\n",
    "                'extra_trees': trial.suggest_categorical('extra_trees', [True, False]),\n",
    "                'path_smooth': trial.suggest_float('path_smooth', 0.0, 5.0),\n",
    "            }\n",
    "        else:\n",
    "            raise ValueError(f\"Search space for feature set for Optuna study not yet specified for LGBMRegressor.\")\n",
    "    elif study_estimator == BaseModelOptunaStudyEstimator.XGBREGRESSOR:\n",
    "        if BASE_MODEL_OPTUNA_STUDY_FEATURE_SET == FEATURE_SET_1:\n",
    "            return {\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 0.010, 0.013),\n",
    "                'max_depth': trial.suggest_categorical('max_depth', [6]),\n",
    "                'subsample': trial.suggest_float('subsample', 0.95, 1.0),\n",
    "                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.22, 0.26),\n",
    "                'alpha': trial.suggest_float('alpha', 2.0, 8.0, log=True),\n",
    "                'lambda': trial.suggest_float('lambda', 1e-4, 0.1, log=True),\n",
    "                'gamma': trial.suggest_float('gamma', 1e-4, 0.01, log=True),\n",
    "                'min_child_weight': trial.suggest_int('min_child_weight', 30, 37),\n",
    "            }\n",
    "        else:\n",
    "            raise ValueError(f\"Search space for feature set for Optuna study not yet specified for XGBRegressor.\")\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported optuna study estimator\")\n",
    "\n",
    "def get_base_model_predictions(study_estimator, trial_params, X_train_fold, y_train_fold, X_validation_fold, y_validation_fold):\n",
    "    if study_estimator == BaseModelOptunaStudyEstimator.CATBOOSTREGRESSOR:\n",
    "        model = CatBoostRegressor(\n",
    "            **trial_params,\n",
    "            iterations=30000,\n",
    "            use_best_model=True,\n",
    "            cat_features=cat_features,\n",
    "            bootstrap_type='Bayesian',\n",
    "            loss_function='RMSE',\n",
    "            eval_metric='RMSE',\n",
    "            task_type='GPU' if torch.cuda.is_available() else 'CPU',\n",
    "            devices='0',\n",
    "            random_seed=RANDOM_SEEDS[0],\n",
    "            verbose=False,\n",
    "            allow_writing_files=False\n",
    "        )\n",
    "        model.fit(\n",
    "            X_train_fold, y_train_fold,\n",
    "            eval_set=(X_validation_fold, y_validation_fold),\n",
    "            early_stopping_rounds=BASE_MODEL_EARLY_STOPPING_ROUNDS\n",
    "        )\n",
    "        return model.predict(X_validation_fold)\n",
    "    elif study_estimator == BaseModelOptunaStudyEstimator.LGBMREGRESSOR:\n",
    "        model = lgb.LGBMRegressor(\n",
    "            **trial_params,\n",
    "            n_estimators=30000,\n",
    "            objective='regression',\n",
    "            metric='rmse',\n",
    "            bagging_freq=1,\n",
    "            verbose=-1,\n",
    "            n_jobs=-1,\n",
    "            random_state=RANDOM_SEEDS[0]\n",
    "        )\n",
    "        model.fit(\n",
    "            X_train_fold, y_train_fold,\n",
    "            eval_set=[(X_validation_fold, y_validation_fold)],\n",
    "            callbacks=[lgb.early_stopping(stopping_rounds=BASE_MODEL_EARLY_STOPPING_ROUNDS, verbose=0)]\n",
    "        )\n",
    "        return model.predict(X_validation_fold)\n",
    "    elif study_estimator == BaseModelOptunaStudyEstimator.XGBREGRESSOR:\n",
    "        model = XGBRegressor(\n",
    "            **trial_params,\n",
    "            n_estimators=30000,\n",
    "            tree_method='hist' if torch.cuda.is_available() else 'auto',\n",
    "            device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "            enable_categorical=True,\n",
    "            objective='reg:squarederror',\n",
    "            eval_metric='rmse',\n",
    "            early_stopping_rounds=BASE_MODEL_EARLY_STOPPING_ROUNDS,\n",
    "            n_jobs=-1,\n",
    "            random_state=RANDOM_SEEDS[0],\n",
    "            verbosity=0\n",
    "        )\n",
    "        model.fit(\n",
    "            X_train_fold, y_train_fold,\n",
    "            eval_set=[(X_validation_fold, y_validation_fold)],\n",
    "            verbose=False\n",
    "        )\n",
    "        return model.predict(X_validation_fold)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported optuna study estimator\")\n",
    "\n",
    "def base_model_optuna_study_objective(trial):\n",
    "    base_model_params = get_base_model_optuna_params(trial, BASE_MODEL_OPTUNA_STUDY_ESTIMATOR)\n",
    "\n",
    "    X_train = train_data[BASE_MODEL_OPTUNA_STUDY_FEATURE_SET]\n",
    "    y_train = train_data[target_col]\n",
    "\n",
    "    base_model_optuna_study_kf = KFold(n_splits=BASE_MODEL_OPTUNA_STUDY_KFOLD_NUM_SPLITS, shuffle=True, random_state=RANDOM_SEEDS[0])\n",
    "    base_model_optuna_study_kf_splits = base_model_optuna_study_kf.split(X_train, y_train)\n",
    "    base_model_optuna_study_kf_enumeration = enumerate(base_model_optuna_study_kf_splits)\n",
    "\n",
    "    total_rmse = 0\n",
    "\n",
    "    for fold, (train_indices, validation_indices) in base_model_optuna_study_kf_enumeration:\n",
    "        X_train_fold = X_train.iloc[train_indices]\n",
    "        X_validation_fold = X_train.iloc[validation_indices]\n",
    "        y_train_fold = y_train.iloc[train_indices]\n",
    "        y_validation_fold = y_train.iloc[validation_indices]\n",
    "\n",
    "        y_validation_pred = get_base_model_predictions(\n",
    "            BASE_MODEL_OPTUNA_STUDY_ESTIMATOR,\n",
    "            base_model_params,\n",
    "            X_train_fold, y_train_fold,\n",
    "            X_validation_fold, y_validation_fold\n",
    "        )\n",
    "\n",
    "        y_validation_pred = np.clip(y_validation_pred, 0, 100)\n",
    "\n",
    "        rmse_fold = np.sqrt(mean_squared_error(y_validation_fold, y_validation_pred))\n",
    "        total_rmse += rmse_fold\n",
    "\n",
    "        trial.report(rmse_fold, step=fold)\n",
    "        if trial.should_prune():\n",
    "            raise optuna.TrialPruned()\n",
    "\n",
    "    average_rmse = total_rmse / BASE_MODEL_OPTUNA_STUDY_KFOLD_NUM_SPLITS\n",
    "    return average_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3ef87a51",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-13T04:01:33.732014Z",
     "iopub.status.busy": "2026-01-13T04:01:33.731741Z",
     "iopub.status.idle": "2026-01-13T09:21:56.592018Z",
     "shell.execute_reply": "2026-01-13T09:21:56.591275Z"
    },
    "papermill": {
     "duration": 19222.874167,
     "end_time": "2026-01-13T09:21:56.597496",
     "exception": false,
     "start_time": "2026-01-13T04:01:33.723329",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/optuna/samplers/_tpe/sampler.py:263: ExperimentalWarning: ``multivariate`` option is an experimental feature. The interface can change in the future.\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2026-01-13 04:01:33,735]\u001b[0m A new study created in memory with name: base_model_study\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started base model hyperparameter tuning for XGBRegressor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2026-01-13 04:04:57,652]\u001b[0m Trial 0 finished with value: 8.726177726849043 and parameters: {'learning_rate': 0.012573055762596704, 'max_depth': 6, 'subsample': 0.9537202052254542, 'colsample_bytree': 0.24498539715389286, 'alpha': 3.1718130112402285, 'lambda': 0.0001046701010372363, 'gamma': 0.0003932884377438277, 'min_child_weight': 32}. Best is trial 0 with value: 8.726177726849043.\u001b[0m\n",
      "\u001b[32m[I 2026-01-13 04:08:30,565]\u001b[0m Trial 1 finished with value: 8.726290567244162 and parameters: {'learning_rate': 0.012379832457437281, 'max_depth': 6, 'subsample': 0.9756851324452843, 'colsample_bytree': 0.2483640578980392, 'alpha': 4.396884833912206, 'lambda': 0.01775992734807952, 'gamma': 0.007530245275925834, 'min_child_weight': 34}. Best is trial 0 with value: 8.726177726849043.\u001b[0m\n",
      "\u001b[32m[I 2026-01-13 04:12:38,489]\u001b[0m Trial 2 finished with value: 8.72691205539387 and parameters: {'learning_rate': 0.010879732524962087, 'max_depth': 6, 'subsample': 0.9957677252424031, 'colsample_bytree': 0.25176327997128356, 'alpha': 3.1283312099820746, 'lambda': 0.0002322737161355929, 'gamma': 0.007699240319556832, 'min_child_weight': 35}. Best is trial 0 with value: 8.726177726849043.\u001b[0m\n",
      "\u001b[32m[I 2026-01-13 04:16:23,947]\u001b[0m Trial 3 finished with value: 8.726143200301799 and parameters: {'learning_rate': 0.011880203921039906, 'max_depth': 6, 'subsample': 0.9622514089927899, 'colsample_bytree': 0.22500469132647158, 'alpha': 3.016737341726347, 'lambda': 0.00023838667821738635, 'gamma': 0.00011101143853130251, 'min_child_weight': 36}. Best is trial 3 with value: 8.726143200301799.\u001b[0m\n",
      "\u001b[32m[I 2026-01-13 04:20:24,175]\u001b[0m Trial 4 finished with value: 8.727052491339618 and parameters: {'learning_rate': 0.011430786045047327, 'max_depth': 6, 'subsample': 0.9978429656798008, 'colsample_bytree': 0.2292172398084568, 'alpha': 3.9500706744268776, 'lambda': 0.07395885240874307, 'gamma': 0.004880923199169363, 'min_child_weight': 33}. Best is trial 3 with value: 8.726143200301799.\u001b[0m\n",
      "\u001b[32m[I 2026-01-13 04:21:41,730]\u001b[0m Trial 5 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 04:25:22,065]\u001b[0m Trial 6 finished with value: 8.726140737236989 and parameters: {'learning_rate': 0.012062014060209979, 'max_depth': 6, 'subsample': 0.9657201433494365, 'colsample_bytree': 0.24104566602395103, 'alpha': 5.240716047572047, 'lambda': 0.00035772108251148167, 'gamma': 0.008489736139649911, 'min_child_weight': 31}. Best is trial 6 with value: 8.726140737236989.\u001b[0m\n",
      "\u001b[32m[I 2026-01-13 04:26:42,447]\u001b[0m Trial 7 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 04:30:20,118]\u001b[0m Trial 8 finished with value: 8.726317602074866 and parameters: {'learning_rate': 0.012456161796346388, 'max_depth': 6, 'subsample': 0.9731295595489583, 'colsample_bytree': 0.22966569681054963, 'alpha': 2.8087995025236827, 'lambda': 0.00022025584773984277, 'gamma': 0.00376426333769103, 'min_child_weight': 33}. Best is trial 6 with value: 8.726140737236989.\u001b[0m\n",
      "\u001b[32m[I 2026-01-13 04:31:30,901]\u001b[0m Trial 9 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 04:32:48,460]\u001b[0m Trial 10 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 04:34:05,972]\u001b[0m Trial 11 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 04:35:23,190]\u001b[0m Trial 12 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 04:36:36,995]\u001b[0m Trial 13 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 04:40:24,671]\u001b[0m Trial 14 finished with value: 8.72591811486714 and parameters: {'learning_rate': 0.01179804688527545, 'max_depth': 6, 'subsample': 0.955346154622386, 'colsample_bytree': 0.2318361475807024, 'alpha': 5.414614220372512, 'lambda': 0.000979886266786386, 'gamma': 0.0035220594273550994, 'min_child_weight': 31}. Best is trial 14 with value: 8.72591811486714.\u001b[0m\n",
      "\u001b[32m[I 2026-01-13 04:41:37,792]\u001b[0m Trial 15 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 04:42:50,789]\u001b[0m Trial 16 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 04:44:05,417]\u001b[0m Trial 17 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 04:47:44,304]\u001b[0m Trial 18 finished with value: 8.72579409462223 and parameters: {'learning_rate': 0.012102594862028753, 'max_depth': 6, 'subsample': 0.9539280686957683, 'colsample_bytree': 0.25018885176990624, 'alpha': 5.081556897494343, 'lambda': 0.00021207242033019353, 'gamma': 0.005722456979284692, 'min_child_weight': 32}. Best is trial 18 with value: 8.72579409462223.\u001b[0m\n",
      "\u001b[32m[I 2026-01-13 04:48:51,661]\u001b[0m Trial 19 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 04:52:14,411]\u001b[0m Trial 20 finished with value: 8.726078585658236 and parameters: {'learning_rate': 0.012968010730524619, 'max_depth': 6, 'subsample': 0.9565039380040724, 'colsample_bytree': 0.25795289466955146, 'alpha': 4.220268450107668, 'lambda': 0.00041078964197918905, 'gamma': 0.0032030597378089817, 'min_child_weight': 33}. Best is trial 18 with value: 8.72579409462223.\u001b[0m\n",
      "\u001b[32m[I 2026-01-13 04:53:16,828]\u001b[0m Trial 21 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 04:54:27,767]\u001b[0m Trial 22 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 04:56:53,716]\u001b[0m Trial 23 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 05:00:16,105]\u001b[0m Trial 24 finished with value: 8.72580798235372 and parameters: {'learning_rate': 0.01298635492760628, 'max_depth': 6, 'subsample': 0.9588999952884582, 'colsample_bytree': 0.2548329361672326, 'alpha': 3.5045630980468925, 'lambda': 0.00023481699874966974, 'gamma': 0.002365394050709279, 'min_child_weight': 37}. Best is trial 18 with value: 8.72579409462223.\u001b[0m\n",
      "\u001b[32m[I 2026-01-13 05:01:26,509]\u001b[0m Trial 25 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 05:02:37,694]\u001b[0m Trial 26 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 05:03:47,920]\u001b[0m Trial 27 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 05:05:01,986]\u001b[0m Trial 28 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 05:06:23,638]\u001b[0m Trial 29 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 05:07:41,763]\u001b[0m Trial 30 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 05:08:54,601]\u001b[0m Trial 31 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 05:12:19,799]\u001b[0m Trial 32 finished with value: 8.725898244374024 and parameters: {'learning_rate': 0.012962045452313904, 'max_depth': 6, 'subsample': 0.962787154600921, 'colsample_bytree': 0.25450866487966245, 'alpha': 4.838542175744239, 'lambda': 0.00033447164934212855, 'gamma': 0.002072067728705258, 'min_child_weight': 33}. Best is trial 18 with value: 8.72579409462223.\u001b[0m\n",
      "\u001b[32m[I 2026-01-13 05:13:31,982]\u001b[0m Trial 33 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 05:14:38,998]\u001b[0m Trial 34 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 05:15:51,621]\u001b[0m Trial 35 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 05:16:59,469]\u001b[0m Trial 36 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 05:18:12,828]\u001b[0m Trial 37 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 05:19:25,932]\u001b[0m Trial 38 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 05:21:47,206]\u001b[0m Trial 39 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 05:23:02,099]\u001b[0m Trial 40 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 05:24:12,984]\u001b[0m Trial 41 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 05:25:21,707]\u001b[0m Trial 42 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 05:26:35,962]\u001b[0m Trial 43 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 05:27:54,668]\u001b[0m Trial 44 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 05:29:08,272]\u001b[0m Trial 45 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 05:30:15,738]\u001b[0m Trial 46 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 05:31:35,762]\u001b[0m Trial 47 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 05:32:48,854]\u001b[0m Trial 48 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 05:33:55,919]\u001b[0m Trial 49 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 05:35:02,535]\u001b[0m Trial 50 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 05:36:13,695]\u001b[0m Trial 51 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 05:37:29,344]\u001b[0m Trial 52 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 05:41:35,009]\u001b[0m Trial 53 finished with value: 8.725855142730039 and parameters: {'learning_rate': 0.010947463232515862, 'max_depth': 6, 'subsample': 0.9719454202071872, 'colsample_bytree': 0.22057035112986517, 'alpha': 6.1403259976918605, 'lambda': 0.08550233876673975, 'gamma': 0.0037345380738171815, 'min_child_weight': 31}. Best is trial 18 with value: 8.72579409462223.\u001b[0m\n",
      "\u001b[32m[I 2026-01-13 05:42:59,388]\u001b[0m Trial 54 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 05:44:21,057]\u001b[0m Trial 55 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 05:45:35,588]\u001b[0m Trial 56 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 05:46:53,136]\u001b[0m Trial 57 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 05:47:55,905]\u001b[0m Trial 58 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 05:49:13,579]\u001b[0m Trial 59 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 05:50:40,217]\u001b[0m Trial 60 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 05:51:52,505]\u001b[0m Trial 61 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 05:53:12,463]\u001b[0m Trial 62 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 05:56:42,089]\u001b[0m Trial 63 finished with value: 8.725956055496667 and parameters: {'learning_rate': 0.012789238195234285, 'max_depth': 6, 'subsample': 0.9729560767610372, 'colsample_bytree': 0.25782991887460516, 'alpha': 6.368353798521329, 'lambda': 0.0015760350030756165, 'gamma': 0.0022800788616165954, 'min_child_weight': 34}. Best is trial 18 with value: 8.72579409462223.\u001b[0m\n",
      "\u001b[32m[I 2026-01-13 05:57:58,091]\u001b[0m Trial 64 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 05:59:10,345]\u001b[0m Trial 65 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 06:00:24,601]\u001b[0m Trial 66 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 06:01:36,954]\u001b[0m Trial 67 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 06:02:47,617]\u001b[0m Trial 68 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 06:04:01,927]\u001b[0m Trial 69 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 06:05:12,533]\u001b[0m Trial 70 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 06:06:33,454]\u001b[0m Trial 71 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 06:07:54,766]\u001b[0m Trial 72 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 06:09:15,313]\u001b[0m Trial 73 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 06:10:29,215]\u001b[0m Trial 74 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 06:14:00,668]\u001b[0m Trial 75 finished with value: 8.726062481339724 and parameters: {'learning_rate': 0.012947191371548284, 'max_depth': 6, 'subsample': 0.9761172741675651, 'colsample_bytree': 0.2503554562233814, 'alpha': 2.9981239189606406, 'lambda': 0.0001915394282031335, 'gamma': 0.0009458610973108987, 'min_child_weight': 37}. Best is trial 18 with value: 8.72579409462223.\u001b[0m\n",
      "\u001b[32m[I 2026-01-13 06:16:19,197]\u001b[0m Trial 76 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 06:17:26,031]\u001b[0m Trial 77 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 06:18:32,880]\u001b[0m Trial 78 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 06:19:47,564]\u001b[0m Trial 79 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 06:22:06,883]\u001b[0m Trial 80 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 06:23:16,812]\u001b[0m Trial 81 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 06:24:33,121]\u001b[0m Trial 82 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 06:25:36,268]\u001b[0m Trial 83 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 06:26:54,206]\u001b[0m Trial 84 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 06:28:09,524]\u001b[0m Trial 85 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 06:29:23,864]\u001b[0m Trial 86 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 06:30:36,906]\u001b[0m Trial 87 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 06:31:51,743]\u001b[0m Trial 88 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 06:33:05,781]\u001b[0m Trial 89 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 06:35:34,297]\u001b[0m Trial 90 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 06:36:48,150]\u001b[0m Trial 91 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 06:37:59,762]\u001b[0m Trial 92 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 06:39:22,294]\u001b[0m Trial 93 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 06:40:39,283]\u001b[0m Trial 94 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 06:41:52,800]\u001b[0m Trial 95 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 06:43:06,128]\u001b[0m Trial 96 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 06:44:23,651]\u001b[0m Trial 97 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 06:45:40,010]\u001b[0m Trial 98 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 06:46:47,984]\u001b[0m Trial 99 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 06:48:02,234]\u001b[0m Trial 100 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 06:49:16,601]\u001b[0m Trial 101 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 06:50:29,529]\u001b[0m Trial 102 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 06:51:37,516]\u001b[0m Trial 103 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 06:52:51,263]\u001b[0m Trial 104 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 06:55:10,148]\u001b[0m Trial 105 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 06:56:21,406]\u001b[0m Trial 106 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 06:57:37,910]\u001b[0m Trial 107 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 06:58:40,740]\u001b[0m Trial 108 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 06:59:57,939]\u001b[0m Trial 109 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 07:01:00,689]\u001b[0m Trial 110 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 07:02:14,403]\u001b[0m Trial 111 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 07:03:25,952]\u001b[0m Trial 112 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 07:04:36,247]\u001b[0m Trial 113 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 07:05:53,471]\u001b[0m Trial 114 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 07:07:07,691]\u001b[0m Trial 115 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 07:08:27,299]\u001b[0m Trial 116 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 07:09:43,538]\u001b[0m Trial 117 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 07:14:00,306]\u001b[0m Trial 118 finished with value: 8.725769618431977 and parameters: {'learning_rate': 0.010508858403624931, 'max_depth': 6, 'subsample': 0.9734664462875761, 'colsample_bytree': 0.23082918356552026, 'alpha': 3.953240560223657, 'lambda': 0.09732635622149266, 'gamma': 0.003323619167802695, 'min_child_weight': 30}. Best is trial 118 with value: 8.725769618431977.\u001b[0m\n",
      "\u001b[32m[I 2026-01-13 07:17:03,709]\u001b[0m Trial 119 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 07:18:21,749]\u001b[0m Trial 120 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 07:19:42,756]\u001b[0m Trial 121 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 07:20:50,995]\u001b[0m Trial 122 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 07:21:57,800]\u001b[0m Trial 123 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 07:23:15,661]\u001b[0m Trial 124 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 07:26:15,652]\u001b[0m Trial 125 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 07:27:32,827]\u001b[0m Trial 126 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 07:29:04,549]\u001b[0m Trial 127 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 07:30:18,110]\u001b[0m Trial 128 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 07:31:29,175]\u001b[0m Trial 129 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 07:32:39,038]\u001b[0m Trial 130 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 07:33:45,806]\u001b[0m Trial 131 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 07:36:02,989]\u001b[0m Trial 132 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 07:38:20,397]\u001b[0m Trial 133 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 07:39:33,877]\u001b[0m Trial 134 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 07:40:50,238]\u001b[0m Trial 135 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 07:43:09,142]\u001b[0m Trial 136 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 07:44:22,869]\u001b[0m Trial 137 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 07:45:38,189]\u001b[0m Trial 138 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 07:47:06,325]\u001b[0m Trial 139 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 07:48:22,175]\u001b[0m Trial 140 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 07:49:44,322]\u001b[0m Trial 141 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 07:51:05,495]\u001b[0m Trial 142 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 07:52:21,408]\u001b[0m Trial 143 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 07:53:42,371]\u001b[0m Trial 144 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 07:54:54,706]\u001b[0m Trial 145 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 07:56:16,123]\u001b[0m Trial 146 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 07:58:51,462]\u001b[0m Trial 147 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 08:00:03,252]\u001b[0m Trial 148 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 08:01:12,367]\u001b[0m Trial 149 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 08:02:33,807]\u001b[0m Trial 150 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 08:03:50,402]\u001b[0m Trial 151 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 08:04:56,865]\u001b[0m Trial 152 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 08:06:14,074]\u001b[0m Trial 153 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 08:07:27,679]\u001b[0m Trial 154 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 08:08:55,172]\u001b[0m Trial 155 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 08:10:06,426]\u001b[0m Trial 156 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 08:11:22,085]\u001b[0m Trial 157 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 08:12:39,479]\u001b[0m Trial 158 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 08:13:57,200]\u001b[0m Trial 159 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 08:15:14,250]\u001b[0m Trial 160 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 08:16:24,106]\u001b[0m Trial 161 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 08:19:22,267]\u001b[0m Trial 162 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 08:20:51,451]\u001b[0m Trial 163 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 08:22:21,677]\u001b[0m Trial 164 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 08:23:34,080]\u001b[0m Trial 165 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 08:24:55,554]\u001b[0m Trial 166 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 08:26:17,272]\u001b[0m Trial 167 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 08:27:47,539]\u001b[0m Trial 168 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 08:29:07,430]\u001b[0m Trial 169 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 08:30:24,647]\u001b[0m Trial 170 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 08:31:53,932]\u001b[0m Trial 171 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 08:33:15,208]\u001b[0m Trial 172 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 08:34:36,147]\u001b[0m Trial 173 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 08:35:49,936]\u001b[0m Trial 174 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 08:37:20,270]\u001b[0m Trial 175 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 08:38:44,534]\u001b[0m Trial 176 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 08:39:46,992]\u001b[0m Trial 177 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 08:41:15,583]\u001b[0m Trial 178 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 08:43:29,695]\u001b[0m Trial 179 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 08:44:59,534]\u001b[0m Trial 180 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 08:46:03,273]\u001b[0m Trial 181 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 08:47:14,022]\u001b[0m Trial 182 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 08:48:25,758]\u001b[0m Trial 183 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 08:50:50,208]\u001b[0m Trial 184 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 08:53:06,726]\u001b[0m Trial 185 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 08:54:30,551]\u001b[0m Trial 186 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 08:55:40,976]\u001b[0m Trial 187 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 08:58:04,465]\u001b[0m Trial 188 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 09:02:21,440]\u001b[0m Trial 189 finished with value: 8.72543475398144 and parameters: {'learning_rate': 0.010273368786921377, 'max_depth': 6, 'subsample': 0.9733169036905926, 'colsample_bytree': 0.2455540838252638, 'alpha': 5.780775743676757, 'lambda': 0.03248053563060009, 'gamma': 0.006385890783139168, 'min_child_weight': 33}. Best is trial 189 with value: 8.72543475398144.\u001b[0m\n",
      "\u001b[32m[I 2026-01-13 09:06:41,974]\u001b[0m Trial 190 finished with value: 8.72580673159406 and parameters: {'learning_rate': 0.010564431319857775, 'max_depth': 6, 'subsample': 0.9730312790898691, 'colsample_bytree': 0.24498639223937138, 'alpha': 6.696790664365769, 'lambda': 0.09380763968458698, 'gamma': 0.005175628868153532, 'min_child_weight': 34}. Best is trial 189 with value: 8.72543475398144.\u001b[0m\n",
      "\u001b[32m[I 2026-01-13 09:08:03,249]\u001b[0m Trial 191 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 09:09:20,930]\u001b[0m Trial 192 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 09:10:38,865]\u001b[0m Trial 193 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 09:15:06,437]\u001b[0m Trial 194 finished with value: 8.725666180628638 and parameters: {'learning_rate': 0.010234145309020655, 'max_depth': 6, 'subsample': 0.9703460987550281, 'colsample_bytree': 0.25214359425476834, 'alpha': 5.024188901606744, 'lambda': 0.011905948978236812, 'gamma': 0.006960054855906403, 'min_child_weight': 32}. Best is trial 189 with value: 8.72543475398144.\u001b[0m\n",
      "\u001b[32m[I 2026-01-13 09:16:37,203]\u001b[0m Trial 195 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 09:17:55,261]\u001b[0m Trial 196 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 09:19:22,117]\u001b[0m Trial 197 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 09:20:39,242]\u001b[0m Trial 198 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-01-13 09:21:56,571]\u001b[0m Trial 199 pruned. \u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# trials finished: 200\n",
      "Best trial AUC: 8.72543475398144\n",
      "Best trial params:\n",
      "- learning_rate: 0.010273368786921377\n",
      "- max_depth: 6\n",
      "- subsample: 0.9733169036905926\n",
      "- colsample_bytree: 0.2455540838252638\n",
      "- alpha: 5.780775743676757\n",
      "- lambda: 0.03248053563060009\n",
      "- gamma: 0.006385890783139168\n",
      "- min_child_weight: 33\n"
     ]
    }
   ],
   "source": [
    "if SKIP_BASE_MODEL_HYPERPARAMETER_TUNING:\n",
    "    print(\"Skipped base model hyperparameter tuning\")\n",
    "else:\n",
    "    print(f\"Started base model hyperparameter tuning for {BASE_MODEL_OPTUNA_STUDY_ESTIMATOR.value}\")\n",
    "    sampler = optuna.samplers.TPESampler(n_ei_candidates=50, multivariate=True)\n",
    "    study = optuna.create_study(sampler=sampler, direction='minimize', study_name='base_model_study')\n",
    "    study.optimize(base_model_optuna_study_objective, n_trials=BASE_MODEL_OPTUNA_STUDY_NUM_TRIALS)\n",
    "    \n",
    "    print(f\"# trials finished: {len(study.trials)}\")\n",
    "    trial = study.best_trial\n",
    "    print(f\"Best trial AUC: {trial.value}\")\n",
    "    print(f\"Best trial params:\")\n",
    "    for param_key, param_value in trial.params.items():\n",
    "        print(f\"- {param_key}: {param_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e194f158",
   "metadata": {
    "papermill": {
     "duration": 0.014185,
     "end_time": "2026-01-13T09:21:56.625817",
     "exception": false,
     "start_time": "2026-01-13T09:21:56.611632",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 8. Base Model Candidates\n",
    "\n",
    "The base models that are candidates for the ensemble are specified. Instead of only relying on the meta-model to filter out unhelpful base model predictions, Optuna studies will also be conducted to find the feature set (set of base model predictions) that should be used for the meta-model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "823d9878",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-13T09:21:56.655533Z",
     "iopub.status.busy": "2026-01-13T09:21:56.654901Z",
     "iopub.status.idle": "2026-01-13T09:21:56.658388Z",
     "shell.execute_reply": "2026-01-13T09:21:56.657726Z"
    },
    "papermill": {
     "duration": 0.019898,
     "end_time": "2026-01-13T09:21:56.659823",
     "exception": false,
     "start_time": "2026-01-13T09:21:56.639925",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# number of splits to use for K-Fold Cross-Validation for base models\n",
    "BASE_MODEL_KFOLD_NUM_SPLITS = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe6917c",
   "metadata": {
    "papermill": {
     "duration": 0.013923,
     "end_time": "2026-01-13T09:21:56.687803",
     "exception": false,
     "start_time": "2026-01-13T09:21:56.673880",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 8.1 CatBoostRegressor\n",
    "\n",
    "### 8.1.1 Helper Methods (CatBoostRegressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c29c2fe0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-13T09:21:56.717904Z",
     "iopub.status.busy": "2026-01-13T09:21:56.717607Z",
     "iopub.status.idle": "2026-01-13T09:21:56.725530Z",
     "shell.execute_reply": "2026-01-13T09:21:56.724957Z"
    },
    "papermill": {
     "duration": 0.024876,
     "end_time": "2026-01-13T09:21:56.726925",
     "exception": false,
     "start_time": "2026-01-13T09:21:56.702049",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_catboostregressor_preds(params_dict, feature_names):\n",
    "    oof_preds_accumulator = np.zeros(len(train_data))\n",
    "    test_preds_accumulator = np.zeros(len(test_data))\n",
    "\n",
    "    X_train = train_data[feature_names]\n",
    "    y_train = train_data[target_col]\n",
    "\n",
    "    for random_seed in RANDOM_SEEDS:\n",
    "        kf = KFold(n_splits=BASE_MODEL_KFOLD_NUM_SPLITS, shuffle=True, random_state=random_seed)\n",
    "        kf_splits = kf.split(X_train, y_train)\n",
    "        kf_enumeration = enumerate(kf_splits)\n",
    "\n",
    "        seed_oof_preds = np.zeros(len(train_data))\n",
    "        \n",
    "        for fold, (train_indices, validation_indices) in kf_enumeration:\n",
    "            X_train_fold = X_train.iloc[train_indices]\n",
    "            X_validation_fold = X_train.iloc[validation_indices]\n",
    "            y_train_fold = y_train.iloc[train_indices]\n",
    "            y_validation_fold = y_train.iloc[validation_indices]\n",
    "\n",
    "            model = CatBoostRegressor(\n",
    "                **params_dict,\n",
    "                iterations=30000,\n",
    "                use_best_model=True,\n",
    "                cat_features=cat_features,\n",
    "                bootstrap_type='Bayesian',\n",
    "                loss_function='RMSE',\n",
    "                eval_metric='RMSE',\n",
    "                task_type='GPU' if torch.cuda.is_available() else 'CPU',\n",
    "                devices='0',\n",
    "                random_seed=random_seed,\n",
    "                verbose=False,\n",
    "                allow_writing_files=False\n",
    "            )\n",
    "            \n",
    "            model.fit(\n",
    "                X_train_fold, y_train_fold,\n",
    "                eval_set=(X_validation_fold, y_validation_fold),\n",
    "                early_stopping_rounds=BASE_MODEL_EARLY_STOPPING_ROUNDS\n",
    "            )\n",
    "\n",
    "            y_validation_pred = model.predict(X_validation_fold)\n",
    "            y_test_pred = model.predict(test_data[feature_names])\n",
    "\n",
    "            y_validation_pred = np.clip(y_validation_pred, 0, 100)\n",
    "            y_test_pred = np.clip(y_test_pred, 0, 100)\n",
    "\n",
    "            seed_oof_preds[validation_indices] = y_validation_pred\n",
    "            test_preds_accumulator += y_test_pred\n",
    "\n",
    "        oof_preds_accumulator += seed_oof_preds\n",
    "\n",
    "    final_oof_preds = oof_preds_accumulator / len(RANDOM_SEEDS)\n",
    "    final_test_preds = test_preds_accumulator / (BASE_MODEL_KFOLD_NUM_SPLITS * len(RANDOM_SEEDS))\n",
    "    \n",
    "    return pd.Series(final_oof_preds), pd.Series(final_test_preds)\n",
    "\n",
    "def get_catboostregressor_stacking_estimator(index, params_dict, feature_names):\n",
    "    return StackingEstimator(\n",
    "        name=f\"CatBoostRegressor_{index}\",\n",
    "        params_dict=params_dict,\n",
    "        feature_names=feature_names,\n",
    "        get_preds=get_catboostregressor_preds\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5094304c",
   "metadata": {
    "papermill": {
     "duration": 0.014489,
     "end_time": "2026-01-13T09:21:56.755657",
     "exception": false,
     "start_time": "2026-01-13T09:21:56.741168",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 8.1.2 Add Estimators (CatBoostRegressor)\n",
    "\n",
    "Add CatBoostRegressor estimators to list that StackingPredictionsRetriever will process. Hyperparameters were found using Optuna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b07c5b37",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-13T09:21:56.785455Z",
     "iopub.status.busy": "2026-01-13T09:21:56.785002Z",
     "iopub.status.idle": "2026-01-13T09:21:56.788834Z",
     "shell.execute_reply": "2026-01-13T09:21:56.788150Z"
    },
    "papermill": {
     "duration": 0.02032,
     "end_time": "2026-01-13T09:21:56.790256",
     "exception": false,
     "start_time": "2026-01-13T09:21:56.769936",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# CatBoostRegressor base models using FEATURE_SET_1\n",
    "estimators += [\n",
    "    #   get_catboostregressor_stacking_estimator(\n",
    "    #     index=1,\n",
    "    #     params_dict={ # Optuna study RMSE: 8.732405143185524\n",
    "    #         'learning_rate': 0.012512877480828674,\n",
    "    #         'depth': 6,\n",
    "    #         'l2_leaf_reg': 2.785033525504829,\n",
    "    #         'random_strength': 1.9972412072333539,\n",
    "    #         'colsample_bylevel': 0.2578536380324921,\n",
    "    #         'min_data_in_leaf': 93,\n",
    "    #         'bagging_temperature': 0.1409287678742852,\n",
    "    #     },\n",
    "    #     feature_names=FEATURE_SET_1\n",
    "    # ),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7e961e",
   "metadata": {
    "papermill": {
     "duration": 0.014219,
     "end_time": "2026-01-13T09:21:56.818577",
     "exception": false,
     "start_time": "2026-01-13T09:21:56.804358",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 8.2 LGBMRegressor\n",
    "\n",
    "### 8.2.1 Helper Methods (LGBMRegressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "70ee9b41",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-13T09:21:56.848161Z",
     "iopub.status.busy": "2026-01-13T09:21:56.847965Z",
     "iopub.status.idle": "2026-01-13T09:21:56.855264Z",
     "shell.execute_reply": "2026-01-13T09:21:56.854747Z"
    },
    "papermill": {
     "duration": 0.023809,
     "end_time": "2026-01-13T09:21:56.856559",
     "exception": false,
     "start_time": "2026-01-13T09:21:56.832750",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_lgbmregressor_preds(params_dict, feature_names):\n",
    "    oof_preds_accumulator = np.zeros(len(train_data))\n",
    "    test_preds_accumulator = np.zeros(len(test_data))\n",
    "\n",
    "    X_train = train_data[feature_names]\n",
    "    y_train = train_data[target_col]\n",
    "\n",
    "    for random_seed in RANDOM_SEEDS:\n",
    "        kf = KFold(n_splits=BASE_MODEL_KFOLD_NUM_SPLITS, shuffle=True, random_state=random_seed)\n",
    "        kf_splits = kf.split(X_train, y_train)\n",
    "        kf_enumeration = enumerate(kf_splits)\n",
    "\n",
    "        seed_oof_preds = np.zeros(len(train_data))\n",
    "\n",
    "        for fold, (train_indices, validation_indices) in kf_enumeration:\n",
    "            X_train_fold = X_train.iloc[train_indices]\n",
    "            X_validation_fold = X_train.iloc[validation_indices]\n",
    "            y_train_fold = y_train.iloc[train_indices]\n",
    "            y_validation_fold = y_train.iloc[validation_indices]\n",
    "\n",
    "            model = lgb.LGBMRegressor(\n",
    "                **params_dict,\n",
    "                n_estimators=30000,\n",
    "                objective='regression',\n",
    "                metric='rmse',\n",
    "                bagging_freq=1,\n",
    "                verbose=-1,\n",
    "                n_jobs=-1,\n",
    "                random_state=random_seed\n",
    "            )\n",
    "            \n",
    "            model.fit(\n",
    "                X_train_fold, y_train_fold,\n",
    "                eval_set=[(X_validation_fold, y_validation_fold)],\n",
    "                callbacks=[lgb.early_stopping(stopping_rounds=BASE_MODEL_EARLY_STOPPING_ROUNDS, verbose=0)]\n",
    "            )\n",
    "\n",
    "            y_validation_pred = model.predict(X_validation_fold)\n",
    "            y_test_pred = model.predict(test_data[feature_names])\n",
    "\n",
    "            y_validation_pred = np.clip(y_validation_pred, 0, 100)\n",
    "            y_test_pred = np.clip(y_test_pred, 0, 100)\n",
    "\n",
    "            seed_oof_preds[validation_indices] = y_validation_pred\n",
    "            test_preds_accumulator += y_test_pred\n",
    "\n",
    "        oof_preds_accumulator += seed_oof_preds\n",
    "\n",
    "    final_oof_preds = oof_preds_accumulator / len(RANDOM_SEEDS)\n",
    "    final_test_preds = test_preds_accumulator / (BASE_MODEL_KFOLD_NUM_SPLITS * len(RANDOM_SEEDS))\n",
    "    \n",
    "    return pd.Series(final_oof_preds), pd.Series(final_test_preds)\n",
    "\n",
    "def get_lgbmregressor_stacking_estimator(index, params_dict, feature_names):\n",
    "    return StackingEstimator(\n",
    "        name=f\"LGBMRegressor_{index}\",\n",
    "        params_dict=params_dict,\n",
    "        feature_names=feature_names,\n",
    "        get_preds=get_lgbmregressor_preds\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79885bd8",
   "metadata": {
    "papermill": {
     "duration": 0.014,
     "end_time": "2026-01-13T09:21:56.884439",
     "exception": false,
     "start_time": "2026-01-13T09:21:56.870439",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 8.2.2 Add Estimators (LGBMRegressor)\n",
    "\n",
    "Add LGBMRegressor estimators to list that StackingPredictionsRetriever will process. Hyperparameters were found using Optuna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b787a991",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-13T09:21:56.913835Z",
     "iopub.status.busy": "2026-01-13T09:21:56.913394Z",
     "iopub.status.idle": "2026-01-13T09:21:56.918147Z",
     "shell.execute_reply": "2026-01-13T09:21:56.917594Z"
    },
    "papermill": {
     "duration": 0.020819,
     "end_time": "2026-01-13T09:21:56.919368",
     "exception": false,
     "start_time": "2026-01-13T09:21:56.898549",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# LGBMRegressor base models using FEATURE_SET_1\n",
    "estimators += [\n",
    "    get_lgbmregressor_stacking_estimator(\n",
    "        index=1,\n",
    "        params_dict={ # Optuna study RMSE: 8.731615849391675\n",
    "            'learning_rate': 0.00560062116243721,\n",
    "            'num_leaves': 56,\n",
    "            'min_child_samples': 26,\n",
    "            'min_split_gain': 0.04953245371789207,\n",
    "            'subsample': 0.8713341159266043,\n",
    "            'colsample_bytree': 0.1954098975745725,\n",
    "            'reg_alpha': 25.4080280093216,\n",
    "            'reg_lambda': 7.3219797767089885,\n",
    "        },\n",
    "        feature_names=FEATURE_SET_1\n",
    "    ),\n",
    "    get_lgbmregressor_stacking_estimator(\n",
    "        index=2,\n",
    "        params_dict={ # Optuna study RMSE: 8.731273044954213\n",
    "            'learning_rate': 0.004361721228499642,\n",
    "            'num_leaves': 100,\n",
    "            'min_child_samples': 12,\n",
    "            'min_split_gain': 0.07298489238351248,\n",
    "            'subsample': 0.8791520668621439,\n",
    "            'colsample_bytree': 0.18986486739419273,\n",
    "            'reg_alpha': 0.10748714887376254,\n",
    "            'reg_lambda': 10.19912390767223,\n",
    "        },\n",
    "        feature_names=FEATURE_SET_1\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c704c1ae",
   "metadata": {
    "papermill": {
     "duration": 0.013802,
     "end_time": "2026-01-13T09:21:56.947159",
     "exception": false,
     "start_time": "2026-01-13T09:21:56.933357",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 8.3 XGBRegressor\n",
    "\n",
    "### 8.3.1 Helper Methods (XGBRegressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0075adad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-13T09:21:56.976141Z",
     "iopub.status.busy": "2026-01-13T09:21:56.975939Z",
     "iopub.status.idle": "2026-01-13T09:21:56.983715Z",
     "shell.execute_reply": "2026-01-13T09:21:56.983180Z"
    },
    "papermill": {
     "duration": 0.024054,
     "end_time": "2026-01-13T09:21:56.985078",
     "exception": false,
     "start_time": "2026-01-13T09:21:56.961024",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_xgbregressor_preds(params_dict, feature_names):\n",
    "    oof_preds_accumulator = np.zeros(len(train_data))\n",
    "    test_preds_accumulator = np.zeros(len(test_data))\n",
    "\n",
    "    X_train = train_data[feature_names]\n",
    "    y_train = train_data[target_col]\n",
    "\n",
    "    for random_seed in RANDOM_SEEDS:\n",
    "        kf = KFold(n_splits=BASE_MODEL_KFOLD_NUM_SPLITS, shuffle=True, random_state=random_seed)\n",
    "        kf_splits = kf.split(X_train, y_train)\n",
    "        kf_enumeration = enumerate(kf_splits)\n",
    "\n",
    "        seed_oof_preds = np.zeros(len(train_data))\n",
    "\n",
    "        for fold, (train_indices, validation_indices) in kf_enumeration:\n",
    "            X_train_fold = X_train.iloc[train_indices]\n",
    "            X_validation_fold = X_train.iloc[validation_indices]\n",
    "            y_train_fold = y_train.iloc[train_indices]\n",
    "            y_validation_fold = y_train.iloc[validation_indices]\n",
    "\n",
    "            model = XGBRegressor(\n",
    "                **params_dict,\n",
    "                n_estimators=30000,\n",
    "                tree_method='hist' if torch.cuda.is_available() else 'auto',\n",
    "                device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "                enable_categorical=True,\n",
    "                objective='reg:squarederror',\n",
    "                eval_metric='rmse',\n",
    "                early_stopping_rounds=BASE_MODEL_EARLY_STOPPING_ROUNDS,\n",
    "                n_jobs=-1,\n",
    "                random_state=random_seed,\n",
    "                verbosity=0\n",
    "            )\n",
    "            \n",
    "            model.fit(\n",
    "                X_train_fold, y_train_fold,\n",
    "                eval_set=[(X_validation_fold, y_validation_fold)],\n",
    "                verbose=False\n",
    "            )\n",
    "\n",
    "            y_validation_pred = model.predict(X_validation_fold)\n",
    "            y_test_pred = model.predict(test_data[feature_names])\n",
    "\n",
    "            y_validation_pred = np.clip(y_validation_pred, 0, 100)\n",
    "            y_test_pred = np.clip(y_test_pred, 0, 100)\n",
    "\n",
    "            seed_oof_preds[validation_indices] = y_validation_pred\n",
    "            test_preds_accumulator += y_test_pred\n",
    "\n",
    "        oof_preds_accumulator += seed_oof_preds\n",
    "\n",
    "    final_oof_preds = oof_preds_accumulator / len(RANDOM_SEEDS)\n",
    "    final_test_preds = test_preds_accumulator / (BASE_MODEL_KFOLD_NUM_SPLITS * len(RANDOM_SEEDS))\n",
    "    \n",
    "    return pd.Series(final_oof_preds), pd.Series(final_test_preds)\n",
    "\n",
    "def get_xgbregressor_stacking_estimator(index, params_dict, feature_names):\n",
    "    return StackingEstimator(\n",
    "        name=f\"XGBRegressor_{index}\",\n",
    "        params_dict=params_dict,\n",
    "        feature_names=feature_names,\n",
    "        get_preds=get_xgbregressor_preds\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12bfba2",
   "metadata": {
    "papermill": {
     "duration": 0.01374,
     "end_time": "2026-01-13T09:21:57.013833",
     "exception": false,
     "start_time": "2026-01-13T09:21:57.000093",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 8.3.2 Add Estimators (XGBRegressor)\n",
    "\n",
    "Add XGBRegressor estimators to list that StackingPredictionsRetriever will process. Hyperparameters were found using Optuna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "622073e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-13T09:21:57.042607Z",
     "iopub.status.busy": "2026-01-13T09:21:57.042400Z",
     "iopub.status.idle": "2026-01-13T09:21:57.050349Z",
     "shell.execute_reply": "2026-01-13T09:21:57.049827Z"
    },
    "papermill": {
     "duration": 0.024023,
     "end_time": "2026-01-13T09:21:57.051679",
     "exception": false,
     "start_time": "2026-01-13T09:21:57.027656",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# XGBRegressor base models using FEATURE_SET_1\n",
    "estimators += [\n",
    "    get_xgbregressor_stacking_estimator(\n",
    "        index=1,\n",
    "        params_dict={ # Optuna study RMSE: 8.726746380604618\n",
    "            'learning_rate': 0.013587597098853266,\n",
    "            'max_depth': 6,\n",
    "            'subsample': 0.9572735014614386,\n",
    "            'colsample_bytree': 0.23913914702804767,\n",
    "            'alpha': 1.4405674568559026,\n",
    "            'lambda': 0.039839278291000785,\n",
    "            'gamma': 0.041457934625352785,\n",
    "            'min_child_weight': 25,\n",
    "        },\n",
    "        feature_names=FEATURE_SET_1\n",
    "    ),\n",
    "    get_xgbregressor_stacking_estimator(\n",
    "        index=2,\n",
    "        params_dict={ # Optuna study RMSE: 8.72559295676817\n",
    "            'learning_rate': 0.011042630421555327,\n",
    "            'max_depth': 6,\n",
    "            'subsample': 0.9599956732090699,\n",
    "            'colsample_bytree': 0.25445172888708584,\n",
    "            'alpha': 0.6887980139572322,\n",
    "            'lambda': 0.05097488457198369,\n",
    "            'gamma': 0.07390409174758057,\n",
    "            'min_child_weight': 38,\n",
    "        },\n",
    "        feature_names=FEATURE_SET_1\n",
    "    ),\n",
    "    get_xgbregressor_stacking_estimator(\n",
    "        index=3,\n",
    "        params_dict={ # Optuna study RMSE: 8.725078961771361\n",
    "            'learning_rate': 0.008078244339287369,\n",
    "            'max_depth': 6,\n",
    "            'subsample': 0.9163689142226277,\n",
    "            'colsample_bytree': 0.2296982964930553,\n",
    "            'alpha': 0.19395772739611541,\n",
    "            'lambda': 0.04919117142485752,\n",
    "            'gamma': 0.16665026636261474,\n",
    "            'min_child_weight': 43,\n",
    "        },\n",
    "        feature_names=FEATURE_SET_1\n",
    "    ),\n",
    "    get_xgbregressor_stacking_estimator(\n",
    "        index=4,\n",
    "        params_dict={ # Optuna study RMSE: 8.725332819931655\n",
    "            'learning_rate': 0.009581863710458615,\n",
    "            'max_depth': 6,\n",
    "            'subsample': 0.9731807527392785,\n",
    "            'colsample_bytree': 0.22833370318746365,\n",
    "            'alpha': 0.48389973296874966,\n",
    "            'lambda': 0.7147942393033277,\n",
    "            'gamma': 0.020976364139759063,\n",
    "            'min_child_weight': 38,\n",
    "        },\n",
    "        feature_names=FEATURE_SET_1\n",
    "    ),\n",
    "    get_xgbregressor_stacking_estimator(\n",
    "        index=5,\n",
    "        params_dict={ # Optuna study RMSE: 8.726063500294797\n",
    "            'learning_rate': 0.01372187453137857,\n",
    "            'max_depth': 6,\n",
    "            'subsample': 0.977043107173594,\n",
    "            'colsample_bytree': 0.23095579097825794,\n",
    "            'alpha': 2.551583903037864,\n",
    "            'lambda': 0.13885611044124127,\n",
    "            'gamma': 0.00021296247007217191,\n",
    "            'min_child_weight': 33,\n",
    "        },\n",
    "        feature_names=FEATURE_SET_1\n",
    "    ),\n",
    "    get_xgbregressor_stacking_estimator(\n",
    "        index=6,\n",
    "        params_dict={ # Optuna study RMSE: 8.725542960628767\n",
    "            'learning_rate': 0.010016586025401255,\n",
    "            'max_depth': 6,\n",
    "            'subsample': 0.9522750478052623,\n",
    "            'colsample_bytree': 0.23393985284449337,\n",
    "            'alpha': 2.1874877683283414,\n",
    "            'lambda': 0.025727886413125782,\n",
    "            'gamma': 0.0007752478566528677,\n",
    "            'min_child_weight': 32,\n",
    "        },\n",
    "        feature_names=FEATURE_SET_1\n",
    "    ),\n",
    "    get_xgbregressor_stacking_estimator(\n",
    "        index=7,\n",
    "        params_dict={ # Optuna study RMSE: 8.72542385443002\n",
    "            'learning_rate': 0.011352870555104531,\n",
    "            'max_depth': 6,\n",
    "            'subsample': 0.9590160606833864,\n",
    "            'colsample_bytree': 0.23374922886341426,\n",
    "            'alpha': 4.446672845051681,\n",
    "            'lambda': 0.03219358347958651,\n",
    "            'gamma': 0.00015288138932193678,\n",
    "            'min_child_weight': 35,\n",
    "        },\n",
    "        feature_names=FEATURE_SET_1\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125c5531",
   "metadata": {
    "papermill": {
     "duration": 0.013893,
     "end_time": "2026-01-13T09:21:57.079450",
     "exception": false,
     "start_time": "2026-01-13T09:21:57.065557",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 8.4 Number of Base Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fd74e5b6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-13T09:21:57.109290Z",
     "iopub.status.busy": "2026-01-13T09:21:57.108762Z",
     "iopub.status.idle": "2026-01-13T09:21:57.112479Z",
     "shell.execute_reply": "2026-01-13T09:21:57.111824Z"
    },
    "papermill": {
     "duration": 0.020218,
     "end_time": "2026-01-13T09:21:57.113801",
     "exception": false,
     "start_time": "2026-01-13T09:21:57.093583",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of base models: 9\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total number of base models: {len(estimators)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5cf13b",
   "metadata": {
    "papermill": {
     "duration": 0.013934,
     "end_time": "2026-01-13T09:21:57.142203",
     "exception": false,
     "start_time": "2026-01-13T09:21:57.128269",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 9. Base Model Predictions\n",
    "\n",
    "## 9.1 Get Base Model Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4e888274",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-13T09:21:57.171469Z",
     "iopub.status.busy": "2026-01-13T09:21:57.171241Z",
     "iopub.status.idle": "2026-01-13T10:00:10.397408Z",
     "shell.execute_reply": "2026-01-13T10:00:10.396809Z"
    },
    "papermill": {
     "duration": 2293.242845,
     "end_time": "2026-01-13T10:00:10.399188",
     "exception": false,
     "start_time": "2026-01-13T09:21:57.156343",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Finished clearing predictions\n",
      "[INFO] Importing predictions..\n",
      "[INFO] 8 train predictions were imported:\n",
      "LGBMRegressor_1 (bc61f24c868323d36835d4301ce2690f), LGBMRegressor_2 (7edbc68d7e91bf012f62e14a3c8ce24a), XGBRegressor_1 (d5b33f920572395ea591f85395372949), XGBRegressor_2 (1651e6cc0bc873c5f37ae6eebceb2002), XGBRegressor_3 (7c033486efe2592e85d22287e98391fd), XGBRegressor_4 (7afd382667da316172eb1fc14749b8b8), XGBRegressor_5 (2ed31731d23c20a3feaad9d83527263c), XGBRegressor_6 (b7ab8f03fb6215815e39cc574693db7b)\n",
      "[INFO] 8 test predictions were imported:\n",
      "LGBMRegressor_1 (bc61f24c868323d36835d4301ce2690f), LGBMRegressor_2 (7edbc68d7e91bf012f62e14a3c8ce24a), XGBRegressor_1 (d5b33f920572395ea591f85395372949), XGBRegressor_2 (1651e6cc0bc873c5f37ae6eebceb2002), XGBRegressor_3 (7c033486efe2592e85d22287e98391fd), XGBRegressor_4 (7afd382667da316172eb1fc14749b8b8), XGBRegressor_5 (2ed31731d23c20a3feaad9d83527263c), XGBRegressor_6 (b7ab8f03fb6215815e39cc574693db7b)\n",
      "[INFO] Finished importing predictions\n",
      "[INFO] Getting predictions..\n",
      "[INFO] Getting predictions for estimator XGBRegressor_7 (f3a7f891ae0b2a9289432074da2d0433)\n",
      "[INFO] Saved predictions\n",
      "[INFO] Skipped retrieving predictions for following estimators as their current ones are not stale:\n",
      "LGBMRegressor_1 (bc61f24c868323d36835d4301ce2690f), LGBMRegressor_2 (7edbc68d7e91bf012f62e14a3c8ce24a), XGBRegressor_1 (d5b33f920572395ea591f85395372949), XGBRegressor_2 (1651e6cc0bc873c5f37ae6eebceb2002), XGBRegressor_3 (7c033486efe2592e85d22287e98391fd), XGBRegressor_4 (7afd382667da316172eb1fc14749b8b8), XGBRegressor_5 (2ed31731d23c20a3feaad9d83527263c), XGBRegressor_6 (b7ab8f03fb6215815e39cc574693db7b)\n",
      "[INFO] Finished getting all predictions\n"
     ]
    }
   ],
   "source": [
    "stacking_preds_retriever = StackingPredictionsRetriever(\n",
    "    estimators=estimators,\n",
    "    working_dir_path=\"/kaggle/working/\",\n",
    "    train_preds_filename=\"base_models_train_preds\",\n",
    "    test_preds_filename=\"base_models_test_preds\",\n",
    "    preds_save_interval=1\n",
    ")\n",
    "stacking_preds_retriever.clear_preds()\n",
    "stacking_preds_retriever.import_preds(\"/kaggle/input/predicting-student-test-scores-base-model-preds/\")\n",
    "stacking_preds_retriever.get_preds()\n",
    "\n",
    "base_model_train_preds, base_model_test_preds = stacking_preds_retriever.get_current_train_and_test_preds()\n",
    "base_model_train_preds.sort_index(axis=1, inplace=True, key=lambda index: index.map(lambda col_name: (col_name.split(\"_\")[0], int(col_name.split()[0].split(\"_\")[-1]))))\n",
    "base_model_test_preds.sort_index(axis=1, inplace=True, key=lambda index: index.map(lambda col_name: (col_name.split(\"_\")[0], int(col_name.split()[0].split(\"_\")[-1]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d56ae58",
   "metadata": {
    "papermill": {
     "duration": 0.014928,
     "end_time": "2026-01-13T10:00:10.429359",
     "exception": false,
     "start_time": "2026-01-13T10:00:10.414431",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 9.2 Base Models RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ce4023f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-13T10:00:10.460667Z",
     "iopub.status.busy": "2026-01-13T10:00:10.460349Z",
     "iopub.status.idle": "2026-01-13T10:00:10.497695Z",
     "shell.execute_reply": "2026-01-13T10:00:10.497014Z"
    },
    "papermill": {
     "duration": 0.055167,
     "end_time": "2026-01-13T10:00:10.499391",
     "exception": false,
     "start_time": "2026-01-13T10:00:10.444224",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor_2 (1651e6cc0bc873c5f37ae6eebceb2002)     8.706541\n",
       "XGBRegressor_5 (2ed31731d23c20a3feaad9d83527263c)     8.706560\n",
       "XGBRegressor_4 (7afd382667da316172eb1fc14749b8b8)     8.706637\n",
       "XGBRegressor_7 (f3a7f891ae0b2a9289432074da2d0433)     8.706706\n",
       "XGBRegressor_1 (d5b33f920572395ea591f85395372949)     8.707001\n",
       "XGBRegressor_6 (b7ab8f03fb6215815e39cc574693db7b)     8.707044\n",
       "XGBRegressor_3 (7c033486efe2592e85d22287e98391fd)     8.707424\n",
       "LGBMRegressor_2 (7edbc68d7e91bf012f62e14a3c8ce24a)    8.715358\n",
       "LGBMRegressor_1 (bc61f24c868323d36835d4301ce2690f)    8.715844\n",
       "dtype: float64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model_rmse = pd.Series()\n",
    "for estimator in base_model_train_preds.columns:\n",
    "    base_model_rmse[estimator] = np.sqrt(mean_squared_error(train_data[target_col], base_model_train_preds[estimator]))\n",
    "base_model_rmse.sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed95856",
   "metadata": {
    "papermill": {
     "duration": 0.01435,
     "end_time": "2026-01-13T10:00:10.529132",
     "exception": false,
     "start_time": "2026-01-13T10:00:10.514782",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 10. Submission (Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5e0fe8ca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-13T10:00:10.558561Z",
     "iopub.status.busy": "2026-01-13T10:00:10.558296Z",
     "iopub.status.idle": "2026-01-13T10:00:11.046477Z",
     "shell.execute_reply": "2026-01-13T10:00:11.045777Z"
    },
    "papermill": {
     "duration": 0.504727,
     "end_time": "2026-01-13T10:00:11.048039",
     "exception": false,
     "start_time": "2026-01-13T10:00:10.543312",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission file prepared.\n"
     ]
    }
   ],
   "source": [
    "# prepare submission\n",
    "submission = pd.DataFrame({'id': test_data.index, target_col: base_model_test_preds['XGBRegressor_7 (f3a7f891ae0b2a9289432074da2d0433)']})\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print('Submission file prepared.')"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 14993753,
     "sourceId": 119082,
     "sourceType": "competition"
    },
    {
     "datasetId": 9179059,
     "sourceId": 14479335,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31234,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 21540.278241,
   "end_time": "2026-01-13T10:00:13.953063",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2026-01-13T04:01:13.674822",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
