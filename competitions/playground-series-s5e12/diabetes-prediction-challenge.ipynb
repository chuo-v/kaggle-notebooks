{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5baf446",
   "metadata": {
    "papermill": {
     "duration": 0.014342,
     "end_time": "2025-12-10T02:32:50.588289",
     "exception": false,
     "start_time": "2025-12-10T02:32:50.573947",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 1. Overview\n",
    "\n",
    "This is a notebook for training models to submit predictions to the \"Diabetes Prediction Challenge\" Kaggle competition ([playground-series-s5e12](https://www.kaggle.com/competitions/playground-series-s5e12)).\n",
    "\n",
    "Synthetic data is used for this playground competition, and the objective is to, for each patient in the test set, predict the probability that the patient will be diagnosed with diabetes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25234263",
   "metadata": {
    "papermill": {
     "duration": 0.012306,
     "end_time": "2025-12-10T02:32:50.613388",
     "exception": false,
     "start_time": "2025-12-10T02:32:50.601082",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 2. Setup\n",
    "\n",
    "## 2.1 Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c1704c1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T02:32:50.640309Z",
     "iopub.status.busy": "2025-12-10T02:32:50.639562Z",
     "iopub.status.idle": "2025-12-10T02:33:01.683423Z",
     "shell.execute_reply": "2025-12-10T02:33:01.682303Z"
    },
    "papermill": {
     "duration": 11.059921,
     "end_time": "2025-12-10T02:33:01.685361",
     "exception": false,
     "start_time": "2025-12-10T02:32:50.625440",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import copy\n",
    "import optuna\n",
    "import os\n",
    "import hashlib as hl # for StackingEstimator\n",
    "import inspect # for StackingEstimator\n",
    "import random\n",
    "import warnings\n",
    "from catboost import CatBoostClassifier\n",
    "from enum import Enum\n",
    "from pathlib import Path # for StackingPredictionsRetriever\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.linear_model import LinearRegression # for meta model\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import OrdinalEncoder, StandardScaler\n",
    "from types import FunctionType\n",
    "from xgboost import XGBClassifier, XGBRFClassifier\n",
    "\n",
    "warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "pd.set_option('display.max_colwidth', None) # Display full column content\n",
    "pd.set_option('display.max_rows', None) # Display all rows\n",
    "pd.set_option('display.width', 1000) # Set larger display width"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f004d35",
   "metadata": {
    "papermill": {
     "duration": 0.012014,
     "end_time": "2025-12-10T02:33:01.709687",
     "exception": false,
     "start_time": "2025-12-10T02:33:01.697673",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2.2 Reproducibility\n",
    "\n",
    "For reproducibility of results, an arbitrary number will be used for the random seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0b65f95",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T02:33:01.736893Z",
     "iopub.status.busy": "2025-12-10T02:33:01.735774Z",
     "iopub.status.idle": "2025-12-10T02:33:01.750799Z",
     "shell.execute_reply": "2025-12-10T02:33:01.749838Z"
    },
    "papermill": {
     "duration": 0.030896,
     "end_time": "2025-12-10T02:33:01.752737",
     "exception": false,
     "start_time": "2025-12-10T02:33:01.721841",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "RANDOM_SEEDS = [11, 42]\n",
    "random.seed(RANDOM_SEEDS[0])\n",
    "np.random.seed(RANDOM_SEEDS[0])\n",
    "torch.manual_seed(RANDOM_SEEDS[0])\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(RANDOM_SEEDS[0])\n",
    "    torch.cuda.manual_seed_all(RANDOM_SEEDS[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e452f26",
   "metadata": {
    "papermill": {
     "duration": 0.011977,
     "end_time": "2025-12-10T02:33:01.777080",
     "exception": false,
     "start_time": "2025-12-10T02:33:01.765103",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2.3 Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "057291e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T02:33:01.802589Z",
     "iopub.status.busy": "2025-12-10T02:33:01.802180Z",
     "iopub.status.idle": "2025-12-10T02:33:01.807754Z",
     "shell.execute_reply": "2025-12-10T02:33:01.806682Z"
    },
    "papermill": {
     "duration": 0.020463,
     "end_time": "2025-12-10T02:33:01.809592",
     "exception": false,
     "start_time": "2025-12-10T02:33:01.789129",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b528cc5b",
   "metadata": {
    "papermill": {
     "duration": 0.012534,
     "end_time": "2025-12-10T02:33:01.834533",
     "exception": false,
     "start_time": "2025-12-10T02:33:01.821999",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2.4 DataFrames\n",
    "\n",
    "Read the data provided for the competition into dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95c105c0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T02:33:01.862278Z",
     "iopub.status.busy": "2025-12-10T02:33:01.861548Z",
     "iopub.status.idle": "2025-12-10T02:33:04.930765Z",
     "shell.execute_reply": "2025-12-10T02:33:04.929742Z"
    },
    "papermill": {
     "duration": 3.085407,
     "end_time": "2025-12-10T02:33:04.932636",
     "exception": false,
     "start_time": "2025-12-10T02:33:01.847229",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "INPUT_DIR = '/kaggle/input'\n",
    "orig_train_data = pd.read_csv(os.path.join(INPUT_DIR, 'playground-series-s5e12/train.csv'))\n",
    "orig_test_data = pd.read_csv(os.path.join(INPUT_DIR, 'playground-series-s5e12/test.csv'))\n",
    "\n",
    "# set index\n",
    "orig_train_data.set_index('id', inplace=True)\n",
    "orig_test_data.set_index('id', inplace=True)\n",
    "\n",
    "# target column\n",
    "target_col = \"diagnosed_diabetes\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f578b251",
   "metadata": {
    "papermill": {
     "duration": 0.012006,
     "end_time": "2025-12-10T02:33:04.956768",
     "exception": false,
     "start_time": "2025-12-10T02:33:04.944762",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 3. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "689a32fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T02:33:04.982260Z",
     "iopub.status.busy": "2025-12-10T02:33:04.981954Z",
     "iopub.status.idle": "2025-12-10T02:33:04.986594Z",
     "shell.execute_reply": "2025-12-10T02:33:04.985586Z"
    },
    "papermill": {
     "duration": 0.019552,
     "end_time": "2025-12-10T02:33:04.988399",
     "exception": false,
     "start_time": "2025-12-10T02:33:04.968847",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# to skip the generation of plots (e.g. KDE) in this section that take time; set to False to generate the plots \n",
    "SKIP_PLOTS = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3e41b5b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T02:33:05.014866Z",
     "iopub.status.busy": "2025-12-10T02:33:05.014397Z",
     "iopub.status.idle": "2025-12-10T02:33:05.694758Z",
     "shell.execute_reply": "2025-12-10T02:33:05.693823Z"
    },
    "papermill": {
     "duration": 0.695303,
     "end_time": "2025-12-10T02:33:05.696392",
     "exception": false,
     "start_time": "2025-12-10T02:33:05.001089",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>alcohol_consumption_per_week</th>\n",
       "      <th>physical_activity_minutes_per_week</th>\n",
       "      <th>diet_score</th>\n",
       "      <th>sleep_hours_per_day</th>\n",
       "      <th>screen_time_hours_per_day</th>\n",
       "      <th>bmi</th>\n",
       "      <th>waist_to_hip_ratio</th>\n",
       "      <th>systolic_bp</th>\n",
       "      <th>diastolic_bp</th>\n",
       "      <th>heart_rate</th>\n",
       "      <th>cholesterol_total</th>\n",
       "      <th>hdl_cholesterol</th>\n",
       "      <th>ldl_cholesterol</th>\n",
       "      <th>triglycerides</th>\n",
       "      <th>family_history_diabetes</th>\n",
       "      <th>hypertension_history</th>\n",
       "      <th>cardiovascular_history</th>\n",
       "      <th>diagnosed_diabetes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>700000.000000</td>\n",
       "      <td>700000.000000</td>\n",
       "      <td>700000.000000</td>\n",
       "      <td>700000.000000</td>\n",
       "      <td>700000.000000</td>\n",
       "      <td>700000.000000</td>\n",
       "      <td>700000.000000</td>\n",
       "      <td>700000.000000</td>\n",
       "      <td>700000.000000</td>\n",
       "      <td>700000.000000</td>\n",
       "      <td>700000.000000</td>\n",
       "      <td>700000.000000</td>\n",
       "      <td>700000.000000</td>\n",
       "      <td>700000.000000</td>\n",
       "      <td>700000.000000</td>\n",
       "      <td>700000.000000</td>\n",
       "      <td>700000.000000</td>\n",
       "      <td>700000.000000</td>\n",
       "      <td>700000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>50.359734</td>\n",
       "      <td>2.072411</td>\n",
       "      <td>80.230803</td>\n",
       "      <td>5.963695</td>\n",
       "      <td>7.002200</td>\n",
       "      <td>6.012733</td>\n",
       "      <td>25.874684</td>\n",
       "      <td>0.858766</td>\n",
       "      <td>116.294193</td>\n",
       "      <td>75.440924</td>\n",
       "      <td>70.167749</td>\n",
       "      <td>186.818801</td>\n",
       "      <td>53.823214</td>\n",
       "      <td>102.905854</td>\n",
       "      <td>123.081850</td>\n",
       "      <td>0.149401</td>\n",
       "      <td>0.181990</td>\n",
       "      <td>0.030324</td>\n",
       "      <td>0.623296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>11.655520</td>\n",
       "      <td>1.048189</td>\n",
       "      <td>51.195071</td>\n",
       "      <td>1.463336</td>\n",
       "      <td>0.901907</td>\n",
       "      <td>2.022707</td>\n",
       "      <td>2.860705</td>\n",
       "      <td>0.037980</td>\n",
       "      <td>11.010390</td>\n",
       "      <td>6.825775</td>\n",
       "      <td>6.938722</td>\n",
       "      <td>16.730832</td>\n",
       "      <td>8.266545</td>\n",
       "      <td>19.022416</td>\n",
       "      <td>24.739397</td>\n",
       "      <td>0.356484</td>\n",
       "      <td>0.385837</td>\n",
       "      <td>0.171478</td>\n",
       "      <td>0.484560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>19.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>3.100000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>15.100000</td>\n",
       "      <td>0.680000</td>\n",
       "      <td>91.000000</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>117.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>42.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>49.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>6.400000</td>\n",
       "      <td>4.600000</td>\n",
       "      <td>23.900000</td>\n",
       "      <td>0.830000</td>\n",
       "      <td>108.000000</td>\n",
       "      <td>71.000000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>175.000000</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>89.000000</td>\n",
       "      <td>106.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>50.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>71.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>25.900000</td>\n",
       "      <td>0.860000</td>\n",
       "      <td>116.000000</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>187.000000</td>\n",
       "      <td>54.000000</td>\n",
       "      <td>103.000000</td>\n",
       "      <td>123.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>58.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>96.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.600000</td>\n",
       "      <td>7.400000</td>\n",
       "      <td>27.800000</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>124.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>199.000000</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>116.000000</td>\n",
       "      <td>139.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>89.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>747.000000</td>\n",
       "      <td>9.900000</td>\n",
       "      <td>9.900000</td>\n",
       "      <td>16.500000</td>\n",
       "      <td>38.400000</td>\n",
       "      <td>1.050000</td>\n",
       "      <td>163.000000</td>\n",
       "      <td>104.000000</td>\n",
       "      <td>101.000000</td>\n",
       "      <td>289.000000</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>205.000000</td>\n",
       "      <td>290.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 age  alcohol_consumption_per_week  physical_activity_minutes_per_week     diet_score  sleep_hours_per_day  screen_time_hours_per_day            bmi  waist_to_hip_ratio    systolic_bp   diastolic_bp     heart_rate  cholesterol_total  hdl_cholesterol  ldl_cholesterol  triglycerides  family_history_diabetes  hypertension_history  cardiovascular_history  diagnosed_diabetes\n",
       "count  700000.000000                 700000.000000                       700000.000000  700000.000000        700000.000000              700000.000000  700000.000000       700000.000000  700000.000000  700000.000000  700000.000000      700000.000000    700000.000000    700000.000000  700000.000000            700000.000000         700000.000000           700000.000000       700000.000000\n",
       "mean       50.359734                      2.072411                           80.230803       5.963695             7.002200                   6.012733      25.874684            0.858766     116.294193      75.440924      70.167749         186.818801        53.823214       102.905854     123.081850                 0.149401              0.181990                0.030324            0.623296\n",
       "std        11.655520                      1.048189                           51.195071       1.463336             0.901907                   2.022707       2.860705            0.037980      11.010390       6.825775       6.938722          16.730832         8.266545        19.022416      24.739397                 0.356484              0.385837                0.171478            0.484560\n",
       "min        19.000000                      1.000000                            1.000000       0.100000             3.100000                   0.600000      15.100000            0.680000      91.000000      51.000000      42.000000         117.000000        21.000000        51.000000      31.000000                 0.000000              0.000000                0.000000            0.000000\n",
       "25%        42.000000                      1.000000                           49.000000       5.000000             6.400000                   4.600000      23.900000            0.830000     108.000000      71.000000      65.000000         175.000000        48.000000        89.000000     106.000000                 0.000000              0.000000                0.000000            0.000000\n",
       "50%        50.000000                      2.000000                           71.000000       6.000000             7.000000                   6.000000      25.900000            0.860000     116.000000      75.000000      70.000000         187.000000        54.000000       103.000000     123.000000                 0.000000              0.000000                0.000000            1.000000\n",
       "75%        58.000000                      3.000000                           96.000000       7.000000             7.600000                   7.400000      27.800000            0.880000     124.000000      80.000000      75.000000         199.000000        59.000000       116.000000     139.000000                 0.000000              0.000000                0.000000            1.000000\n",
       "max        89.000000                      9.000000                          747.000000       9.900000             9.900000                  16.500000      38.400000            1.050000     163.000000     104.000000     101.000000         289.000000        90.000000       205.000000     290.000000                 1.000000              1.000000                1.000000            1.000000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig_train_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dcf95855",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T02:33:05.724745Z",
     "iopub.status.busy": "2025-12-10T02:33:05.723719Z",
     "iopub.status.idle": "2025-12-10T02:33:05.997100Z",
     "shell.execute_reply": "2025-12-10T02:33:05.996107Z"
    },
    "papermill": {
     "duration": 0.288565,
     "end_time": "2025-12-10T02:33:05.998688",
     "exception": false,
     "start_time": "2025-12-10T02:33:05.710123",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>alcohol_consumption_per_week</th>\n",
       "      <th>physical_activity_minutes_per_week</th>\n",
       "      <th>diet_score</th>\n",
       "      <th>sleep_hours_per_day</th>\n",
       "      <th>screen_time_hours_per_day</th>\n",
       "      <th>bmi</th>\n",
       "      <th>waist_to_hip_ratio</th>\n",
       "      <th>systolic_bp</th>\n",
       "      <th>diastolic_bp</th>\n",
       "      <th>heart_rate</th>\n",
       "      <th>cholesterol_total</th>\n",
       "      <th>hdl_cholesterol</th>\n",
       "      <th>ldl_cholesterol</th>\n",
       "      <th>triglycerides</th>\n",
       "      <th>family_history_diabetes</th>\n",
       "      <th>hypertension_history</th>\n",
       "      <th>cardiovascular_history</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>300000.000000</td>\n",
       "      <td>300000.000000</td>\n",
       "      <td>300000.000000</td>\n",
       "      <td>300000.000000</td>\n",
       "      <td>300000.000000</td>\n",
       "      <td>300000.000000</td>\n",
       "      <td>300000.000000</td>\n",
       "      <td>300000.000000</td>\n",
       "      <td>300000.000000</td>\n",
       "      <td>300000.000000</td>\n",
       "      <td>300000.000000</td>\n",
       "      <td>300000.000000</td>\n",
       "      <td>300000.000000</td>\n",
       "      <td>300000.000000</td>\n",
       "      <td>300000.000000</td>\n",
       "      <td>300000.000000</td>\n",
       "      <td>300000.000000</td>\n",
       "      <td>300000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>50.432397</td>\n",
       "      <td>2.089693</td>\n",
       "      <td>92.349087</td>\n",
       "      <td>5.945838</td>\n",
       "      <td>6.997795</td>\n",
       "      <td>6.011278</td>\n",
       "      <td>25.881906</td>\n",
       "      <td>0.859007</td>\n",
       "      <td>116.374117</td>\n",
       "      <td>75.396013</td>\n",
       "      <td>70.048350</td>\n",
       "      <td>187.308620</td>\n",
       "      <td>53.813557</td>\n",
       "      <td>103.416083</td>\n",
       "      <td>123.538480</td>\n",
       "      <td>0.152920</td>\n",
       "      <td>0.184410</td>\n",
       "      <td>0.033110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>11.938741</td>\n",
       "      <td>1.066214</td>\n",
       "      <td>62.187399</td>\n",
       "      <td>1.481068</td>\n",
       "      <td>0.914693</td>\n",
       "      <td>2.060472</td>\n",
       "      <td>2.894289</td>\n",
       "      <td>0.038523</td>\n",
       "      <td>11.252146</td>\n",
       "      <td>6.950340</td>\n",
       "      <td>7.090543</td>\n",
       "      <td>18.413053</td>\n",
       "      <td>8.398126</td>\n",
       "      <td>20.571855</td>\n",
       "      <td>28.965441</td>\n",
       "      <td>0.359911</td>\n",
       "      <td>0.387819</td>\n",
       "      <td>0.178924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>19.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>3.100000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>15.100000</td>\n",
       "      <td>0.690000</td>\n",
       "      <td>91.000000</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>107.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>42.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>6.400000</td>\n",
       "      <td>4.600000</td>\n",
       "      <td>23.900000</td>\n",
       "      <td>0.830000</td>\n",
       "      <td>108.000000</td>\n",
       "      <td>71.000000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>174.000000</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>89.000000</td>\n",
       "      <td>104.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>50.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>25.900000</td>\n",
       "      <td>0.860000</td>\n",
       "      <td>116.000000</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>187.000000</td>\n",
       "      <td>54.000000</td>\n",
       "      <td>103.000000</td>\n",
       "      <td>123.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>59.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>115.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.600000</td>\n",
       "      <td>7.400000</td>\n",
       "      <td>27.800000</td>\n",
       "      <td>0.890000</td>\n",
       "      <td>124.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>117.000000</td>\n",
       "      <td>142.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>89.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>748.000000</td>\n",
       "      <td>9.900000</td>\n",
       "      <td>9.900000</td>\n",
       "      <td>15.900000</td>\n",
       "      <td>38.300000</td>\n",
       "      <td>1.050000</td>\n",
       "      <td>170.000000</td>\n",
       "      <td>104.000000</td>\n",
       "      <td>101.000000</td>\n",
       "      <td>285.000000</td>\n",
       "      <td>91.000000</td>\n",
       "      <td>226.000000</td>\n",
       "      <td>290.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 age  alcohol_consumption_per_week  physical_activity_minutes_per_week     diet_score  sleep_hours_per_day  screen_time_hours_per_day            bmi  waist_to_hip_ratio    systolic_bp   diastolic_bp     heart_rate  cholesterol_total  hdl_cholesterol  ldl_cholesterol  triglycerides  family_history_diabetes  hypertension_history  cardiovascular_history\n",
       "count  300000.000000                 300000.000000                       300000.000000  300000.000000        300000.000000              300000.000000  300000.000000       300000.000000  300000.000000  300000.000000  300000.000000      300000.000000    300000.000000    300000.000000  300000.000000            300000.000000         300000.000000           300000.000000\n",
       "mean       50.432397                      2.089693                           92.349087       5.945838             6.997795                   6.011278      25.881906            0.859007     116.374117      75.396013      70.048350         187.308620        53.813557       103.416083     123.538480                 0.152920              0.184410                0.033110\n",
       "std        11.938741                      1.066214                           62.187399       1.481068             0.914693                   2.060472       2.894289            0.038523      11.252146       6.950340       7.090543          18.413053         8.398126        20.571855      28.965441                 0.359911              0.387819                0.178924\n",
       "min        19.000000                      1.000000                            1.000000       0.100000             3.100000                   0.600000      15.100000            0.690000      91.000000      51.000000      42.000000         107.000000        22.000000        51.000000      31.000000                 0.000000              0.000000                0.000000\n",
       "25%        42.000000                      1.000000                           51.000000       5.000000             6.400000                   4.600000      23.900000            0.830000     108.000000      71.000000      65.000000         174.000000        48.000000        89.000000     104.000000                 0.000000              0.000000                0.000000\n",
       "50%        50.000000                      2.000000                           77.000000       6.000000             7.000000                   6.000000      25.900000            0.860000     116.000000      75.000000      70.000000         187.000000        54.000000       103.000000     123.000000                 0.000000              0.000000                0.000000\n",
       "75%        59.000000                      3.000000                          115.000000       7.000000             7.600000                   7.400000      27.800000            0.890000     124.000000      80.000000      75.000000         200.000000        60.000000       117.000000     142.000000                 0.000000              0.000000                0.000000\n",
       "max        89.000000                      9.000000                          748.000000       9.900000             9.900000                  15.900000      38.300000            1.050000     170.000000     104.000000     101.000000         285.000000        91.000000       226.000000     290.000000                 1.000000              1.000000                1.000000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig_test_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da78536a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T02:33:06.028879Z",
     "iopub.status.busy": "2025-12-10T02:33:06.027651Z",
     "iopub.status.idle": "2025-12-10T02:33:06.154465Z",
     "shell.execute_reply": "2025-12-10T02:33:06.153501Z"
    },
    "papermill": {
     "duration": 0.143238,
     "end_time": "2025-12-10T02:33:06.156305",
     "exception": false,
     "start_time": "2025-12-10T02:33:06.013067",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "numeric_col_names = orig_train_data.select_dtypes(include='number').columns.to_series()\n",
    "categorical_col_names = orig_train_data.select_dtypes(include='object').columns.to_series()\n",
    "assert numeric_col_names.size + categorical_col_names.size == orig_train_data.shape[1]\n",
    "\n",
    "# drop target column from numeric column names\n",
    "numeric_col_names.drop(target_col, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9cae680",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T02:33:06.185209Z",
     "iopub.status.busy": "2025-12-10T02:33:06.184884Z",
     "iopub.status.idle": "2025-12-10T02:33:06.518715Z",
     "shell.execute_reply": "2025-12-10T02:33:06.517364Z"
    },
    "papermill": {
     "duration": 0.349929,
     "end_time": "2025-12-10T02:33:06.520513",
     "exception": false,
     "start_time": "2025-12-10T02:33:06.170584",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Train data missing values #####\n",
      "age                                   0\n",
      "alcohol_consumption_per_week          0\n",
      "physical_activity_minutes_per_week    0\n",
      "diet_score                            0\n",
      "sleep_hours_per_day                   0\n",
      "screen_time_hours_per_day             0\n",
      "bmi                                   0\n",
      "waist_to_hip_ratio                    0\n",
      "systolic_bp                           0\n",
      "diastolic_bp                          0\n",
      "heart_rate                            0\n",
      "cholesterol_total                     0\n",
      "hdl_cholesterol                       0\n",
      "ldl_cholesterol                       0\n",
      "triglycerides                         0\n",
      "gender                                0\n",
      "ethnicity                             0\n",
      "education_level                       0\n",
      "income_level                          0\n",
      "smoking_status                        0\n",
      "employment_status                     0\n",
      "family_history_diabetes               0\n",
      "hypertension_history                  0\n",
      "cardiovascular_history                0\n",
      "diagnosed_diabetes                    0\n",
      "dtype: int64\n",
      "\n",
      "##### Test data missing values #####\n",
      "age                                   0\n",
      "alcohol_consumption_per_week          0\n",
      "physical_activity_minutes_per_week    0\n",
      "diet_score                            0\n",
      "sleep_hours_per_day                   0\n",
      "screen_time_hours_per_day             0\n",
      "bmi                                   0\n",
      "waist_to_hip_ratio                    0\n",
      "systolic_bp                           0\n",
      "diastolic_bp                          0\n",
      "heart_rate                            0\n",
      "cholesterol_total                     0\n",
      "hdl_cholesterol                       0\n",
      "ldl_cholesterol                       0\n",
      "triglycerides                         0\n",
      "gender                                0\n",
      "ethnicity                             0\n",
      "education_level                       0\n",
      "income_level                          0\n",
      "smoking_status                        0\n",
      "employment_status                     0\n",
      "family_history_diabetes               0\n",
      "hypertension_history                  0\n",
      "cardiovascular_history                0\n",
      "dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for (dataset_name, dataset) in [('Train data', orig_train_data), ('Test data', orig_test_data)]:\n",
    "    print(f\"##### {dataset_name} missing values #####\")\n",
    "    print(dataset.isnull().sum())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b2826622",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T02:33:06.549669Z",
     "iopub.status.busy": "2025-12-10T02:33:06.548818Z",
     "iopub.status.idle": "2025-12-10T02:33:06.912084Z",
     "shell.execute_reply": "2025-12-10T02:33:06.910613Z"
    },
    "papermill": {
     "duration": 0.379412,
     "end_time": "2025-12-10T02:33:06.913930",
     "exception": false,
     "start_time": "2025-12-10T02:33:06.534518",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Train data categorical cols unique values #####\n",
      "gender:\n",
      "['Female' 'Male' 'Other']\n",
      "ethnicity:\n",
      "['Hispanic' 'White' 'Asian' 'Black' 'Other']\n",
      "education_level:\n",
      "['Highschool' 'Graduate' 'Postgraduate' 'No formal']\n",
      "income_level:\n",
      "['Lower-Middle' 'Upper-Middle' 'Low' 'Middle' 'High']\n",
      "smoking_status:\n",
      "['Current' 'Never' 'Former']\n",
      "employment_status:\n",
      "['Employed' 'Retired' 'Student' 'Unemployed']\n",
      "\n",
      "##### Test data categorical cols unique values #####\n",
      "gender:\n",
      "['Female' 'Male' 'Other']\n",
      "ethnicity:\n",
      "['White' 'Hispanic' 'Black' 'Asian' 'Other']\n",
      "education_level:\n",
      "['Highschool' 'Graduate' 'Postgraduate' 'No formal']\n",
      "income_level:\n",
      "['Middle' 'Low' 'Lower-Middle' 'Upper-Middle' 'High']\n",
      "smoking_status:\n",
      "['Former' 'Never' 'Current']\n",
      "employment_status:\n",
      "['Employed' 'Unemployed' 'Retired' 'Student']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for (dataset_name, dataset) in [('Train data', orig_train_data), ('Test data', orig_test_data)]:\n",
    "    print(f\"##### {dataset_name} categorical cols unique values #####\")\n",
    "    for categorical_col_name in categorical_col_names:\n",
    "        print(f\"{categorical_col_name}:\")\n",
    "        print(dataset[categorical_col_name].unique())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c21f1e2d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T02:33:06.943581Z",
     "iopub.status.busy": "2025-12-10T02:33:06.943201Z",
     "iopub.status.idle": "2025-12-10T02:33:06.950134Z",
     "shell.execute_reply": "2025-12-10T02:33:06.948805Z"
    },
    "papermill": {
     "duration": 0.024313,
     "end_time": "2025-12-10T02:33:06.952335",
     "exception": false,
     "start_time": "2025-12-10T02:33:06.928022",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# KDE plots of target variable and numerical features\n",
    "if not SKIP_PLOTS:\n",
    "    plt.figure(figsize=(12, 24))\n",
    "    kdeplot_col_names = [target_col]\n",
    "    kdeplot_col_names.extend(numeric_col_names)\n",
    "    for i, col in enumerate(kdeplot_col_names, start=1):\n",
    "        plt.subplot(10, 2, i)\n",
    "        sns.kdeplot(data=orig_train_data, x=col, fill=True)\n",
    "        plt.tight_layout()\n",
    "        plt.title(f\"KDE plot of {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cd7b6633",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T02:33:06.984847Z",
     "iopub.status.busy": "2025-12-10T02:33:06.983751Z",
     "iopub.status.idle": "2025-12-10T02:33:06.989649Z",
     "shell.execute_reply": "2025-12-10T02:33:06.988743Z"
    },
    "papermill": {
     "duration": 0.023067,
     "end_time": "2025-12-10T02:33:06.991536",
     "exception": false,
     "start_time": "2025-12-10T02:33:06.968469",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not SKIP_PLOTS:\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.heatmap(\n",
    "        orig_train_data[numeric_col_names].corr(),\n",
    "        cmap='Reds',\n",
    "        annot=True,\n",
    "        linewidths=2,\n",
    "        fmt='.2f',\n",
    "        vmin=-1,\n",
    "        vmax=1\n",
    "    )\n",
    "    plt.title('Correlation Matrix of Numerical Features', fontsize=18, pad=20)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ac37e0",
   "metadata": {
    "papermill": {
     "duration": 0.01334,
     "end_time": "2025-12-10T02:33:07.018488",
     "exception": false,
     "start_time": "2025-12-10T02:33:07.005148",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 4. Data Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eb3c1384",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T02:33:07.048284Z",
     "iopub.status.busy": "2025-12-10T02:33:07.047945Z",
     "iopub.status.idle": "2025-12-10T02:33:07.211781Z",
     "shell.execute_reply": "2025-12-10T02:33:07.210728Z"
    },
    "papermill": {
     "duration": 0.181745,
     "end_time": "2025-12-10T02:33:07.214823",
     "exception": false,
     "start_time": "2025-12-10T02:33:07.033078",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data = orig_train_data.copy()\n",
    "test_data = orig_test_data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8479f3eb",
   "metadata": {
    "papermill": {
     "duration": 0.0148,
     "end_time": "2025-12-10T02:33:07.248978",
     "exception": false,
     "start_time": "2025-12-10T02:33:07.234178",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 4.1 Ordinal Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "14407503",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T02:33:07.278040Z",
     "iopub.status.busy": "2025-12-10T02:33:07.277685Z",
     "iopub.status.idle": "2025-12-10T02:33:08.294622Z",
     "shell.execute_reply": "2025-12-10T02:33:08.293539Z"
    },
    "papermill": {
     "duration": 1.033431,
     "end_time": "2025-12-10T02:33:08.296599",
     "exception": false,
     "start_time": "2025-12-10T02:33:07.263168",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "education_level_encoded:\n",
      "{'No formal': 0, 'Highschool': 1, 'Graduate': 2, 'Postgraduate': 3}\n",
      "income_level_encoded:\n",
      "{'Low': 0, 'Lower-Middle': 1, 'Middle': 2, 'Upper-Middle': 3, 'High': 4}\n",
      "smoking_status_encoded:\n",
      "{'Never': 0, 'Former': 1, 'Current': 2}\n"
     ]
    }
   ],
   "source": [
    "# education level\n",
    "education_level_encoder = OrdinalEncoder(categories=[['No formal', 'Highschool', 'Graduate', 'Postgraduate']])\n",
    "train_data['education_level_encoded'] = education_level_encoder.fit_transform(train_data[['education_level']])\n",
    "test_data['education_level_encoded'] = education_level_encoder.fit_transform(test_data[['education_level']])\n",
    "\n",
    "# income level\n",
    "income_level_encoder = OrdinalEncoder(categories=[['Low', 'Lower-Middle','Middle', 'Upper-Middle', 'High']])\n",
    "train_data['income_level_encoded'] = income_level_encoder.fit_transform(train_data[['income_level']])\n",
    "test_data['income_level_encoded'] = income_level_encoder.fit_transform(test_data[['income_level']])\n",
    "\n",
    "# smoking status\n",
    "smoking_status_encoder = OrdinalEncoder(categories=[['Never', 'Former', 'Current']])\n",
    "train_data['smoking_status_encoded'] = smoking_status_encoder.fit_transform(train_data[['smoking_status']])\n",
    "test_data['smoking_status_encoded'] = smoking_status_encoder.fit_transform(test_data[['smoking_status']])\n",
    "\n",
    "# drop original cols\n",
    "for col in ['income_level', 'education_level', 'smoking_status']:\n",
    "    train_data.drop(col, axis=1, inplace=True)\n",
    "    test_data.drop(col, axis=1, inplace=True)\n",
    "\n",
    "# print out value maps to check assigned values are as expected\n",
    "for (encoded_col_name, encoder) in [\n",
    "    ('education_level_encoded', education_level_encoder),\n",
    "    ('income_level_encoded', income_level_encoder),\n",
    "    ('smoking_status_encoded', smoking_status_encoder),\n",
    "]:\n",
    "    categories = encoder.categories_[0]\n",
    "    value_map = { category: i for i, category in enumerate(categories) }\n",
    "    print(f\"{encoded_col_name}:\\n{value_map}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80bdc8c4",
   "metadata": {
    "papermill": {
     "duration": 0.013365,
     "end_time": "2025-12-10T02:33:08.323457",
     "exception": false,
     "start_time": "2025-12-10T02:33:08.310092",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 4.3 Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d1eb9c21",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T02:33:08.351890Z",
     "iopub.status.busy": "2025-12-10T02:33:08.351275Z",
     "iopub.status.idle": "2025-12-10T02:33:08.371508Z",
     "shell.execute_reply": "2025-12-10T02:33:08.370585Z"
    },
    "papermill": {
     "duration": 0.036346,
     "end_time": "2025-12-10T02:33:08.373241",
     "exception": false,
     "start_time": "2025-12-10T02:33:08.336895",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fix_blood_pressure(df):\n",
    "    mask = df['diastolic_bp'] > df['systolic_bp']\n",
    "    df.loc[mask, ['systolic_bp', 'diastolic_bp']] = (\n",
    "        df.loc[mask, ['diastolic_bp', 'systolic_bp']].values\n",
    "    )\n",
    "    return df\n",
    "\n",
    "train_data = fix_blood_pressure(train_data)\n",
    "test_data = fix_blood_pressure(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ac1d65",
   "metadata": {
    "papermill": {
     "duration": 0.013154,
     "end_time": "2025-12-10T02:33:08.399782",
     "exception": false,
     "start_time": "2025-12-10T02:33:08.386628",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 4.4 Feature Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ac6542ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T02:33:08.427750Z",
     "iopub.status.busy": "2025-12-10T02:33:08.427394Z",
     "iopub.status.idle": "2025-12-10T02:33:08.436369Z",
     "shell.execute_reply": "2025-12-10T02:33:08.435360Z"
    },
    "papermill": {
     "duration": 0.025423,
     "end_time": "2025-12-10T02:33:08.438492",
     "exception": false,
     "start_time": "2025-12-10T02:33:08.413069",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_generated_features(df):\n",
    "    # log transforms for skewed data\n",
    "    for col in ['triglycerides', 'ldl_cholesterol', 'cholesterol_total']:\n",
    "        df[f'log_{col}'] = np.log1p(df[col])\n",
    "\n",
    "    # medical ratios & interactions\n",
    "    df['cholesterol_ratio'] = df['cholesterol_total'] / (df['hdl_cholesterol'] + 1e-5)\n",
    "    df['ldl_hdl_ratio'] = df['ldl_cholesterol'] / (df['hdl_cholesterol'] + 1e-5)\n",
    "    df['pulse_pressure'] = df['systolic_bp'] - df['diastolic_bp']\n",
    "    df['mean_arterial_pressure'] = (df['systolic_bp'] + 2 * df['diastolic_bp']) / 3\n",
    "    df['age_x_bmi'] = df['age'] * df['bmi']\n",
    "    df['waist_x_bmi'] = df['waist_to_hip_ratio'] * df['bmi']\n",
    "    df['family_history_diabetes_x_log_triglycerides'] = df['family_history_diabetes'] * df['log_triglycerides']\n",
    "    df['hypertension_history_x_systolic_bp'] = df['hypertension_history'] * df['systolic_bp']\n",
    "    df['activity_x_diet'] = df['physical_activity_minutes_per_week'] * df['diet_score']\n",
    "\n",
    "    # squared\n",
    "    df['age_sq'] = df['age'] ** 2\n",
    "    df['bmi_sq'] = df['bmi'] ** 2\n",
    "    df['waist_to_hip_ratio_sq'] = df['waist_to_hip_ratio'] ** 2\n",
    "    df['systolic_bp_sq'] = df['systolic_bp'] ** 2\n",
    "\n",
    "    # risk grouping\n",
    "    df['comorbidity_count'] = (\n",
    "        df['hypertension_history'] + \n",
    "        df['cardiovascular_history'] + \n",
    "        df['family_history_diabetes']\n",
    "    )\n",
    "\n",
    "    # binning\n",
    "    df['bmi_cat'] = pd.cut(df['bmi'], bins=[-1, 25, 30, 100], labels=[0, 1, 2]).astype(int)\n",
    "    bmi_cat_encoder = OrdinalEncoder(categories=[[0, 1, 2]])\n",
    "    df['bmi_cat_encoded'] = bmi_cat_encoder.fit_transform(df[['bmi_cat']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cb49f55d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T02:33:08.467887Z",
     "iopub.status.busy": "2025-12-10T02:33:08.467427Z",
     "iopub.status.idle": "2025-12-10T02:33:08.474586Z",
     "shell.execute_reply": "2025-12-10T02:33:08.473516Z"
    },
    "papermill": {
     "duration": 0.041126,
     "end_time": "2025-12-10T02:33:08.493937",
     "exception": false,
     "start_time": "2025-12-10T02:33:08.452811",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_kmeans_features(train_df, test_df, n_clusters):\n",
    "    features_to_cluster = [\n",
    "        'age', 'bmi', 'mean_arterial_pressure', 'cholesterol_ratio', 'log_triglycerides'\n",
    "    ]\n",
    "    \n",
    "    combined = pd.concat([train_df[features_to_cluster], test_df[features_to_cluster]], axis=0)\n",
    "    scaler = StandardScaler()\n",
    "    scaled_data = scaler.fit_transform(combined)\n",
    "    \n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=RANDOM_SEEDS[0], n_init=10)\n",
    "    clusters = kmeans.fit_predict(scaled_data)\n",
    "\n",
    "    train_df['cluster_label'] = clusters[:len(train_df)].astype(object)\n",
    "    test_df['cluster_label'] = clusters[len(train_df):].astype(object)\n",
    "\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f012d8e1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T02:33:08.523398Z",
     "iopub.status.busy": "2025-12-10T02:33:08.522603Z",
     "iopub.status.idle": "2025-12-10T02:33:23.546564Z",
     "shell.execute_reply": "2025-12-10T02:33:23.545057Z"
    },
    "papermill": {
     "duration": 15.040782,
     "end_time": "2025-12-10T02:33:23.548247",
     "exception": false,
     "start_time": "2025-12-10T02:33:08.507465",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# add generated features\n",
    "add_generated_features(train_data)\n",
    "add_generated_features(test_data)\n",
    "\n",
    "# apply clustering\n",
    "train_data, test_data = add_kmeans_features(train_data, test_data, n_clusters=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bf083311",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T02:33:23.576529Z",
     "iopub.status.busy": "2025-12-10T02:33:23.576220Z",
     "iopub.status.idle": "2025-12-10T02:33:23.583098Z",
     "shell.execute_reply": "2025-12-10T02:33:23.581991Z"
    },
    "papermill": {
     "duration": 0.022705,
     "end_time": "2025-12-10T02:33:23.584558",
     "exception": false,
     "start_time": "2025-12-10T02:33:23.561853",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['age', 'alcohol_consumption_per_week', 'physical_activity_minutes_per_week', 'diet_score', 'sleep_hours_per_day', 'screen_time_hours_per_day', 'bmi', 'waist_to_hip_ratio', 'systolic_bp', 'diastolic_bp', 'heart_rate', 'cholesterol_total', 'hdl_cholesterol', 'ldl_cholesterol', 'triglycerides', 'gender', 'ethnicity', 'employment_status', 'family_history_diabetes', 'hypertension_history', 'cardiovascular_history', 'diagnosed_diabetes', 'education_level_encoded', 'income_level_encoded', 'smoking_status_encoded', 'log_triglycerides', 'log_ldl_cholesterol', 'log_cholesterol_total', 'cholesterol_ratio', 'ldl_hdl_ratio', 'pulse_pressure', 'mean_arterial_pressure', 'age_x_bmi', 'waist_x_bmi', 'family_history_diabetes_x_log_triglycerides', 'hypertension_history_x_systolic_bp', 'activity_x_diet', 'age_sq', 'bmi_sq', 'waist_to_hip_ratio_sq', 'systolic_bp_sq', 'comorbidity_count', 'bmi_cat', 'bmi_cat_encoded', 'cluster_label'], dtype='object')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35fa381d",
   "metadata": {
    "papermill": {
     "duration": 0.013159,
     "end_time": "2025-12-10T02:33:23.611414",
     "exception": false,
     "start_time": "2025-12-10T02:33:23.598255",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 4.4 Remaining Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "af08f4c7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T02:33:23.639776Z",
     "iopub.status.busy": "2025-12-10T02:33:23.639349Z",
     "iopub.status.idle": "2025-12-10T02:33:24.183910Z",
     "shell.execute_reply": "2025-12-10T02:33:24.182789Z"
    },
    "papermill": {
     "duration": 0.56079,
     "end_time": "2025-12-10T02:33:24.185675",
     "exception": false,
     "start_time": "2025-12-10T02:33:23.624885",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cat_features = train_data.drop(target_col, axis=1).select_dtypes(include='object').columns.to_list()\n",
    "if len(cat_features) > 0:\n",
    "    for col in cat_features:\n",
    "        train_data[col] = train_data[col].astype('category')\n",
    "        test_data[col] = test_data[col].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302620a2",
   "metadata": {
    "papermill": {
     "duration": 0.013271,
     "end_time": "2025-12-10T02:33:24.213132",
     "exception": false,
     "start_time": "2025-12-10T02:33:24.199861",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 4.5 MLP Data Preparation\n",
    "\n",
    "Since MLP cannot handle categorical features and requires the data to be scaled, the training and test data that will be used for it are prepared as separate dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "930e5563",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T02:33:24.241345Z",
     "iopub.status.busy": "2025-12-10T02:33:24.241023Z",
     "iopub.status.idle": "2025-12-10T02:33:26.398205Z",
     "shell.execute_reply": "2025-12-10T02:33:26.397037Z"
    },
    "papermill": {
     "duration": 2.173471,
     "end_time": "2025-12-10T02:33:26.399971",
     "exception": false,
     "start_time": "2025-12-10T02:33:24.226500",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prepare_mlp_data(df, cat_features):\n",
    "    df = df.copy()\n",
    "    df = pd.get_dummies(df, columns=cat_features, drop_first=True, dtype=int)\n",
    "    cols_to_scale = [c for c in df.columns if c != 'diagnosed_diabetes']\n",
    "    scaler = StandardScaler()\n",
    "    df[cols_to_scale] = scaler.fit_transform(df[cols_to_scale])\n",
    "    return df\n",
    "\n",
    "train_data_mlp = prepare_mlp_data(train_data, cat_features)\n",
    "test_data_mlp = prepare_mlp_data(test_data, cat_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "786038e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T02:33:26.429336Z",
     "iopub.status.busy": "2025-12-10T02:33:26.428184Z",
     "iopub.status.idle": "2025-12-10T02:33:26.434934Z",
     "shell.execute_reply": "2025-12-10T02:33:26.434025Z"
    },
    "papermill": {
     "duration": 0.022859,
     "end_time": "2025-12-10T02:33:26.436590",
     "exception": false,
     "start_time": "2025-12-10T02:33:26.413731",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['age', 'alcohol_consumption_per_week', 'physical_activity_minutes_per_week', 'diet_score', 'sleep_hours_per_day', 'screen_time_hours_per_day', 'bmi', 'waist_to_hip_ratio', 'systolic_bp', 'diastolic_bp', 'heart_rate', 'cholesterol_total', 'hdl_cholesterol', 'ldl_cholesterol', 'triglycerides', 'family_history_diabetes', 'hypertension_history', 'cardiovascular_history', 'diagnosed_diabetes', 'education_level_encoded', 'income_level_encoded', 'smoking_status_encoded', 'log_triglycerides', 'log_ldl_cholesterol', 'log_cholesterol_total', 'cholesterol_ratio', 'ldl_hdl_ratio', 'pulse_pressure', 'mean_arterial_pressure', 'age_x_bmi', 'waist_x_bmi', 'family_history_diabetes_x_log_triglycerides', 'hypertension_history_x_systolic_bp', 'activity_x_diet', 'age_sq', 'bmi_sq', 'waist_to_hip_ratio_sq', 'systolic_bp_sq', 'comorbidity_count', 'bmi_cat', 'bmi_cat_encoded', 'gender_Male', 'gender_Other', 'ethnicity_Black', 'ethnicity_Hispanic', 'ethnicity_Other', 'ethnicity_White',\n",
       "       'employment_status_Retired', 'employment_status_Student', 'employment_status_Unemployed', 'cluster_label_1', 'cluster_label_2', 'cluster_label_3', 'cluster_label_4', 'cluster_label_5', 'cluster_label_6'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_mlp.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f51c8c6",
   "metadata": {
    "papermill": {
     "duration": 0.013496,
     "end_time": "2025-12-10T02:33:26.463846",
     "exception": false,
     "start_time": "2025-12-10T02:33:26.450350",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 5. Stacking Initial Setup\n",
    "\n",
    "We'll use stacking, an [ensemble learning](https://en.wikipedia.org/wiki/Ensemble_learning) strategy, to generate the predictions. As we'll need to gather predictions from various base models (a.k.a. level-0 models) to feed as input features to a meta model (a.k.a. level-1 model), in order to streamline the process of experimenting with different combinations of base models, some helper classes will be defined in this section. These classes can also be found [here](https://github.com/chuo-v/machine-learning-utils/blob/master/ensemble-learning/stacking/stacking_predictions_retriever.py) at one of my GitHub repositories used to organize some utilities I implemented for machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "34b3ca7b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T02:33:26.493036Z",
     "iopub.status.busy": "2025-12-10T02:33:26.492682Z",
     "iopub.status.idle": "2025-12-10T02:33:26.528750Z",
     "shell.execute_reply": "2025-12-10T02:33:26.527756Z"
    },
    "papermill": {
     "duration": 0.052948,
     "end_time": "2025-12-10T02:33:26.530408",
     "exception": false,
     "start_time": "2025-12-10T02:33:26.477460",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class StackingEstimator:\n",
    "    \"\"\"\n",
    "    A class representing an estimator that will be used for stacking, an ensemble learning strategy.\n",
    "\n",
    "    Intended to be used in conjunction with the `StackingPredictionsRetriever` class, which helps\n",
    "    retrieve predictions for multiple instances of `StackingEstimator`; as the predictions are saved\n",
    "    in files, on subsequent requests to retrieve predictions, even as the set of estimators has been\n",
    "    modified, the `StackingPredictionsRetriever` class can determine the predictions of estimators\n",
    "    that are non-stale and available (if any) by using the `get_hash` method of the `StackingEstimator`\n",
    "    class to determine the relevance and staleness of any saved predictions.\n",
    "\n",
    "    Proper usage of this class requires one important condition to be satisfied: the predictions made\n",
    "    using the estimator are determinstic, i.e. they are exactly the same everytime the estimator is\n",
    "    run with the same inputs (`name`, `params_dict`, `feature_names`, `get_predictions`).\n",
    "    \"\"\"\n",
    "    name = \"\"\n",
    "    params_dict = {}\n",
    "    feature_names = []\n",
    "    get_predictions = lambda: None\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        name: str,\n",
    "        feature_names: [str],\n",
    "        params_dict: {},\n",
    "        get_preds: FunctionType\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializes a new instance of `StackingEstimator`.\n",
    "\n",
    "        :param name:\n",
    "            A string representing a name for the estimator. It is used for the column names of\n",
    "            the training and test predictions for each estimator, and is also used as an input\n",
    "            to calculate a hash value for the estimator. It is recommended to use a different\n",
    "            name from the names used for other estimators passed to `StackingPredictionsRetriever`.\n",
    "        :param feature_names:\n",
    "            A list of strings representing the names of the features that will be used for the\n",
    "            estimator. It will be passed as an argument to `get_preds`. Internally, it is only\n",
    "            used as an input to calculate a hash value for the estimator.\n",
    "        :param params_dict:\n",
    "            A dictionary of parameters that will be specified for the estimator. It will be\n",
    "            passed as an argument to `get_preds`. Internally, it is only used as an input\n",
    "            to calculate a hash value for the estimator.\n",
    "        :param get_preds:\n",
    "            A function for getting the predictions for the estimator. It should only take two\n",
    "            arguments: 'params_dict' and 'feature_names', and should return predictions for\n",
    "            the training and test data (in that order) as a tuple of two `pandas.Series`.\n",
    "        \"\"\"\n",
    "        # parameter check\n",
    "        if not isinstance(name, str):\n",
    "            raise ValueError(\"`name` argument should be of type `str`\")\n",
    "        if not isinstance(feature_names, list):\n",
    "            raise ValueError(f\"`feature_names` argument for estimator \\\"{name}\\\" should be of type `list`\")\n",
    "        elif not all(isinstance(feature_name, str) for feature_name in feature_names):\n",
    "            raise ValueError(f\"`feature_names` argument for estimator \\\"{name}\\\" should only contain instances of `str`\")\n",
    "        if not isinstance(params_dict, dict):\n",
    "            raise ValueError(f\"`params_dict` argument for estimator \\\"{name}\\\" should be of type `dict`\")\n",
    "        get_preds_params = inspect.signature(get_preds).parameters.values()\n",
    "        get_preds_param_names = [param.name for param in get_preds_params]\n",
    "        if len(get_preds_param_names) != 2:\n",
    "            raise ValueError(f\"`get_preds` function for estimator \\\"{name}\\\" should take two arguments\")\n",
    "        elif \"params_dict\" not in get_preds_param_names:\n",
    "            raise ValueError(f\"`get_preds` function for estimator \\\"{name}\\\" should take a \\\"params_dict\\\" argument\")\n",
    "        elif \"feature_names\" not in get_preds_param_names:\n",
    "            raise ValueError(f\"`get_preds` function for estimator \\\"{name}\\\" should take a \\\"feature_names\\\" argument\")\n",
    "\n",
    "        self.name = name\n",
    "        self.feature_names = feature_names\n",
    "        self.params_dict = params_dict\n",
    "        self.get_preds = get_preds\n",
    "\n",
    "    def get_hash_value(self):\n",
    "        \"\"\"\n",
    "        Calculates and returns a hash value for the estimator using\n",
    "        `name`, `feature_names` and `params_dict` as inputs.\n",
    "        \"\"\"\n",
    "        feature_names_str = \"_\".join(sorted(self.feature_names))\n",
    "        params_dict_str = \"_\".join(f\"{key}-{value}\" for (key, value) in sorted(self.params_dict.items()))\n",
    "        hash_input_str = \"_\".join([self.name, feature_names_str, params_dict_str])\n",
    "        md5_hash = hl.md5(hash_input_str.encode('utf-8')).hexdigest()\n",
    "        return md5_hash\n",
    "\n",
    "class StackingPredictionsRetriever:\n",
    "    \"\"\"\n",
    "    A class for streamlining stacking (an ensemble learning strategy) that saves predictions\n",
    "    from estimators to file so that when trying out different combinations of (base) estimators,\n",
    "    the predictions that are not stale can be reused, saving the time of having the estimators\n",
    "    make predictions again.\n",
    "\n",
    "    Intended to be used in conjunction with the `StackingEstimator` class. The `hash_value` of\n",
    "    `StackingEstimator` is used to determine the staleness and relevance of the predictions for\n",
    "    an estimator. The implementation for making predictions using an estimator needs to be\n",
    "    provided as a function to `get_preds` for `StackingEstimator`; when predictions need to be\n",
    "    made using an estimator, this class will call `get_preds` for the `StackingEstimator` instance.\n",
    "\n",
    "    Proper usage of this class requires one important condition to be satisfied: the predictions made\n",
    "    using the estimators are determinstic, i.e. they are exactly the same everytime a\n",
    "    `StackingEstimator` instance is run with the same inputs.\n",
    "    \"\"\"\n",
    "    estimators = []\n",
    "    working_dir_path = \"\"\n",
    "    train_preds_filename = \"\"\n",
    "    test_preds_filename = \"\"\n",
    "    preds_save_interval = 0\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        estimators: [StackingEstimator],\n",
    "        working_dir_path: str,\n",
    "        train_preds_filename: str = \"train_preds\",\n",
    "        test_preds_filename: str = \"test_preds\",\n",
    "        preds_save_interval: int = 5,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializes a new instance of `StackingPredictionsRetriever`.\n",
    "\n",
    "        :param estimators:\n",
    "            A list of `StackingEstimator` instances for which the class will retrieve predictions.\n",
    "        :param working_dir_path:\n",
    "            The path for the working directory where the files with predictions will be saved.\n",
    "        :param train_preds_filename:\n",
    "            The name of the file in which predictions for the training set will be stored.\n",
    "        :param test_preds_filename:\n",
    "            The name of the file in which predictions for the test set will be stored.\n",
    "        :param preds_save_interval:\n",
    "            An integer which specifies the interval at which predictions will be saved when\n",
    "            `get_preds` is called, corresponding to the number of estimators whose predictions\n",
    "            have been retrieved since the predictions were previously saved. Any estimators\n",
    "            whose predictions are not stale and therefore were not required to make predictions\n",
    "            again are not included in this number.\n",
    "        \"\"\"\n",
    "        # parameter check\n",
    "        if not isinstance(estimators, list):\n",
    "            raise ValueError(\"`estimators` must be passed as a list\")\n",
    "        if not all(isinstance(e, StackingEstimator) for e in estimators):\n",
    "            raise ValueError(\"`estimators` should only contain instances of `StackingEstimator`\")\n",
    "        if not isinstance(working_dir_path, str):\n",
    "            raise ValueError(\"`working_dir_path` argument should be of type `str`\")\n",
    "        if not isinstance(preds_save_interval, int):\n",
    "            raise ValueError(\"`preds_save_interval` argument should be of type `int`\")\n",
    "\n",
    "        self.estimators = estimators\n",
    "        self.working_dir_path = working_dir_path\n",
    "        self.train_preds_filename = train_preds_filename\n",
    "        self.test_preds_filename = test_preds_filename\n",
    "        self.preds_save_interval = preds_save_interval\n",
    "\n",
    "    def get_train_preds_file_path(self):\n",
    "        \"\"\"\n",
    "        Returns the file path for storing predictions for training data.\n",
    "        \"\"\"\n",
    "        return Path(f\"{self.working_dir_path}/{self.train_preds_filename}.csv\")\n",
    "\n",
    "    def get_test_preds_file_path(self):\n",
    "        \"\"\"\n",
    "        Returns the file path for storing predictions for test data.\n",
    "        \"\"\"\n",
    "        return Path(f\"{self.working_dir_path}/{self.test_preds_filename}.csv\")\n",
    "\n",
    "    def get_current_train_and_test_preds(self):\n",
    "        \"\"\"\n",
    "        Returns the current predictions for training and test data (in that order)\n",
    "        as a tuple of two `pandas.DataFrame`.\n",
    "\n",
    "        The predictions are attempted to be retrieved from the file paths returned\n",
    "        by `get_train_preds_file_path` and `get_test_preds_file_path`; if there are\n",
    "        any issues with doing so (e.g. file does not exist, dataframe is empty),\n",
    "        empty dataframes will be returned instead.\n",
    "        In the case an `pandas.errors.EmptyDataError` exception is raised when\n",
    "        reading from a file, the corresponding file will be removed.\n",
    "        \"\"\"\n",
    "        curr_train_preds = pd.DataFrame()\n",
    "        curr_test_preds = pd.DataFrame()\n",
    "        train_preds_file_path = self.get_train_preds_file_path()\n",
    "        test_preds_file_path = self.get_test_preds_file_path()\n",
    "\n",
    "        if train_preds_file_path.is_file():\n",
    "            try:\n",
    "                curr_train_preds = pd.read_csv(train_preds_file_path)\n",
    "            except pd.errors.EmptyDataError:\n",
    "                train_preds_file_path.unlink()\n",
    "        if test_preds_file_path.is_file():\n",
    "            try:\n",
    "                curr_test_preds = pd.read_csv(test_preds_file_path)\n",
    "            except pd.errors.EmptyDataError:\n",
    "                test_preds_file_path.unlink()\n",
    "\n",
    "        return curr_train_preds, curr_test_preds\n",
    "\n",
    "    def get_preds(self):\n",
    "        \"\"\"\n",
    "        Retrieves predictions from all estimators in `estimators`, storing them in\n",
    "        two files at the file paths specified by `working_dir_path`,\n",
    "        `train_preds_filename` and `test_preds_filename`.\n",
    "\n",
    "        If non-stale (relevant) predictions are found for an estimator, retrieval\n",
    "        of predictions by calling `get_preds` on the estimator will be skipped,\n",
    "        and the existing predictions for the estimator will be kept.\n",
    "        \"\"\"\n",
    "        print(\"[INFO] Getting predictions..\")\n",
    "        curr_train_preds, curr_test_preds = self.get_current_train_and_test_preds()\n",
    "\n",
    "        preds_retrieved_count = 0\n",
    "        num_preds_retrieved_but_not_yet_saved = 0\n",
    "        estimators_skipped = []\n",
    "\n",
    "        for estimator in self.estimators:\n",
    "            estimator_hash_value = estimator.get_hash_value()\n",
    "            estimator_name = f\"{estimator.name} ({estimator_hash_value})\"\n",
    "\n",
    "            # skip retrieving predictions for estimator if non-stale predictions are already available\n",
    "            train_preds_available = any(estimator_hash_value in col_name for col_name in curr_train_preds.columns)\n",
    "            test_preds_available = any(estimator_hash_value in col_name for col_name in curr_test_preds.columns)\n",
    "            if train_preds_available and test_preds_available:\n",
    "                estimators_skipped += [estimator_name]\n",
    "                continue\n",
    "\n",
    "            print(f\"[INFO] Getting predictions for estimator {estimator_name}\")\n",
    "            train_preds, test_preds = estimator.get_preds(estimator.params_dict, estimator.feature_names)\n",
    "            if not isinstance(train_preds, pd.core.series.Series):\n",
    "                raise ValueError(\"`train_preds` should be of type `pandas.Series`\")\n",
    "            if not isinstance(test_preds, pd.core.series.Series):\n",
    "                raise ValueError(\"`test_preds` should be of type `pandas.Series`\")\n",
    "            curr_train_preds[estimator_name] = train_preds\n",
    "            curr_test_preds[estimator_name] = test_preds\n",
    "            preds_retrieved_count += 1\n",
    "\n",
    "            # save predictions at an interval of `preds_save_interval`\n",
    "            if preds_retrieved_count % self.preds_save_interval == 0:\n",
    "                curr_train_preds.sort_index(axis=1).to_csv(self.get_train_preds_file_path(), index=False)\n",
    "                curr_test_preds.sort_index(axis=1).to_csv(self.get_test_preds_file_path(), index=False)\n",
    "                num_preds_retrieved_but_not_yet_saved = 0\n",
    "                print(\"[INFO] Saved predictions\")\n",
    "            else:\n",
    "                num_preds_retrieved_but_not_yet_saved += 1\n",
    "\n",
    "        if estimators_skipped:\n",
    "            estimators_skipped.sort()\n",
    "            formatted_estimators = \", \".join(estimators_skipped)\n",
    "            print(f\"[INFO] Skipped retrieving predictions for following estimators as their current ones are not stale:\\n{formatted_estimators}\")\n",
    "\n",
    "        if num_preds_retrieved_but_not_yet_saved != 0:\n",
    "            curr_train_preds.sort_index(axis=1).to_csv(self.get_train_preds_file_path(), index=False)\n",
    "            curr_test_preds.sort_index(axis=1).to_csv(self.get_test_preds_file_path(), index=False)\n",
    "            print(\"[INFO] Saved predictions\")\n",
    "\n",
    "        print(\"[INFO] Finished getting all predictions\")\n",
    "\n",
    "    def sync_preds(self):\n",
    "        \"\"\"\n",
    "        Syncs the predictions stored at the two file paths specified by\n",
    "        `working_dir_path`, `train_preds_filename` and `test_preds_filename` by\n",
    "        removing predictions for any estimator that is not currently in `estimators`.\n",
    "\n",
    "        Note that new predictions for estimators that do not currently have predictions\n",
    "        in the files will not be added; `get_preds` should be used for this purpose\n",
    "        instead.\n",
    "        \"\"\"\n",
    "        print(\"[INFO] Syncing predictions..\")\n",
    "        estimator_hash_values = [estimator.get_hash_value() for estimator in self.estimators]\n",
    "        should_remove_col = lambda col_name: not any(hash_value in col_name for hash_value in estimator_hash_values)\n",
    "\n",
    "        curr_train_preds, curr_test_preds = self.get_current_train_and_test_preds()\n",
    "\n",
    "        if not curr_train_preds.empty:\n",
    "            col_names_to_remove = [col_name for col_name in curr_train_preds.columns if should_remove_col(col_name)]\n",
    "            if col_names_to_remove:\n",
    "                print(f\"[INFO] Dropping columns for following estimators from training predictions:\\n{col_names_to_remove}\")\n",
    "                curr_train_preds.drop(columns=col_names_to_remove, inplace=True)\n",
    "                curr_train_preds.to_csv(self.get_train_preds_file_path(), index=False)\n",
    "            else:\n",
    "                print(f\"[INFO] No columns for training predictions were dropped\")\n",
    "        if not curr_test_preds.empty:\n",
    "            col_names_to_remove = [col_name for col_name in curr_test_preds.columns if should_remove_col(col_name)]\n",
    "            if col_names_to_remove:\n",
    "                print(f\"[INFO] Dropping columns for following estimators from test predictions:\\n{col_names_to_remove}\")\n",
    "                curr_test_preds.drop(columns=col_names_to_remove, inplace=True)\n",
    "                curr_test_preds.to_csv(self.get_test_preds_file_path(), index=False)\n",
    "            else:\n",
    "                print(f\"[INFO] No columns for test predictions were dropped\")\n",
    "\n",
    "        print(\"[INFO] Finished syncing predictions\")\n",
    "\n",
    "    def import_preds(self, input_dir_path):\n",
    "        \"\"\"\n",
    "        Imports predictions stored at the two file paths at `input_dir_path` with\n",
    "        `train_preds_filename` and `test_preds_filename` as their filenames. If no\n",
    "        such files are found, no predictions will be imported.\n",
    "\n",
    "        Only predictions for estimators specified in `estimators` will be imported.\n",
    "        Any predictions for estimators that were already available will be overwritten\n",
    "        with predictions for the same estimators found in the files at `input_dir_path`.\n",
    "\n",
    "        :param input_dir_path:\n",
    "            The path to the directory for the training and test predictions files.\n",
    "            The file names are expected to be the same as `train_preds_filename`\n",
    "            and `test_preds_filename`\n",
    "        \"\"\"\n",
    "        print(\"[INFO] Importing predictions..\")\n",
    "        curr_train_preds, curr_test_preds = self.get_current_train_and_test_preds()\n",
    "        input_train_preds = pd.DataFrame()\n",
    "        input_test_preds = pd.DataFrame()\n",
    "\n",
    "        input_train_preds_path = Path(f\"{input_dir_path}/{self.train_preds_filename}.csv\")\n",
    "        input_test_preds_path = Path(f\"{input_dir_path}/{self.test_preds_filename}.csv\")\n",
    "        if input_train_preds_path.is_file():\n",
    "            try:\n",
    "                input_train_preds = pd.read_csv(input_train_preds_path)\n",
    "            except: pass\n",
    "        if input_test_preds_path.is_file():\n",
    "            try:\n",
    "                input_test_preds = pd.read_csv(input_test_preds_path)\n",
    "            except: pass\n",
    "\n",
    "        estimators_with_imported_train_preds = []\n",
    "        estimators_with_imported_test_preds = []\n",
    "        for estimator in self.estimators:\n",
    "            estimator_hash_value = estimator.get_hash_value()\n",
    "            estimator_name = f\"{estimator.name} ({estimator_hash_value})\"\n",
    "            train_preds_available = any(estimator_hash_value in col_name for col_name in input_train_preds.columns)\n",
    "            test_preds_available = any(estimator_hash_value in col_name for col_name in input_test_preds.columns)\n",
    "\n",
    "            if train_preds_available:\n",
    "                curr_train_preds[estimator_name] = input_train_preds[estimator_name]\n",
    "                estimators_with_imported_train_preds += [estimator_name]\n",
    "            if test_preds_available:\n",
    "                curr_test_preds[estimator_name] = input_test_preds[estimator_name]\n",
    "                estimators_with_imported_test_preds += [estimator_name]\n",
    "\n",
    "        if not estimators_with_imported_train_preds:\n",
    "            print(\"[INFO] No train predictions were imported\")\n",
    "        else:\n",
    "            curr_train_preds.sort_index(axis=1).to_csv(self.get_train_preds_file_path(), index=False)\n",
    "            formatted_estimators = \", \".join(estimators_with_imported_train_preds)\n",
    "            print(f\"[INFO] {len(estimators_with_imported_train_preds)} train predictions were imported:\\n{formatted_estimators}\")\n",
    "        if not estimators_with_imported_test_preds:\n",
    "            print(\"[INFO] No test predictions were imported\")\n",
    "        else:\n",
    "            curr_test_preds.sort_index(axis=1).to_csv(self.get_test_preds_file_path(), index=False)\n",
    "            formatted_estimators = \", \".join(estimators_with_imported_test_preds)\n",
    "            print(f\"[INFO] {len(estimators_with_imported_test_preds)} test predictions were imported:\\n{formatted_estimators}\")\n",
    "        \n",
    "        print(\"[INFO] Finished importing predictions\")\n",
    "\n",
    "    def clear_preds(self):\n",
    "        \"\"\"\n",
    "        Removes all stored predictions by deleting the two files at filepaths specified\n",
    "        by `working_dir_path`, `train_preds_filename` and `test_preds_filename`.\n",
    "        \"\"\"\n",
    "        train_preds_file_path = self.get_train_preds_file_path()\n",
    "        test_preds_file_path = self.get_test_preds_file_path()\n",
    "\n",
    "        if train_preds_file_path.is_file():\n",
    "            train_preds_file_path.unlink()\n",
    "        if test_preds_file_path.is_file():\n",
    "            test_preds_file_path.unlink()\n",
    "\n",
    "        print(\"[INFO] Finished clearing predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f048a9ce",
   "metadata": {
    "papermill": {
     "duration": 0.013611,
     "end_time": "2025-12-10T02:33:26.557990",
     "exception": false,
     "start_time": "2025-12-10T02:33:26.544379",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Next, we'll simply create a variable for storing the estimators (`StackingEstimator` instances) that we'll pass to the `StackingPredictionsRetriever` class for getting all the predictions from our base models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "90d2dfcd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T02:33:26.586767Z",
     "iopub.status.busy": "2025-12-10T02:33:26.586344Z",
     "iopub.status.idle": "2025-12-10T02:33:26.591036Z",
     "shell.execute_reply": "2025-12-10T02:33:26.589887Z"
    },
    "papermill": {
     "duration": 0.021185,
     "end_time": "2025-12-10T02:33:26.592725",
     "exception": false,
     "start_time": "2025-12-10T02:33:26.571540",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "estimators = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce50890",
   "metadata": {
    "papermill": {
     "duration": 0.01351,
     "end_time": "2025-12-10T02:33:26.619853",
     "exception": false,
     "start_time": "2025-12-10T02:33:26.606343",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 6. Base Model Definitions\n",
    "\n",
    "Custom implementations of some of the base models that require them can be found in this section.\n",
    "\n",
    "## 6.1 MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ff072343",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T02:33:26.648802Z",
     "iopub.status.busy": "2025-12-10T02:33:26.648374Z",
     "iopub.status.idle": "2025-12-10T02:33:26.664668Z",
     "shell.execute_reply": "2025-12-10T02:33:26.663306Z"
    },
    "papermill": {
     "duration": 0.033,
     "end_time": "2025-12-10T02:33:26.666338",
     "exception": false,
     "start_time": "2025-12-10T02:33:26.633338",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MLPClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, input_dim, hidden_layers, dropout, learning_rate, batch_size, weight_decay, epochs, device, patience=10):\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.dropout = dropout\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.weight_decay = weight_decay\n",
    "        self.epochs = epochs\n",
    "        self.device = device\n",
    "        self.patience = patience\n",
    "        self.model = None\n",
    "\n",
    "    def build_model(self):\n",
    "        layers = []\n",
    "        in_dim = self.input_dim\n",
    "        for h_dim in self.hidden_layers:\n",
    "            layers.append(nn.Linear(in_dim, h_dim))\n",
    "            layers.append(nn.BatchNorm1d(h_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(self.dropout))\n",
    "            in_dim = h_dim\n",
    "        layers.append(nn.Linear(in_dim, 1))\n",
    "        layers.append(nn.Sigmoid())\n",
    "        return nn.Sequential(*layers).to(self.device)\n",
    "\n",
    "    def fit(self, X_train, y_train, X_val=None, y_val=None):\n",
    "        X_train_t = torch.FloatTensor(X_train.values).to(self.device)\n",
    "        y_train_t = torch.FloatTensor(y_train.values).to(self.device).unsqueeze(1)\n",
    "        train_dataset = TensorDataset(X_train_t, y_train_t)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "        val_loader = None\n",
    "        if X_val is not None and y_val is not None:\n",
    "            X_val_t = torch.FloatTensor(X_val.values).to(self.device)\n",
    "            y_val_t = torch.FloatTensor(y_val.values).to(self.device).unsqueeze(1)\n",
    "            val_dataset = TensorDataset(X_val_t, y_val_t)\n",
    "            val_loader = DataLoader(val_dataset, batch_size=self.batch_size*2, shuffle=False)\n",
    "\n",
    "        self.model = self.build_model()\n",
    "        criterion = nn.BCELoss()\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate, weight_decay=self.weight_decay)\n",
    "\n",
    "        best_val_loss = float('inf')\n",
    "        patience_counter = 0\n",
    "        best_model_state = None\n",
    "\n",
    "        self.model.train()\n",
    "        \n",
    "        for epoch in range(self.epochs):\n",
    "            running_loss = 0.0\n",
    "            for X_batch, y_batch in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                y_pred = self.model(X_batch)\n",
    "                loss = criterion(y_pred, y_batch)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "\n",
    "            if val_loader:\n",
    "                self.model.eval() # switch to eval mode (disable dropout)\n",
    "                val_loss = 0.0\n",
    "                with torch.no_grad():\n",
    "                    for X_v, y_v in val_loader:\n",
    "                        val_pred = self.model(X_v)\n",
    "                        val_loss += criterion(val_pred, y_v).item()\n",
    "                \n",
    "                # check for improvement\n",
    "                avg_val_loss = val_loss / len(val_loader)\n",
    "                \n",
    "                if avg_val_loss < best_val_loss:\n",
    "                    best_val_loss = avg_val_loss\n",
    "                    patience_counter = 0\n",
    "                    # save the best model weights\n",
    "                    best_model_state = copy.deepcopy(self.model.state_dict())\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "                    \n",
    "                self.model.train() # switch back to train mode\n",
    "\n",
    "                if patience_counter >= self.patience:\n",
    "                    break\n",
    "\n",
    "        if best_model_state:\n",
    "            self.model.load_state_dict(best_model_state)\n",
    "            \n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        self.model.eval()\n",
    "        X_t = torch.FloatTensor(X.values).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            preds = self.model(X_t).cpu().numpy()\n",
    "        return np.column_stack((1 - preds, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23515246",
   "metadata": {
    "papermill": {
     "duration": 0.014279,
     "end_time": "2025-12-10T02:33:26.694266",
     "exception": false,
     "start_time": "2025-12-10T02:33:26.679987",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 7. Base Model Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4649f924",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T02:33:26.723035Z",
     "iopub.status.busy": "2025-12-10T02:33:26.722718Z",
     "iopub.status.idle": "2025-12-10T02:33:26.727826Z",
     "shell.execute_reply": "2025-12-10T02:33:26.726776Z"
    },
    "papermill": {
     "duration": 0.021789,
     "end_time": "2025-12-10T02:33:26.729598",
     "exception": false,
     "start_time": "2025-12-10T02:33:26.707809",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# to skip hyperparameter tuning when it's not needed; set to `False` to do the tuning\n",
    "SKIP_BASE_MODEL_HYPERPARAMETER_TUNING = True\n",
    "\n",
    "# value set for early stopping for models that support it; this value will be used for actual model training as well\n",
    "EARLY_STOPPING_ROUNDS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2099ad4e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T02:33:26.758110Z",
     "iopub.status.busy": "2025-12-10T02:33:26.757756Z",
     "iopub.status.idle": "2025-12-10T02:33:26.763182Z",
     "shell.execute_reply": "2025-12-10T02:33:26.762223Z"
    },
    "papermill": {
     "duration": 0.021754,
     "end_time": "2025-12-10T02:33:26.764981",
     "exception": false,
     "start_time": "2025-12-10T02:33:26.743227",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BaseModelOptunaStudyEstimator(Enum):\n",
    "    CATBOOSTCLASSIFIER = \"CatBoostClassifier\"\n",
    "    XGBCLASSIFIER = \"XGBClassifier\"\n",
    "    XGBRFCLASSIFIER = \"XGBRFClassifier\"\n",
    "    MLPCLASSIFIER = \"MLPClassifier\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3918df7",
   "metadata": {
    "papermill": {
     "duration": 0.013945,
     "end_time": "2025-12-10T02:33:26.792615",
     "exception": false,
     "start_time": "2025-12-10T02:33:26.778670",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Manually configure the values for the following variables for different studies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6a25c2f0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T02:33:26.821281Z",
     "iopub.status.busy": "2025-12-10T02:33:26.820917Z",
     "iopub.status.idle": "2025-12-10T02:33:26.826346Z",
     "shell.execute_reply": "2025-12-10T02:33:26.825378Z"
    },
    "papermill": {
     "duration": 0.021904,
     "end_time": "2025-12-10T02:33:26.828181",
     "exception": false,
     "start_time": "2025-12-10T02:33:26.806277",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# estimator to use for Optuna study\n",
    "BASE_MODEL_OPTUNA_STUDY_ESTIMATOR = BaseModelOptunaStudyEstimator.XGBRFCLASSIFIER\n",
    "\n",
    "# maximum number of trials Optuna will conduct for the optimization\n",
    "BASE_MODEL_OPTUNA_STUDY_NUM_TRIALS = 500\n",
    "\n",
    "# number of splits to use for Stratified K-Fold Cross-Validation for Optuna study\n",
    "BASE_MODEL_OPTUNA_STUDY_KFOLD_NUM_SPLITS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c4bafeb5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T02:33:26.857494Z",
     "iopub.status.busy": "2025-12-10T02:33:26.857077Z",
     "iopub.status.idle": "2025-12-10T02:33:26.878043Z",
     "shell.execute_reply": "2025-12-10T02:33:26.876963Z"
    },
    "papermill": {
     "duration": 0.037735,
     "end_time": "2025-12-10T02:33:26.879626",
     "exception": false,
     "start_time": "2025-12-10T02:33:26.841891",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_base_model_optuna_params(trial, study_estimator):\n",
    "    if study_estimator == BaseModelOptunaStudyEstimator.CATBOOSTCLASSIFIER:\n",
    "        return {\n",
    "            \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.005, 0.1, log=True),\n",
    "            \"depth\": trial.suggest_int(\"depth\", 3, 10),\n",
    "            \"l2_leaf_reg\": trial.suggest_float(\"l2_leaf_reg\", 1, 30),\n",
    "            'bagging_temperature': trial.suggest_float('bagging_temperature', 0, 30),\n",
    "            'random_strength': trial.suggest_float('random_strength', 0, 20),\n",
    "            'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 1, 20),\n",
    "        }\n",
    "    elif study_estimator == BaseModelOptunaStudyEstimator.XGBCLASSIFIER:\n",
    "        return {\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.1, log=True),\n",
    "            'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "            'subsample': trial.suggest_float('subsample', 0.5, 1),\n",
    "            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "            'alpha': trial.suggest_float('alpha', 0.001, 10.0, log=True),\n",
    "            'gamma': trial.suggest_float('gamma', 0.001, 10.0, log=True),\n",
    "            'lambda': trial.suggest_float('lambda', 0.001, 5.0, log=True),\n",
    "            'min_child_weight': trial.suggest_int('min_child_weight', 1, 20),\n",
    "        }\n",
    "    elif study_estimator == BaseModelOptunaStudyEstimator.XGBRFCLASSIFIER:\n",
    "        return {\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n",
    "            'max_depth': trial.suggest_int('max_depth', 10, 25),\n",
    "            'subsample': trial.suggest_float('subsample', 0.4, 0.9),\n",
    "            'colsample_bynode': trial.suggest_float('colsample_bynode', 0.4, 0.9),\n",
    "            'reg_alpha': trial.suggest_float('reg_alpha', 1e-3, 10.0, log=True),\n",
    "            'reg_lambda': trial.suggest_float('reg_lambda', 1e-3, 10.0, log=True),\n",
    "        }\n",
    "    elif study_estimator == BaseModelOptunaStudyEstimator.MLPCLASSIFIER:\n",
    "        return {\n",
    "            'hidden_layers': trial.suggest_categorical('hidden_layers', [\n",
    "                (128, 64), \n",
    "                (256, 128),\n",
    "                (512, 256, 128),\n",
    "                (128, 64, 32)\n",
    "            ]),\n",
    "            'dropout': trial.suggest_float('dropout', 0.1, 0.4),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 1e-4, 1e-2, log=True),\n",
    "            'batch_size': trial.suggest_categorical('batch_size', [512, 1024, 2048]),\n",
    "            'weight_decay': trial.suggest_float('weight_decay', 1e-6, 1e-4, log=True),\n",
    "        }\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported optuna study estimator\")\n",
    "\n",
    "def get_base_model_predictions(study_estimator, trial_params, X_train_fold, y_train_fold, X_validation_fold, y_validation_fold):\n",
    "    if study_estimator == BaseModelOptunaStudyEstimator.CATBOOSTCLASSIFIER:\n",
    "        model = CatBoostClassifier(\n",
    "            **trial_params,\n",
    "            iterations=30000,\n",
    "            use_best_model=True,\n",
    "            cat_features=cat_features,\n",
    "            loss_function='Logloss',\n",
    "            eval_metric='AUC',\n",
    "            task_type='GPU' if torch.cuda.is_available() else 'CPU',\n",
    "            devices='0',\n",
    "            metric_period=1000,\n",
    "            random_seed=RANDOM_SEEDS[0],\n",
    "            verbose=False,\n",
    "            allow_writing_files=False\n",
    "        )\n",
    "        model.fit(\n",
    "            X_train_fold, y_train_fold,\n",
    "            eval_set=(X_validation_fold, y_validation_fold),\n",
    "            early_stopping_rounds=EARLY_STOPPING_ROUNDS\n",
    "        )\n",
    "        return model.predict_proba(X_validation_fold)[:, 1]\n",
    "    elif study_estimator == BaseModelOptunaStudyEstimator.XGBCLASSIFIER:\n",
    "        model = XGBClassifier(\n",
    "            **trial_params,\n",
    "            n_estimators=30000,\n",
    "            tree_method='hist' if torch.cuda.is_available() else 'auto',\n",
    "            device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "            enable_categorical=True,\n",
    "            objective='binary:logistic',\n",
    "            eval_metric='auc',\n",
    "            n_jobs=-1,\n",
    "            random_state=RANDOM_SEEDS[0],\n",
    "            verbosity=0\n",
    "        )\n",
    "        model.fit(\n",
    "            X_train_fold, y_train_fold,\n",
    "            eval_set=[(X_validation_fold, y_validation_fold)],\n",
    "            early_stopping_rounds=EARLY_STOPPING_ROUNDS,\n",
    "            verbose=False\n",
    "        )\n",
    "        return model.predict_proba(X_validation_fold)[:, 1]\n",
    "    elif study_estimator == BaseModelOptunaStudyEstimator.XGBRFCLASSIFIER:\n",
    "        model = XGBRFClassifier(\n",
    "            **trial_params,\n",
    "            tree_method='hist' if torch.cuda.is_available() else 'auto',\n",
    "            device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "            enable_categorical=True,\n",
    "            learning_rate=1.0,\n",
    "            n_jobs=-1,\n",
    "            random_state=RANDOM_SEEDS[0],\n",
    "            verbose=0\n",
    "        )\n",
    "        model.fit(X_train_fold, y_train_fold)\n",
    "        return model.predict_proba(X_validation_fold)[:, 1]\n",
    "    elif study_estimator == BaseModelOptunaStudyEstimator.MLPCLASSIFIER:\n",
    "        model = MLPClassifier(\n",
    "            **trial_params,\n",
    "            input_dim=X_train_fold.shape[1],\n",
    "            epochs=100,\n",
    "            patience=10,\n",
    "            device=DEVICE\n",
    "        )\n",
    "        model.fit(\n",
    "            X_train_fold, y_train_fold,\n",
    "            X_val=X_validation_fold, y_val=y_validation_fold\n",
    "        )\n",
    "        return model.predict_proba(X_validation_fold)[:, 1]\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported optuna study estimator\")\n",
    "\n",
    "def base_model_optuna_study_objective(trial):\n",
    "    base_model_params = get_base_model_optuna_params(trial, BASE_MODEL_OPTUNA_STUDY_ESTIMATOR)\n",
    "\n",
    "    if BASE_MODEL_OPTUNA_STUDY_ESTIMATOR == BaseModelOptunaStudyEstimator.MLPCLASSIFIER:\n",
    "        optuna_train_data = train_data_mlp\n",
    "    else:\n",
    "        optuna_train_data = train_data\n",
    "\n",
    "    base_model_optuna_study_skf = StratifiedKFold(n_splits=BASE_MODEL_OPTUNA_STUDY_KFOLD_NUM_SPLITS, shuffle=True, random_state=RANDOM_SEEDS[0])\n",
    "    base_model_optuna_study_skf_splits = base_model_optuna_study_skf.split(optuna_train_data.drop(target_col, axis=1), optuna_train_data[target_col])\n",
    "    base_model_optuna_study_skf_enumeration = enumerate(base_model_optuna_study_skf_splits)\n",
    "\n",
    "    total_roc_auc = 0\n",
    "\n",
    "    for fold, (train_indices, validation_indices) in base_model_optuna_study_skf_enumeration:\n",
    "        X_train_fold = optuna_train_data.drop(target_col, axis=1).iloc[train_indices]\n",
    "        X_validation_fold = optuna_train_data.drop(target_col, axis=1).iloc[validation_indices]\n",
    "        y_train_fold = optuna_train_data[target_col].iloc[train_indices]\n",
    "        y_validation_fold = optuna_train_data[target_col].iloc[validation_indices]\n",
    "\n",
    "        y_validation_pred_proba = get_base_model_predictions(\n",
    "            BASE_MODEL_OPTUNA_STUDY_ESTIMATOR,\n",
    "            base_model_params,\n",
    "            X_train_fold, y_train_fold,\n",
    "            X_validation_fold, y_validation_fold\n",
    "        )\n",
    "        roc_auc_fold = roc_auc_score(y_validation_fold, y_validation_pred_proba)\n",
    "        total_roc_auc += roc_auc_fold\n",
    "\n",
    "        trial.report(roc_auc_fold, step=fold)\n",
    "        if trial.should_prune():\n",
    "            raise optuna.TrialPruned()\n",
    "\n",
    "    average_roc_auc = total_roc_auc / BASE_MODEL_OPTUNA_STUDY_KFOLD_NUM_SPLITS\n",
    "    return average_roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3daef7d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T02:33:26.908464Z",
     "iopub.status.busy": "2025-12-10T02:33:26.908106Z",
     "iopub.status.idle": "2025-12-10T02:33:26.915171Z",
     "shell.execute_reply": "2025-12-10T02:33:26.914115Z"
    },
    "papermill": {
     "duration": 0.023252,
     "end_time": "2025-12-10T02:33:26.916608",
     "exception": false,
     "start_time": "2025-12-10T02:33:26.893356",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped base model hyperparameter tuning\n"
     ]
    }
   ],
   "source": [
    "if SKIP_BASE_MODEL_HYPERPARAMETER_TUNING:\n",
    "    print(\"Skipped base model hyperparameter tuning\")\n",
    "else:\n",
    "    print(f\"Started base model hyperparameter tuning for {BASE_MODEL_OPTUNA_STUDY_ESTIMATOR.value}\")\n",
    "    sampler = optuna.samplers.TPESampler(n_ei_candidates=48, multivariate=True)\n",
    "    study = optuna.create_study(sampler=sampler, direction=\"maximize\")\n",
    "    study.optimize(base_model_optuna_study_objective, n_trials=BASE_MODEL_OPTUNA_STUDY_NUM_TRIALS)\n",
    "    \n",
    "    print(f\"# trials finished: {len(study.trials)}\")\n",
    "    trial = study.best_trial\n",
    "    print(f\"Best trial AUC: {trial.value}\")\n",
    "    print(f\"Best trial params:\")\n",
    "    for param_key, param_value in trial.params.items():\n",
    "        print(f\"- {param_key}: {param_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073c9a3d",
   "metadata": {
    "papermill": {
     "duration": 0.013611,
     "end_time": "2025-12-10T02:33:26.944027",
     "exception": false,
     "start_time": "2025-12-10T02:33:26.930416",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 8. Base Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "da20737d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T02:33:26.978816Z",
     "iopub.status.busy": "2025-12-10T02:33:26.978438Z",
     "iopub.status.idle": "2025-12-10T02:33:26.983074Z",
     "shell.execute_reply": "2025-12-10T02:33:26.981941Z"
    },
    "papermill": {
     "duration": 0.021517,
     "end_time": "2025-12-10T02:33:26.984985",
     "exception": false,
     "start_time": "2025-12-10T02:33:26.963468",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# number of splits to use for Stratified K-Fold Cross-Validation for base models\n",
    "BASE_MODEL_KFOLD_NUM_SPLITS = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67f8546",
   "metadata": {
    "papermill": {
     "duration": 0.013409,
     "end_time": "2025-12-10T02:33:27.012486",
     "exception": false,
     "start_time": "2025-12-10T02:33:26.999077",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 8.1 CatBoostClassifier\n",
    "\n",
    "### 8.1.1 Helper Methods (CatBoostClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b595113e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T02:33:27.043056Z",
     "iopub.status.busy": "2025-12-10T02:33:27.042640Z",
     "iopub.status.idle": "2025-12-10T02:33:27.053759Z",
     "shell.execute_reply": "2025-12-10T02:33:27.052791Z"
    },
    "papermill": {
     "duration": 0.027946,
     "end_time": "2025-12-10T02:33:27.055402",
     "exception": false,
     "start_time": "2025-12-10T02:33:27.027456",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_catboostclassifier_preds(params_dict, feature_names):\n",
    "    oof_preds_accumulator = np.zeros(len(train_data))\n",
    "    test_preds_accumulator = np.zeros(len(test_data))\n",
    "\n",
    "    for random_seed in RANDOM_SEEDS:\n",
    "        skf = StratifiedKFold(n_splits=BASE_MODEL_KFOLD_NUM_SPLITS, shuffle=True, random_state=random_seed)\n",
    "        skf_splits = skf.split(train_data.drop(target_col, axis=1), train_data[target_col])\n",
    "        skf_enumeration = enumerate(skf_splits)\n",
    "\n",
    "        seed_oof_preds = np.zeros(len(train_data))\n",
    "    \n",
    "        for fold, (train_indices, validation_indices) in skf_enumeration:\n",
    "            X_train_fold = train_data.drop(target_col, axis=1).iloc[train_indices]\n",
    "            X_validation_fold = train_data.drop(target_col, axis=1).iloc[validation_indices]\n",
    "            y_train_fold = train_data[target_col].iloc[train_indices]\n",
    "            y_validation_fold = train_data[target_col].iloc[validation_indices]\n",
    "        \n",
    "            model = CatBoostClassifier(\n",
    "                **params_dict,\n",
    "                use_best_model=True,\n",
    "                cat_features=cat_features,\n",
    "                loss_function='Logloss',\n",
    "                eval_metric='AUC',\n",
    "                task_type='GPU' if torch.cuda.is_available() else 'CPU',\n",
    "                devices='0',\n",
    "                metric_period=1000,\n",
    "                random_seed=random_seed,\n",
    "                verbose=False,\n",
    "                allow_writing_files=False\n",
    "            )\n",
    "            model.fit(\n",
    "                X_train_fold, y_train_fold,\n",
    "                eval_set=(X_validation_fold, y_validation_fold),\n",
    "                early_stopping_rounds=EARLY_STOPPING_ROUNDS\n",
    "            )\n",
    "\n",
    "            y_validation_pred_proba = model.predict_proba(X_validation_fold)[:, 1]\n",
    "            y_test_pred_proba = model.predict_proba(test_data)[:, 1]\n",
    "            seed_oof_preds[validation_indices] = np.array(y_validation_pred_proba)\n",
    "            test_preds_accumulator += np.array(y_test_pred_proba)\n",
    "\n",
    "        oof_preds_accumulator += seed_oof_preds\n",
    "\n",
    "    final_oof_preds = oof_preds_accumulator / len(RANDOM_SEEDS)\n",
    "    final_test_preds = test_preds_accumulator / (BASE_MODEL_KFOLD_NUM_SPLITS * len(RANDOM_SEEDS))\n",
    "    return pd.Series(final_oof_preds), pd.Series(final_test_preds)\n",
    "\n",
    "def get_catboostclassifier_stacking_estimator(index, params_dict):\n",
    "    return StackingEstimator(\n",
    "        name=f\"CatBoostClassifier_{index}\",\n",
    "        params_dict=params_dict,\n",
    "        feature_names=train_data.columns.tolist(),\n",
    "        get_preds=get_catboostclassifier_preds\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd62cf13",
   "metadata": {
    "papermill": {
     "duration": 0.013585,
     "end_time": "2025-12-10T02:33:27.082821",
     "exception": false,
     "start_time": "2025-12-10T02:33:27.069236",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 8.1.2 Add Estimators (CatBoostClassifier)\n",
    "\n",
    "Add CatBoostClassifier estimators to list that StackingPredictionsRetriever will process. Hyperparameters were found using Optuna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "78c13585",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T02:33:27.112499Z",
     "iopub.status.busy": "2025-12-10T02:33:27.112164Z",
     "iopub.status.idle": "2025-12-10T02:33:27.119388Z",
     "shell.execute_reply": "2025-12-10T02:33:27.118159Z"
    },
    "papermill": {
     "duration": 0.024221,
     "end_time": "2025-12-10T02:33:27.121418",
     "exception": false,
     "start_time": "2025-12-10T02:33:27.097197",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "estimators += [\n",
    "    get_catboostclassifier_stacking_estimator(\n",
    "        index=1,\n",
    "        params_dict={ # Optuna study AUC: 0.7261767336235222\n",
    "            'iterations': 30000,\n",
    "            'learning_rate': 0.03933473509871599,\n",
    "            'depth': 3,\n",
    "            'l2_leaf_reg': 14.932109771039046,\n",
    "            'bagging_temperature': 0.13345806085697987,\n",
    "            'random_strength': 7.486374538597635,\n",
    "            'min_data_in_leaf': 2,\n",
    "        }\n",
    "    ),\n",
    "    get_catboostclassifier_stacking_estimator(\n",
    "        index=2,\n",
    "        params_dict={ # Optuna study AUC: 0.725842155230371\n",
    "            'iterations': 30000,\n",
    "            'learning_rate': 0.041779205681346576,\n",
    "            'depth': 4,\n",
    "            'l2_leaf_reg': 3.628892496718331,\n",
    "            'bagging_temperature': 0.1922242909320177,\n",
    "            'random_strength': 8.464699585881778,\n",
    "            'min_data_in_leaf': 5,\n",
    "        }\n",
    "    ),\n",
    "     get_catboostclassifier_stacking_estimator(\n",
    "        index=3,\n",
    "        params_dict={ # Optuna study AUC: 0.7257614687804782\n",
    "            'iterations': 30000,\n",
    "            'learning_rate': 0.08955773312600926,\n",
    "            'depth': 4,\n",
    "            'l2_leaf_reg': 8.952470035979275,\n",
    "            'bagging_temperature': 0.21150772067613666,\n",
    "            'random_strength': 14.741499198080962,\n",
    "            'min_data_in_leaf': 1,\n",
    "        }\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46c53c4",
   "metadata": {
    "papermill": {
     "duration": 0.017788,
     "end_time": "2025-12-10T02:33:27.156355",
     "exception": false,
     "start_time": "2025-12-10T02:33:27.138567",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 8.2 XGBClassifier\n",
    "\n",
    "### 8.2.1 Helper Methods (XGBClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "64a66cfb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T02:33:27.194893Z",
     "iopub.status.busy": "2025-12-10T02:33:27.194481Z",
     "iopub.status.idle": "2025-12-10T02:33:27.207935Z",
     "shell.execute_reply": "2025-12-10T02:33:27.206791Z"
    },
    "papermill": {
     "duration": 0.03898,
     "end_time": "2025-12-10T02:33:27.209790",
     "exception": false,
     "start_time": "2025-12-10T02:33:27.170810",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_xgbclassifier_preds(params_dict, feature_names):\n",
    "    oof_preds_accumulator = np.zeros(len(train_data))\n",
    "    test_preds_accumulator = np.zeros(len(test_data))\n",
    "\n",
    "    for random_seed in RANDOM_SEEDS:\n",
    "        skf = StratifiedKFold(n_splits=BASE_MODEL_KFOLD_NUM_SPLITS, shuffle=True, random_state=random_seed)\n",
    "        skf_splits = skf.split(train_data.drop(target_col, axis=1), train_data[target_col])\n",
    "        skf_enumeration = enumerate(skf_splits)\n",
    "\n",
    "        seed_oof_preds = np.zeros(len(train_data))\n",
    "\n",
    "        for fold, (train_indices, validation_indices) in skf_enumeration:\n",
    "            X_train_fold = train_data.drop(target_col, axis=1).iloc[train_indices]\n",
    "            X_validation_fold = train_data.drop(target_col, axis=1).iloc[validation_indices]\n",
    "            y_train_fold = train_data[target_col].iloc[train_indices]\n",
    "            y_validation_fold = train_data[target_col].iloc[validation_indices]\n",
    "\n",
    "            model = XGBClassifier(\n",
    "                **params_dict,\n",
    "                tree_method='hist' if torch.cuda.is_available() else 'auto',\n",
    "                device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "                enable_categorical=True,\n",
    "                objective='binary:logistic',\n",
    "                eval_metric='auc',\n",
    "                early_stopping_rounds=EARLY_STOPPING_ROUNDS,\n",
    "                n_jobs=-1,\n",
    "                random_state=random_seed,\n",
    "                verbosity=0\n",
    "            )\n",
    "            model.fit(\n",
    "                X_train_fold, y_train_fold,\n",
    "                eval_set=[(X_validation_fold, y_validation_fold)],\n",
    "                verbose=False\n",
    "            )\n",
    "\n",
    "            y_validation_pred_proba = model.predict_proba(X_validation_fold)[:, 1]\n",
    "            y_test_pred_proba = model.predict_proba(test_data)[:, 1]\n",
    "            seed_oof_preds[validation_indices] = np.array(y_validation_pred_proba)\n",
    "            test_preds_accumulator += np.array(y_test_pred_proba)\n",
    "\n",
    "        oof_preds_accumulator += seed_oof_preds\n",
    "\n",
    "    final_oof_preds = oof_preds_accumulator / len(RANDOM_SEEDS)\n",
    "    final_test_preds = test_preds_accumulator / (BASE_MODEL_KFOLD_NUM_SPLITS * len(RANDOM_SEEDS))\n",
    "    return pd.Series(final_oof_preds), pd.Series(final_test_preds)\n",
    "\n",
    "def get_xgbclassifier_stacking_estimator(index, params_dict):\n",
    "    return StackingEstimator(\n",
    "        name=f\"XGBClassifier_{index}\",\n",
    "        params_dict=params_dict,\n",
    "        feature_names=train_data.columns.tolist(),\n",
    "        get_preds=get_xgbclassifier_preds\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28dc3a00",
   "metadata": {
    "papermill": {
     "duration": 0.018573,
     "end_time": "2025-12-10T02:33:27.245062",
     "exception": false,
     "start_time": "2025-12-10T02:33:27.226489",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 8.2.2 Add Estimators (XGBClassifier)\n",
    "\n",
    "Add XGBClassifier estimators to list that StackingPredictionsRetriever will process. Hyperparameters were found using Optuna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "17e72c7a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T02:33:27.274358Z",
     "iopub.status.busy": "2025-12-10T02:33:27.274030Z",
     "iopub.status.idle": "2025-12-10T02:33:27.281878Z",
     "shell.execute_reply": "2025-12-10T02:33:27.280983Z"
    },
    "papermill": {
     "duration": 0.024627,
     "end_time": "2025-12-10T02:33:27.283604",
     "exception": false,
     "start_time": "2025-12-10T02:33:27.258977",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "estimators += [\n",
    "    get_xgbclassifier_stacking_estimator(\n",
    "        index=1,\n",
    "        params_dict={ # Optuna study AUC: 0.7275219804910846\n",
    "            'n_estimators': 30000,\n",
    "            'learning_rate': 0.00985498815107458,\n",
    "            'max_depth': 3,\n",
    "            'subsample': 0.975836120137461,\n",
    "            'colsample_bytree': 0.5411854284303592,\n",
    "            'alpha': 9.940781978752474,\n",
    "            'gamma': 0.008422323405815038,\n",
    "            'lambda': 0.025214960531620187,\n",
    "            'min_child_weight': 12,\n",
    "        }\n",
    "    ),\n",
    "    get_xgbclassifier_stacking_estimator(\n",
    "        index=2,\n",
    "        params_dict={ # Optuna study AUC: 0.7273817150393508\n",
    "            'n_estimators': 30000,\n",
    "            'learning_rate': 0.047179227853488916,\n",
    "            'max_depth': 3,\n",
    "            'subsample': 0.9561594029099818,\n",
    "            'colsample_bytree': 0.5200809916944509,\n",
    "            'alpha': 9.323686821094613,\n",
    "            'gamma': 0.06513704074541844,\n",
    "            'lambda': 0.07573405175712218,\n",
    "            'min_child_weight': 14,\n",
    "        }\n",
    "    ),\n",
    "    get_xgbclassifier_stacking_estimator(\n",
    "        index=3,\n",
    "        params_dict={ # Optuna study AUC: 0.7274144144696422\n",
    "            'n_estimators': 30000,\n",
    "            'learning_rate': 0.06778303256075534,\n",
    "            'max_depth': 3,\n",
    "            'subsample': 0.9750702612583769,\n",
    "            'colsample_bytree': 0.5164463777572837,\n",
    "            'alpha': 6.677223824702266,\n",
    "            'gamma': 0.06627215758548254,\n",
    "            'lambda': 0.10239210156952944,\n",
    "            'min_child_weight': 17,\n",
    "        }\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e4b56e",
   "metadata": {
    "papermill": {
     "duration": 0.014547,
     "end_time": "2025-12-10T02:33:27.312047",
     "exception": false,
     "start_time": "2025-12-10T02:33:27.297500",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 8.3 XGBRFClassifier\n",
    "\n",
    "### 8.3.1 Helper Methods (XGBRFClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "206a6b2b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T02:33:27.341312Z",
     "iopub.status.busy": "2025-12-10T02:33:27.340989Z",
     "iopub.status.idle": "2025-12-10T02:33:27.350560Z",
     "shell.execute_reply": "2025-12-10T02:33:27.349445Z"
    },
    "papermill": {
     "duration": 0.026818,
     "end_time": "2025-12-10T02:33:27.352421",
     "exception": false,
     "start_time": "2025-12-10T02:33:27.325603",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_xgbrfclassifier_preds(params_dict, feature_names):\n",
    "    oof_preds_accumulator = np.zeros(len(train_data_mlp))\n",
    "    test_preds_accumulator = np.zeros(len(test_data_mlp))\n",
    "    \n",
    "    for random_seed in RANDOM_SEEDS:\n",
    "        skf = StratifiedKFold(n_splits=BASE_MODEL_KFOLD_NUM_SPLITS, shuffle=True, random_state=random_seed)\n",
    "        \n",
    "        seed_oof_preds = np.zeros(len(train_data_mlp))\n",
    "        \n",
    "        for fold, (train_indices, validation_indices) in enumerate(skf.split(train_data_mlp.drop(target_col, axis=1), train_data_mlp[target_col])):\n",
    "            X_train_fold = train_data_mlp.drop(target_col, axis=1).iloc[train_indices]\n",
    "            X_val_fold = train_data_mlp.drop(target_col, axis=1).iloc[validation_indices]\n",
    "            y_train_fold = train_data_mlp[target_col].iloc[train_indices]\n",
    "            \n",
    "            model = XGBRFClassifier(\n",
    "                **params_dict,\n",
    "                tree_method='hist' if torch.cuda.is_available() else 'auto',\n",
    "                device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "                learning_rate=1.0,\n",
    "                n_jobs=-1,\n",
    "                random_state=random_seed,\n",
    "                verbose=0\n",
    "            )\n",
    "            \n",
    "            model.fit(X_train_fold, y_train_fold)\n",
    "            seed_oof_preds[validation_indices] = model.predict_proba(X_val_fold)[:, 1]\n",
    "            test_preds_accumulator += model.predict_proba(test_data_mlp)[:, 1]\n",
    "            \n",
    "        oof_preds_accumulator += seed_oof_preds\n",
    "\n",
    "    final_oof_preds = oof_preds_accumulator / len(RANDOM_SEEDS)\n",
    "    final_test_preds = test_preds_accumulator / (BASE_MODEL_KFOLD_NUM_SPLITS * len(RANDOM_SEEDS))\n",
    "    \n",
    "    return pd.Series(final_oof_preds), pd.Series(final_test_preds)\n",
    "\n",
    "def get_xgbrfclassifier_stacking_estimator(index, params_dict):\n",
    "    return StackingEstimator(\n",
    "        name=f\"XGBRFClassifier_{index}\",\n",
    "        params_dict=params_dict,\n",
    "        feature_names=train_data_mlp.columns.tolist(),\n",
    "        get_preds=get_xgbrfclassifier_preds\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335f803a",
   "metadata": {
    "papermill": {
     "duration": 0.014005,
     "end_time": "2025-12-10T02:33:27.380024",
     "exception": false,
     "start_time": "2025-12-10T02:33:27.366019",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 8.3.2 Add Estimators (XGBRFClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "635b4993",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T02:33:27.409586Z",
     "iopub.status.busy": "2025-12-10T02:33:27.408749Z",
     "iopub.status.idle": "2025-12-10T02:33:27.414659Z",
     "shell.execute_reply": "2025-12-10T02:33:27.413651Z"
    },
    "papermill": {
     "duration": 0.022636,
     "end_time": "2025-12-10T02:33:27.416380",
     "exception": false,
     "start_time": "2025-12-10T02:33:27.393744",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "estimators += [\n",
    "     get_xgbrfclassifier_stacking_estimator(\n",
    "        index=1,\n",
    "        params_dict={ # Optuna study AUC: 0.7093869754986559\n",
    "            'n_estimators': 385,\n",
    "            'max_depth': 16,\n",
    "            'subsample': 0.5315714592374865,\n",
    "            'colsample_bynode': 0.8874353486231008,\n",
    "            'reg_alpha': 0.0054867256964063835,\n",
    "            'reg_lambda': 0.007252617838414462,\n",
    "        }\n",
    "    ),\n",
    "    #  get_xgbrfclassifier_stacking_estimator(\n",
    "    #     index=2,\n",
    "    #     params_dict={ # Optuna study AUC: 0.7092307295106085\n",
    "    #         'n_estimators': 415,\n",
    "    #         'max_depth': 16,\n",
    "    #         'subsample': 0.4857798345845685,\n",
    "    #         'colsample_bynode': 0.8965128787444345,\n",
    "    #         'reg_alpha': 0.18172738357674292,\n",
    "    #         'reg_lambda': 0.004947196570367552,\n",
    "    #     }\n",
    "    # ),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca202d7b",
   "metadata": {
    "papermill": {
     "duration": 0.013754,
     "end_time": "2025-12-10T02:33:27.443881",
     "exception": false,
     "start_time": "2025-12-10T02:33:27.430127",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 8.4 MLPClassifier\n",
    "\n",
    "### 8.4.1 Helper Methods (MLPClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "01a969ea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T02:33:27.498199Z",
     "iopub.status.busy": "2025-12-10T02:33:27.497375Z",
     "iopub.status.idle": "2025-12-10T02:33:27.507912Z",
     "shell.execute_reply": "2025-12-10T02:33:27.506646Z"
    },
    "papermill": {
     "duration": 0.028299,
     "end_time": "2025-12-10T02:33:27.509725",
     "exception": false,
     "start_time": "2025-12-10T02:33:27.481426",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_mlpclassifier_preds(params_dict, feature_names):\n",
    "    oof_preds_accumulator = np.zeros(len(train_data_mlp))\n",
    "    test_preds_accumulator = np.zeros(len(test_data_mlp))\n",
    "\n",
    "    for random_seed in RANDOM_SEEDS:\n",
    "        skf = StratifiedKFold(n_splits=BASE_MODEL_KFOLD_NUM_SPLITS, shuffle=True, random_state=random_seed)\n",
    "        skf_splits = skf.split(train_data_mlp.drop(target_col, axis=1), train_data_mlp[target_col])\n",
    "        skf_enumeration = enumerate(skf_splits)\n",
    "\n",
    "        seed_oof_preds = np.zeros(len(train_data_mlp))\n",
    "\n",
    "        for fold, (train_indices, validation_indices) in skf_enumeration:\n",
    "            X_train_fold = train_data_mlp.drop(target_col, axis=1).iloc[train_indices]\n",
    "            X_validation_fold = train_data_mlp.drop(target_col, axis=1).iloc[validation_indices]\n",
    "            y_train_fold = train_data_mlp[target_col].iloc[train_indices]\n",
    "            y_validation_fold = train_data_mlp[target_col].iloc[validation_indices]\n",
    "\n",
    "            model = MLPClassifier(\n",
    "                **params_dict,\n",
    "                input_dim=X_train_fold.shape[1],\n",
    "                epochs=100,\n",
    "                patience=10,\n",
    "                device=DEVICE\n",
    "            )\n",
    "            model.fit(X_train_fold, y_train_fold)\n",
    "\n",
    "            y_validation_pred_proba = model.predict_proba(X_validation_fold)[:, 1]\n",
    "            y_test_pred_proba = model.predict_proba(test_data_mlp)[:, 1]\n",
    "            seed_oof_preds[validation_indices] = np.array(y_validation_pred_proba)\n",
    "            test_preds_accumulator += np.array(y_test_pred_proba)\n",
    "\n",
    "        oof_preds_accumulator += seed_oof_preds\n",
    "\n",
    "    final_oof_preds = oof_preds_accumulator / len(RANDOM_SEEDS)\n",
    "    final_test_preds = test_preds_accumulator / (BASE_MODEL_KFOLD_NUM_SPLITS * len(RANDOM_SEEDS))\n",
    "    return pd.Series(final_oof_preds), pd.Series(final_test_preds)\n",
    "\n",
    "def get_mlpclassifier_stacking_estimator(index, params_dict):\n",
    "    return StackingEstimator(\n",
    "        name=f\"MLPClassifier_{index}\",\n",
    "        params_dict=params_dict,\n",
    "        feature_names=train_data_mlp.columns.tolist(),\n",
    "        get_preds=get_mlpclassifier_preds\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a36960",
   "metadata": {
    "papermill": {
     "duration": 0.013525,
     "end_time": "2025-12-10T02:33:27.536985",
     "exception": false,
     "start_time": "2025-12-10T02:33:27.523460",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 8.4.2 Add Estimators (MLPClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c13b976d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T02:33:27.566159Z",
     "iopub.status.busy": "2025-12-10T02:33:27.565832Z",
     "iopub.status.idle": "2025-12-10T02:33:27.570632Z",
     "shell.execute_reply": "2025-12-10T02:33:27.569615Z"
    },
    "papermill": {
     "duration": 0.021883,
     "end_time": "2025-12-10T02:33:27.572412",
     "exception": false,
     "start_time": "2025-12-10T02:33:27.550529",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# estimators += [\n",
    "#     get_mlpclassifier_stacking_estimator(\n",
    "#         index=1,\n",
    "#         params_dict={ # Optuna study AUC: 0.6962418548652664\n",
    "#             'hidden_layers': (128, 64, 32),\n",
    "#             'dropout': 0.35993977676783095,\n",
    "#             'learning_rate': 0.0022107162317045424,\n",
    "#             'batch_size': 1024,\n",
    "#             'weight_decay': 4.655295266533981e-06,\n",
    "#         }\n",
    "#     ),\n",
    "# ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b53c597",
   "metadata": {
    "papermill": {
     "duration": 0.013408,
     "end_time": "2025-12-10T02:33:27.599617",
     "exception": false,
     "start_time": "2025-12-10T02:33:27.586209",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 9. Base Model Predictions\n",
    "\n",
    "## 9.1 Get Base Model Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "94fc0ddf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T02:33:27.628223Z",
     "iopub.status.busy": "2025-12-10T02:33:27.627902Z",
     "iopub.status.idle": "2025-12-10T02:33:44.962751Z",
     "shell.execute_reply": "2025-12-10T02:33:44.961791Z"
    },
    "papermill": {
     "duration": 17.351501,
     "end_time": "2025-12-10T02:33:44.964742",
     "exception": false,
     "start_time": "2025-12-10T02:33:27.613241",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Importing predictions..\n",
      "[INFO] 7 train predictions were imported:\n",
      "CatBoostClassifier_1 (6eca2de83af2fea3676cc9382d0f1011), CatBoostClassifier_2 (b4347d306f6e52d59724382017b55f1b), CatBoostClassifier_3 (fad9fe0d13abe5377a9667ad2452550c), XGBClassifier_1 (3fd5db728b0f635ad3fb51e4ac3a5c8d), XGBClassifier_2 (cd0320203bc21b5b10db068b52204c49), XGBClassifier_3 (801128d9854f893393d31943ee000e82), XGBRFClassifier_1 (aca0224f49af4aff337e4a3bd4021211)\n",
      "[INFO] 7 test predictions were imported:\n",
      "CatBoostClassifier_1 (6eca2de83af2fea3676cc9382d0f1011), CatBoostClassifier_2 (b4347d306f6e52d59724382017b55f1b), CatBoostClassifier_3 (fad9fe0d13abe5377a9667ad2452550c), XGBClassifier_1 (3fd5db728b0f635ad3fb51e4ac3a5c8d), XGBClassifier_2 (cd0320203bc21b5b10db068b52204c49), XGBClassifier_3 (801128d9854f893393d31943ee000e82), XGBRFClassifier_1 (aca0224f49af4aff337e4a3bd4021211)\n",
      "[INFO] Finished importing predictions\n",
      "[INFO] Syncing predictions..\n",
      "[INFO] No columns for training predictions were dropped\n",
      "[INFO] No columns for test predictions were dropped\n",
      "[INFO] Finished syncing predictions\n",
      "[INFO] Getting predictions..\n",
      "[INFO] Skipped retrieving predictions for following estimators as their current ones are not stale:\n",
      "CatBoostClassifier_1 (6eca2de83af2fea3676cc9382d0f1011), CatBoostClassifier_2 (b4347d306f6e52d59724382017b55f1b), CatBoostClassifier_3 (fad9fe0d13abe5377a9667ad2452550c), XGBClassifier_1 (3fd5db728b0f635ad3fb51e4ac3a5c8d), XGBClassifier_2 (cd0320203bc21b5b10db068b52204c49), XGBClassifier_3 (801128d9854f893393d31943ee000e82), XGBRFClassifier_1 (aca0224f49af4aff337e4a3bd4021211)\n",
      "[INFO] Finished getting all predictions\n"
     ]
    }
   ],
   "source": [
    "stacking_preds_retriever = StackingPredictionsRetriever(\n",
    "    estimators=estimators,\n",
    "    working_dir_path=\"/kaggle/working/\",\n",
    "    train_preds_filename=\"base_models_train_preds\",\n",
    "    test_preds_filename=\"base_models_test_preds\",\n",
    "    preds_save_interval=1\n",
    ")\n",
    "stacking_preds_retriever.import_preds(\"/kaggle/input/diabetes-prediction-challenge-base-model-preds/\")\n",
    "stacking_preds_retriever.sync_preds()\n",
    "stacking_preds_retriever.get_preds()\n",
    "\n",
    "base_model_train_preds, base_model_test_preds = stacking_preds_retriever.get_current_train_and_test_preds()\n",
    "base_model_train_preds.sort_index(axis=1, inplace=True, key=lambda index: index.map(lambda col_name: (col_name.split(\"_\")[0], int(col_name.split()[0].split(\"_\")[-1]))))\n",
    "base_model_test_preds.sort_index(axis=1, inplace=True, key=lambda index: index.map(lambda col_name: (col_name.split(\"_\")[0], int(col_name.split()[0].split(\"_\")[-1]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8096a5",
   "metadata": {
    "papermill": {
     "duration": 0.014771,
     "end_time": "2025-12-10T02:33:44.994278",
     "exception": false,
     "start_time": "2025-12-10T02:33:44.979507",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 9.2 Base Models AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fa965cc4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T02:33:45.024548Z",
     "iopub.status.busy": "2025-12-10T02:33:45.023437Z",
     "iopub.status.idle": "2025-12-10T02:33:47.623221Z",
     "shell.execute_reply": "2025-12-10T02:33:47.622250Z"
    },
    "papermill": {
     "duration": 2.616478,
     "end_time": "2025-12-10T02:33:47.624806",
     "exception": false,
     "start_time": "2025-12-10T02:33:45.008328",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier_2 (cd0320203bc21b5b10db068b52204c49)         0.727895\n",
       "XGBClassifier_3 (801128d9854f893393d31943ee000e82)         0.727864\n",
       "XGBClassifier_1 (3fd5db728b0f635ad3fb51e4ac3a5c8d)         0.727811\n",
       "CatBoostClassifier_1 (6eca2de83af2fea3676cc9382d0f1011)    0.726697\n",
       "CatBoostClassifier_3 (fad9fe0d13abe5377a9667ad2452550c)    0.726597\n",
       "CatBoostClassifier_2 (b4347d306f6e52d59724382017b55f1b)    0.726549\n",
       "XGBRFClassifier_1 (aca0224f49af4aff337e4a3bd4021211)       0.710963\n",
       "dtype: float64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model_auc = pd.Series()\n",
    "for estimator in base_model_train_preds.columns:\n",
    "    base_model_auc[estimator] = roc_auc_score(train_data[target_col], base_model_train_preds[estimator])\n",
    "base_model_auc.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6f876b",
   "metadata": {
    "papermill": {
     "duration": 0.014061,
     "end_time": "2025-12-10T02:33:47.653491",
     "exception": false,
     "start_time": "2025-12-10T02:33:47.639430",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 10. Blending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c52c8e93",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T02:33:47.683514Z",
     "iopub.status.busy": "2025-12-10T02:33:47.683208Z",
     "iopub.status.idle": "2025-12-10T02:33:47.687812Z",
     "shell.execute_reply": "2025-12-10T02:33:47.686839Z"
    },
    "papermill": {
     "duration": 0.021825,
     "end_time": "2025-12-10T02:33:47.689492",
     "exception": false,
     "start_time": "2025-12-10T02:33:47.667667",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# num iterations to hill climb\n",
    "BLENDING_NUM_ITERATIONS = 1000\n",
    "\n",
    "# number of splits of Stratified K-Fold to use for hill climbing\n",
    "BLENDING_KFOLD_NUM_SPLITS = 5\n",
    "\n",
    "# use different random seeds from ones used to train base models to avoid\n",
    "# potential leakage or alignment artifacts from original splits\n",
    "BLENDING_RANDOM_SEEDS = [71, 99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "93478f64",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T02:33:47.720007Z",
     "iopub.status.busy": "2025-12-10T02:33:47.719645Z",
     "iopub.status.idle": "2025-12-10T08:08:25.435680Z",
     "shell.execute_reply": "2025-12-10T08:08:25.434386Z"
    },
    "papermill": {
     "duration": 20077.753396,
     "end_time": "2025-12-10T08:08:25.457519",
     "exception": false,
     "start_time": "2025-12-10T02:33:47.704123",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Random Seed 71 #####\n",
      "\n",
      "### Fold 1 ###\n",
      "\n",
      "Starting Hill Climbing on 7 models for 1000 iterations...\n",
      "Iter 100/1000: Best AUC = 0.72800 | Added: XGBClassifier_3 (801128d9854f893393d31943ee000e82)\n",
      "Iter 200/1000: Best AUC = 0.72800 | Added: XGBClassifier_2 (cd0320203bc21b5b10db068b52204c49)\n",
      "Iter 300/1000: Best AUC = 0.72800 | Added: XGBClassifier_3 (801128d9854f893393d31943ee000e82)\n",
      "Iter 400/1000: Best AUC = 0.72800 | Added: XGBClassifier_3 (801128d9854f893393d31943ee000e82)\n",
      "Iter 500/1000: Best AUC = 0.72800 | Added: XGBClassifier_3 (801128d9854f893393d31943ee000e82)\n",
      "Iter 600/1000: Best AUC = 0.72800 | Added: XGBClassifier_3 (801128d9854f893393d31943ee000e82)\n",
      "Iter 700/1000: Best AUC = 0.72800 | Added: XGBClassifier_2 (cd0320203bc21b5b10db068b52204c49)\n",
      "Iter 800/1000: Best AUC = 0.72800 | Added: XGBClassifier_2 (cd0320203bc21b5b10db068b52204c49)\n",
      "Iter 900/1000: Best AUC = 0.72800 | Added: CatBoostClassifier_1 (6eca2de83af2fea3676cc9382d0f1011)\n",
      "Iter 1000/1000: Best AUC = 0.72800 | Added: XGBClassifier_3 (801128d9854f893393d31943ee000e82)\n",
      "[Optimized weights for fold]\n",
      "CatBoostClassifier_1 (6eca2de83af2fea3676cc9382d0f1011): 0.0310\n",
      "CatBoostClassifier_2 (b4347d306f6e52d59724382017b55f1b): 0.0000\n",
      "CatBoostClassifier_3 (fad9fe0d13abe5377a9667ad2452550c): 0.0920\n",
      "XGBClassifier_1 (3fd5db728b0f635ad3fb51e4ac3a5c8d): 0.0000\n",
      "XGBClassifier_2 (cd0320203bc21b5b10db068b52204c49): 0.4770\n",
      "XGBClassifier_3 (801128d9854f893393d31943ee000e82): 0.4000\n",
      "XGBRFClassifier_1 (aca0224f49af4aff337e4a3bd4021211): 0.0000\n",
      "\n",
      "### Fold 2 ###\n",
      "\n",
      "Starting Hill Climbing on 7 models for 1000 iterations...\n",
      "Iter 100/1000: Best AUC = 0.72785 | Added: XGBClassifier_2 (cd0320203bc21b5b10db068b52204c49)\n",
      "Iter 200/1000: Best AUC = 0.72785 | Added: CatBoostClassifier_1 (6eca2de83af2fea3676cc9382d0f1011)\n",
      "Iter 300/1000: Best AUC = 0.72785 | Added: XGBClassifier_3 (801128d9854f893393d31943ee000e82)\n",
      "Iter 400/1000: Best AUC = 0.72785 | Added: XGBClassifier_3 (801128d9854f893393d31943ee000e82)\n",
      "Iter 500/1000: Best AUC = 0.72785 | Added: XGBClassifier_2 (cd0320203bc21b5b10db068b52204c49)\n",
      "Iter 600/1000: Best AUC = 0.72785 | Added: XGBClassifier_2 (cd0320203bc21b5b10db068b52204c49)\n",
      "Iter 700/1000: Best AUC = 0.72785 | Added: XGBClassifier_2 (cd0320203bc21b5b10db068b52204c49)\n",
      "Iter 800/1000: Best AUC = 0.72785 | Added: XGBClassifier_2 (cd0320203bc21b5b10db068b52204c49)\n",
      "Iter 900/1000: Best AUC = 0.72785 | Added: XGBClassifier_3 (801128d9854f893393d31943ee000e82)\n",
      "Iter 1000/1000: Best AUC = 0.72785 | Added: XGBClassifier_2 (cd0320203bc21b5b10db068b52204c49)\n",
      "[Optimized weights for fold]\n",
      "CatBoostClassifier_1 (6eca2de83af2fea3676cc9382d0f1011): 0.0930\n",
      "CatBoostClassifier_2 (b4347d306f6e52d59724382017b55f1b): 0.0000\n",
      "CatBoostClassifier_3 (fad9fe0d13abe5377a9667ad2452550c): 0.0210\n",
      "XGBClassifier_1 (3fd5db728b0f635ad3fb51e4ac3a5c8d): 0.0040\n",
      "XGBClassifier_2 (cd0320203bc21b5b10db068b52204c49): 0.5290\n",
      "XGBClassifier_3 (801128d9854f893393d31943ee000e82): 0.3530\n",
      "XGBRFClassifier_1 (aca0224f49af4aff337e4a3bd4021211): 0.0000\n",
      "\n",
      "### Fold 3 ###\n",
      "\n",
      "Starting Hill Climbing on 7 models for 1000 iterations...\n",
      "Iter 100/1000: Best AUC = 0.72869 | Added: XGBClassifier_3 (801128d9854f893393d31943ee000e82)\n",
      "Iter 200/1000: Best AUC = 0.72869 | Added: XGBClassifier_2 (cd0320203bc21b5b10db068b52204c49)\n",
      "Iter 300/1000: Best AUC = 0.72869 | Added: CatBoostClassifier_1 (6eca2de83af2fea3676cc9382d0f1011)\n",
      "Iter 400/1000: Best AUC = 0.72869 | Added: XGBClassifier_3 (801128d9854f893393d31943ee000e82)\n",
      "Iter 500/1000: Best AUC = 0.72869 | Added: XGBClassifier_2 (cd0320203bc21b5b10db068b52204c49)\n",
      "Iter 600/1000: Best AUC = 0.72869 | Added: XGBClassifier_2 (cd0320203bc21b5b10db068b52204c49)\n",
      "Iter 700/1000: Best AUC = 0.72869 | Added: XGBClassifier_2 (cd0320203bc21b5b10db068b52204c49)\n",
      "Iter 800/1000: Best AUC = 0.72869 | Added: XGBClassifier_3 (801128d9854f893393d31943ee000e82)\n",
      "Iter 900/1000: Best AUC = 0.72869 | Added: XGBClassifier_2 (cd0320203bc21b5b10db068b52204c49)\n",
      "Iter 1000/1000: Best AUC = 0.72869 | Added: XGBClassifier_2 (cd0320203bc21b5b10db068b52204c49)\n",
      "[Optimized weights for fold]\n",
      "CatBoostClassifier_1 (6eca2de83af2fea3676cc9382d0f1011): 0.1010\n",
      "CatBoostClassifier_2 (b4347d306f6e52d59724382017b55f1b): 0.0000\n",
      "CatBoostClassifier_3 (fad9fe0d13abe5377a9667ad2452550c): 0.0120\n",
      "XGBClassifier_1 (3fd5db728b0f635ad3fb51e4ac3a5c8d): 0.0000\n",
      "XGBClassifier_2 (cd0320203bc21b5b10db068b52204c49): 0.4880\n",
      "XGBClassifier_3 (801128d9854f893393d31943ee000e82): 0.3990\n",
      "XGBRFClassifier_1 (aca0224f49af4aff337e4a3bd4021211): 0.0000\n",
      "\n",
      "### Fold 4 ###\n",
      "\n",
      "Starting Hill Climbing on 7 models for 1000 iterations...\n",
      "Iter 100/1000: Best AUC = 0.72767 | Added: CatBoostClassifier_3 (fad9fe0d13abe5377a9667ad2452550c)\n",
      "Iter 200/1000: Best AUC = 0.72767 | Added: XGBClassifier_2 (cd0320203bc21b5b10db068b52204c49)\n",
      "Iter 300/1000: Best AUC = 0.72767 | Added: XGBClassifier_3 (801128d9854f893393d31943ee000e82)\n",
      "Iter 400/1000: Best AUC = 0.72767 | Added: XGBClassifier_2 (cd0320203bc21b5b10db068b52204c49)\n",
      "Iter 500/1000: Best AUC = 0.72767 | Added: XGBClassifier_2 (cd0320203bc21b5b10db068b52204c49)\n",
      "Iter 600/1000: Best AUC = 0.72767 | Added: XGBClassifier_3 (801128d9854f893393d31943ee000e82)\n",
      "Iter 700/1000: Best AUC = 0.72767 | Added: XGBClassifier_3 (801128d9854f893393d31943ee000e82)\n",
      "Iter 800/1000: Best AUC = 0.72767 | Added: XGBClassifier_3 (801128d9854f893393d31943ee000e82)\n",
      "Iter 900/1000: Best AUC = 0.72767 | Added: XGBClassifier_3 (801128d9854f893393d31943ee000e82)\n",
      "Iter 1000/1000: Best AUC = 0.72767 | Added: CatBoostClassifier_1 (6eca2de83af2fea3676cc9382d0f1011)\n",
      "[Optimized weights for fold]\n",
      "CatBoostClassifier_1 (6eca2de83af2fea3676cc9382d0f1011): 0.0370\n",
      "CatBoostClassifier_2 (b4347d306f6e52d59724382017b55f1b): 0.0000\n",
      "CatBoostClassifier_3 (fad9fe0d13abe5377a9667ad2452550c): 0.0610\n",
      "XGBClassifier_1 (3fd5db728b0f635ad3fb51e4ac3a5c8d): 0.0000\n",
      "XGBClassifier_2 (cd0320203bc21b5b10db068b52204c49): 0.5020\n",
      "XGBClassifier_3 (801128d9854f893393d31943ee000e82): 0.4000\n",
      "XGBRFClassifier_1 (aca0224f49af4aff337e4a3bd4021211): 0.0000\n",
      "\n",
      "### Fold 5 ###\n",
      "\n",
      "Starting Hill Climbing on 7 models for 1000 iterations...\n",
      "Iter 100/1000: Best AUC = 0.72756 | Added: XGBClassifier_2 (cd0320203bc21b5b10db068b52204c49)\n",
      "Iter 200/1000: Best AUC = 0.72756 | Added: XGBClassifier_2 (cd0320203bc21b5b10db068b52204c49)\n",
      "Iter 300/1000: Best AUC = 0.72756 | Added: XGBClassifier_3 (801128d9854f893393d31943ee000e82)\n",
      "Iter 400/1000: Best AUC = 0.72756 | Added: XGBClassifier_2 (cd0320203bc21b5b10db068b52204c49)\n",
      "Iter 500/1000: Best AUC = 0.72756 | Added: XGBClassifier_2 (cd0320203bc21b5b10db068b52204c49)\n",
      "Iter 600/1000: Best AUC = 0.72756 | Added: XGBClassifier_3 (801128d9854f893393d31943ee000e82)\n",
      "Iter 700/1000: Best AUC = 0.72756 | Added: XGBClassifier_3 (801128d9854f893393d31943ee000e82)\n",
      "Iter 800/1000: Best AUC = 0.72756 | Added: CatBoostClassifier_1 (6eca2de83af2fea3676cc9382d0f1011)\n",
      "Iter 900/1000: Best AUC = 0.72756 | Added: CatBoostClassifier_1 (6eca2de83af2fea3676cc9382d0f1011)\n",
      "Iter 1000/1000: Best AUC = 0.72756 | Added: XGBClassifier_2 (cd0320203bc21b5b10db068b52204c49)\n",
      "[Optimized weights for fold]\n",
      "CatBoostClassifier_1 (6eca2de83af2fea3676cc9382d0f1011): 0.0460\n",
      "CatBoostClassifier_2 (b4347d306f6e52d59724382017b55f1b): 0.0000\n",
      "CatBoostClassifier_3 (fad9fe0d13abe5377a9667ad2452550c): 0.0460\n",
      "XGBClassifier_1 (3fd5db728b0f635ad3fb51e4ac3a5c8d): 0.0000\n",
      "XGBClassifier_2 (cd0320203bc21b5b10db068b52204c49): 0.5490\n",
      "XGBClassifier_3 (801128d9854f893393d31943ee000e82): 0.3590\n",
      "XGBRFClassifier_1 (aca0224f49af4aff337e4a3bd4021211): 0.0000\n",
      "\n",
      "##### Random Seed 99 #####\n",
      "\n",
      "### Fold 1 ###\n",
      "\n",
      "Starting Hill Climbing on 7 models for 1000 iterations...\n",
      "Iter 100/1000: Best AUC = 0.72804 | Added: XGBClassifier_3 (801128d9854f893393d31943ee000e82)\n",
      "Iter 200/1000: Best AUC = 0.72804 | Added: XGBClassifier_3 (801128d9854f893393d31943ee000e82)\n",
      "Iter 300/1000: Best AUC = 0.72804 | Added: XGBClassifier_2 (cd0320203bc21b5b10db068b52204c49)\n",
      "Iter 400/1000: Best AUC = 0.72804 | Added: XGBClassifier_3 (801128d9854f893393d31943ee000e82)\n",
      "Iter 500/1000: Best AUC = 0.72804 | Added: XGBClassifier_3 (801128d9854f893393d31943ee000e82)\n",
      "Iter 600/1000: Best AUC = 0.72804 | Added: XGBClassifier_1 (3fd5db728b0f635ad3fb51e4ac3a5c8d)\n",
      "Iter 700/1000: Best AUC = 0.72804 | Added: XGBClassifier_3 (801128d9854f893393d31943ee000e82)\n",
      "Iter 800/1000: Best AUC = 0.72804 | Added: CatBoostClassifier_1 (6eca2de83af2fea3676cc9382d0f1011)\n",
      "Iter 900/1000: Best AUC = 0.72804 | Added: CatBoostClassifier_1 (6eca2de83af2fea3676cc9382d0f1011)\n",
      "Iter 1000/1000: Best AUC = 0.72804 | Added: XGBClassifier_3 (801128d9854f893393d31943ee000e82)\n",
      "[Optimized weights for fold]\n",
      "CatBoostClassifier_1 (6eca2de83af2fea3676cc9382d0f1011): 0.0390\n",
      "CatBoostClassifier_2 (b4347d306f6e52d59724382017b55f1b): 0.0000\n",
      "CatBoostClassifier_3 (fad9fe0d13abe5377a9667ad2452550c): 0.0990\n",
      "XGBClassifier_1 (3fd5db728b0f635ad3fb51e4ac3a5c8d): 0.0140\n",
      "XGBClassifier_2 (cd0320203bc21b5b10db068b52204c49): 0.5150\n",
      "XGBClassifier_3 (801128d9854f893393d31943ee000e82): 0.3330\n",
      "XGBRFClassifier_1 (aca0224f49af4aff337e4a3bd4021211): 0.0000\n",
      "\n",
      "### Fold 2 ###\n",
      "\n",
      "Starting Hill Climbing on 7 models for 1000 iterations...\n",
      "Iter 100/1000: Best AUC = 0.72777 | Added: XGBClassifier_3 (801128d9854f893393d31943ee000e82)\n",
      "Iter 200/1000: Best AUC = 0.72777 | Added: XGBClassifier_2 (cd0320203bc21b5b10db068b52204c49)\n",
      "Iter 300/1000: Best AUC = 0.72777 | Added: XGBClassifier_2 (cd0320203bc21b5b10db068b52204c49)\n",
      "Iter 400/1000: Best AUC = 0.72777 | Added: XGBClassifier_3 (801128d9854f893393d31943ee000e82)\n",
      "Iter 500/1000: Best AUC = 0.72777 | Added: CatBoostClassifier_3 (fad9fe0d13abe5377a9667ad2452550c)\n",
      "Iter 600/1000: Best AUC = 0.72777 | Added: XGBClassifier_2 (cd0320203bc21b5b10db068b52204c49)\n",
      "Iter 700/1000: Best AUC = 0.72777 | Added: CatBoostClassifier_1 (6eca2de83af2fea3676cc9382d0f1011)\n",
      "Iter 800/1000: Best AUC = 0.72777 | Added: XGBClassifier_2 (cd0320203bc21b5b10db068b52204c49)\n",
      "Iter 900/1000: Best AUC = 0.72777 | Added: CatBoostClassifier_1 (6eca2de83af2fea3676cc9382d0f1011)\n",
      "Iter 1000/1000: Best AUC = 0.72777 | Added: XGBClassifier_3 (801128d9854f893393d31943ee000e82)\n",
      "[Optimized weights for fold]\n",
      "CatBoostClassifier_1 (6eca2de83af2fea3676cc9382d0f1011): 0.0700\n",
      "CatBoostClassifier_2 (b4347d306f6e52d59724382017b55f1b): 0.0000\n",
      "CatBoostClassifier_3 (fad9fe0d13abe5377a9667ad2452550c): 0.0250\n",
      "XGBClassifier_1 (3fd5db728b0f635ad3fb51e4ac3a5c8d): 0.0000\n",
      "XGBClassifier_2 (cd0320203bc21b5b10db068b52204c49): 0.4780\n",
      "XGBClassifier_3 (801128d9854f893393d31943ee000e82): 0.4270\n",
      "XGBRFClassifier_1 (aca0224f49af4aff337e4a3bd4021211): 0.0000\n",
      "\n",
      "### Fold 3 ###\n",
      "\n",
      "Starting Hill Climbing on 7 models for 1000 iterations...\n",
      "Iter 100/1000: Best AUC = 0.72768 | Added: XGBClassifier_3 (801128d9854f893393d31943ee000e82)\n",
      "Iter 200/1000: Best AUC = 0.72768 | Added: XGBClassifier_3 (801128d9854f893393d31943ee000e82)\n",
      "Iter 300/1000: Best AUC = 0.72768 | Added: XGBClassifier_2 (cd0320203bc21b5b10db068b52204c49)\n",
      "Iter 400/1000: Best AUC = 0.72768 | Added: XGBClassifier_2 (cd0320203bc21b5b10db068b52204c49)\n",
      "Iter 500/1000: Best AUC = 0.72768 | Added: CatBoostClassifier_1 (6eca2de83af2fea3676cc9382d0f1011)\n",
      "Iter 600/1000: Best AUC = 0.72768 | Added: XGBClassifier_3 (801128d9854f893393d31943ee000e82)\n",
      "Iter 700/1000: Best AUC = 0.72768 | Added: XGBClassifier_2 (cd0320203bc21b5b10db068b52204c49)\n",
      "Iter 800/1000: Best AUC = 0.72768 | Added: XGBClassifier_2 (cd0320203bc21b5b10db068b52204c49)\n",
      "Iter 900/1000: Best AUC = 0.72768 | Added: XGBClassifier_2 (cd0320203bc21b5b10db068b52204c49)\n",
      "Iter 1000/1000: Best AUC = 0.72768 | Added: XGBClassifier_2 (cd0320203bc21b5b10db068b52204c49)\n",
      "[Optimized weights for fold]\n",
      "CatBoostClassifier_1 (6eca2de83af2fea3676cc9382d0f1011): 0.0210\n",
      "CatBoostClassifier_2 (b4347d306f6e52d59724382017b55f1b): 0.0000\n",
      "CatBoostClassifier_3 (fad9fe0d13abe5377a9667ad2452550c): 0.0780\n",
      "XGBClassifier_1 (3fd5db728b0f635ad3fb51e4ac3a5c8d): 0.0000\n",
      "XGBClassifier_2 (cd0320203bc21b5b10db068b52204c49): 0.5220\n",
      "XGBClassifier_3 (801128d9854f893393d31943ee000e82): 0.3790\n",
      "XGBRFClassifier_1 (aca0224f49af4aff337e4a3bd4021211): 0.0000\n",
      "\n",
      "### Fold 4 ###\n",
      "\n",
      "Starting Hill Climbing on 7 models for 1000 iterations...\n",
      "Iter 100/1000: Best AUC = 0.72793 | Added: XGBClassifier_3 (801128d9854f893393d31943ee000e82)\n",
      "Iter 200/1000: Best AUC = 0.72793 | Added: XGBClassifier_2 (cd0320203bc21b5b10db068b52204c49)\n",
      "Iter 300/1000: Best AUC = 0.72793 | Added: XGBClassifier_3 (801128d9854f893393d31943ee000e82)\n",
      "Iter 400/1000: Best AUC = 0.72793 | Added: XGBClassifier_2 (cd0320203bc21b5b10db068b52204c49)\n",
      "Iter 500/1000: Best AUC = 0.72793 | Added: CatBoostClassifier_1 (6eca2de83af2fea3676cc9382d0f1011)\n",
      "Iter 600/1000: Best AUC = 0.72793 | Added: XGBClassifier_3 (801128d9854f893393d31943ee000e82)\n",
      "Iter 700/1000: Best AUC = 0.72793 | Added: XGBClassifier_2 (cd0320203bc21b5b10db068b52204c49)\n",
      "Iter 800/1000: Best AUC = 0.72793 | Added: CatBoostClassifier_3 (fad9fe0d13abe5377a9667ad2452550c)\n",
      "Iter 900/1000: Best AUC = 0.72793 | Added: XGBClassifier_3 (801128d9854f893393d31943ee000e82)\n",
      "Iter 1000/1000: Best AUC = 0.72793 | Added: XGBClassifier_2 (cd0320203bc21b5b10db068b52204c49)\n",
      "[Optimized weights for fold]\n",
      "CatBoostClassifier_1 (6eca2de83af2fea3676cc9382d0f1011): 0.0670\n",
      "CatBoostClassifier_2 (b4347d306f6e52d59724382017b55f1b): 0.0000\n",
      "CatBoostClassifier_3 (fad9fe0d13abe5377a9667ad2452550c): 0.0280\n",
      "XGBClassifier_1 (3fd5db728b0f635ad3fb51e4ac3a5c8d): 0.0000\n",
      "XGBClassifier_2 (cd0320203bc21b5b10db068b52204c49): 0.4960\n",
      "XGBClassifier_3 (801128d9854f893393d31943ee000e82): 0.4090\n",
      "XGBRFClassifier_1 (aca0224f49af4aff337e4a3bd4021211): 0.0000\n",
      "\n",
      "### Fold 5 ###\n",
      "\n",
      "Starting Hill Climbing on 7 models for 1000 iterations...\n",
      "Iter 100/1000: Best AUC = 0.72836 | Added: XGBClassifier_3 (801128d9854f893393d31943ee000e82)\n",
      "Iter 200/1000: Best AUC = 0.72836 | Added: XGBClassifier_3 (801128d9854f893393d31943ee000e82)\n",
      "Iter 300/1000: Best AUC = 0.72836 | Added: XGBClassifier_3 (801128d9854f893393d31943ee000e82)\n",
      "Iter 400/1000: Best AUC = 0.72836 | Added: CatBoostClassifier_1 (6eca2de83af2fea3676cc9382d0f1011)\n",
      "Iter 500/1000: Best AUC = 0.72836 | Added: XGBClassifier_3 (801128d9854f893393d31943ee000e82)\n",
      "Iter 600/1000: Best AUC = 0.72836 | Added: XGBClassifier_3 (801128d9854f893393d31943ee000e82)\n",
      "Iter 700/1000: Best AUC = 0.72836 | Added: XGBClassifier_2 (cd0320203bc21b5b10db068b52204c49)\n",
      "Iter 800/1000: Best AUC = 0.72836 | Added: CatBoostClassifier_1 (6eca2de83af2fea3676cc9382d0f1011)\n",
      "Iter 900/1000: Best AUC = 0.72836 | Added: XGBClassifier_3 (801128d9854f893393d31943ee000e82)\n",
      "Iter 1000/1000: Best AUC = 0.72836 | Added: XGBClassifier_2 (cd0320203bc21b5b10db068b52204c49)\n",
      "[Optimized weights for fold]\n",
      "CatBoostClassifier_1 (6eca2de83af2fea3676cc9382d0f1011): 0.1200\n",
      "CatBoostClassifier_2 (b4347d306f6e52d59724382017b55f1b): 0.0000\n",
      "CatBoostClassifier_3 (fad9fe0d13abe5377a9667ad2452550c): 0.0000\n",
      "XGBClassifier_1 (3fd5db728b0f635ad3fb51e4ac3a5c8d): 0.0000\n",
      "XGBClassifier_2 (cd0320203bc21b5b10db068b52204c49): 0.4940\n",
      "XGBClassifier_3 (801128d9854f893393d31943ee000e82): 0.3860\n",
      "XGBRFClassifier_1 (aca0224f49af4aff337e4a3bd4021211): 0.0000\n",
      "\n",
      "##### Final Blended Weights #####\n",
      "CatBoostClassifier_1 (6eca2de83af2fea3676cc9382d0f1011): 0.0625\n",
      "CatBoostClassifier_2 (b4347d306f6e52d59724382017b55f1b): 0.0000\n",
      "CatBoostClassifier_3 (fad9fe0d13abe5377a9667ad2452550c): 0.0462\n",
      "XGBClassifier_1 (3fd5db728b0f635ad3fb51e4ac3a5c8d): 0.0018\n",
      "XGBClassifier_2 (cd0320203bc21b5b10db068b52204c49): 0.5050\n",
      "XGBClassifier_3 (801128d9854f893393d31943ee000e82): 0.3845\n",
      "XGBRFClassifier_1 (aca0224f49af4aff337e4a3bd4021211): 0.0000\n"
     ]
    }
   ],
   "source": [
    "def hill_climbing_blender(base_model_preds_df, y_true, iterations):\n",
    "    models = base_model_preds_df.columns\n",
    "    ensemble_preds = np.zeros(len(y_true))\n",
    "    selected_models = []\n",
    "    \n",
    "    print(f\"Starting Hill Climbing on {len(models)} models for {iterations} iterations...\")\n",
    "\n",
    "    best_auc = 0\n",
    "\n",
    "    for i in range(iterations):\n",
    "        best_iteration_auc = 0\n",
    "        best_model_to_add = None\n",
    "        \n",
    "        # try adding each model to the current ensemble\n",
    "        for model in models:\n",
    "            trial_preds = (ensemble_preds + base_model_preds_df[model]) / (i + 1)\n",
    "            \n",
    "            score = roc_auc_score(y_true, trial_preds)\n",
    "            \n",
    "            if score > best_iteration_auc:\n",
    "                best_iteration_auc = score\n",
    "                best_model_to_add = model\n",
    "\n",
    "        ensemble_preds += base_model_preds_df[best_model_to_add]\n",
    "        selected_models.append(best_model_to_add)\n",
    "        best_auc = best_iteration_auc\n",
    "        \n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(f\"Iter {i+1}/{iterations}: Best AUC = {best_auc:.5f} | Added: {best_model_to_add}\")\n",
    "\n",
    "    # weights are simply how many times each model was selected\n",
    "    weights = {model: 0.0 for model in models}\n",
    "    for model in selected_models:\n",
    "        weights[model] += 1\n",
    "\n",
    "    total_votes = len(selected_models)\n",
    "    weights = {k: v / total_votes for k, v in weights.items()}\n",
    "    \n",
    "    print(\"[Optimized weights for fold]\")\n",
    "    for model, weight in weights.items():\n",
    "        print(f\"{model}: {weight:.4f}\")\n",
    "        \n",
    "    return weights\n",
    "\n",
    "blended_weights_accumulator = {model: 0 for model in base_model_train_preds.columns}\n",
    "final_blended_weights = {model: 0 for model in base_model_train_preds.columns}\n",
    "\n",
    "for random_seed in BLENDING_RANDOM_SEEDS:\n",
    "    print(f\"\\n##### Random Seed {random_seed} #####\")\n",
    "    blend_skf = StratifiedKFold(n_splits=BLENDING_KFOLD_NUM_SPLITS, shuffle=True, random_state=random_seed)\n",
    "    blend_skf_splits = blend_skf.split(train_data.drop(target_col, axis=1), train_data[target_col])\n",
    "    blend_skf_enumeration = enumerate(blend_skf_splits)\n",
    "    \n",
    "    for fold, (train_indices, validation_indices) in blend_skf_enumeration:\n",
    "        print(f\"\\n### Fold {fold + 1} ###\\n\")\n",
    "        X_train_fold = base_model_train_preds.iloc[train_indices]\n",
    "        y_train_fold = train_data[target_col].iloc[train_indices]\n",
    "    \n",
    "        best_weights_fold = hill_climbing_blender(\n",
    "            base_model_preds_df=X_train_fold, \n",
    "            y_true=y_train_fold, \n",
    "            iterations=BLENDING_NUM_ITERATIONS\n",
    "        )\n",
    "        for model, weight in best_weights_fold.items():\n",
    "            blended_weights_accumulator[model] += weight\n",
    "\n",
    "for model, weight in blended_weights_accumulator.items():\n",
    "    final_blended_weights[model] = blended_weights_accumulator[model] / (BLENDING_KFOLD_NUM_SPLITS * len(BLENDING_RANDOM_SEEDS))\n",
    "\n",
    "print(\"\\n##### Final Blended Weights #####\")\n",
    "for model, weight in final_blended_weights.items():\n",
    "    print(f\"{model}: {weight:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b25a652d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T08:08:25.499179Z",
     "iopub.status.busy": "2025-12-10T08:08:25.498264Z",
     "iopub.status.idle": "2025-12-10T08:08:25.529661Z",
     "shell.execute_reply": "2025-12-10T08:08:25.528757Z"
    },
    "papermill": {
     "duration": 0.054296,
     "end_time": "2025-12-10T08:08:25.531502",
     "exception": false,
     "start_time": "2025-12-10T08:08:25.477206",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "blended_train_preds = np.zeros(len(base_model_train_preds))\n",
    "blended_test_preds = np.zeros(len(base_model_test_preds))\n",
    "\n",
    "for model, weight in final_blended_weights.items():\n",
    "    blended_train_preds += weight * base_model_train_preds[model]\n",
    "    blended_test_preds += weight * base_model_test_preds[model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0017d209",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T08:08:25.574050Z",
     "iopub.status.busy": "2025-12-10T08:08:25.573636Z",
     "iopub.status.idle": "2025-12-10T08:08:25.597667Z",
     "shell.execute_reply": "2025-12-10T08:08:25.596644Z"
    },
    "papermill": {
     "duration": 0.047177,
     "end_time": "2025-12-10T08:08:25.599513",
     "exception": false,
     "start_time": "2025-12-10T08:08:25.552336",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def min_max_scale(preds):\n",
    "    min_val = preds.min()\n",
    "    max_val = preds.max()\n",
    "    if max_val > min_val:\n",
    "        return (preds - min_val) / (max_val - min_val)\n",
    "    return preds\n",
    "\n",
    "# scale blended train and test preds\n",
    "scaled_blended_train_preds = min_max_scale(blended_train_preds)\n",
    "scaled_blended_test_preds = min_max_scale(blended_test_preds)\n",
    "\n",
    "# just in case floating point math leaves values very slightly below 0 or above 1\n",
    "scaled_blended_train_preds = np.clip(scaled_blended_train_preds, 0, 1)\n",
    "scaled_blended_test_preds = np.clip(scaled_blended_test_preds, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9eef2b55",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T08:08:25.641119Z",
     "iopub.status.busy": "2025-12-10T08:08:25.640795Z",
     "iopub.status.idle": "2025-12-10T08:08:26.005075Z",
     "shell.execute_reply": "2025-12-10T08:08:26.004222Z"
    },
    "papermill": {
     "duration": 0.38808,
     "end_time": "2025-12-10T08:08:26.007104",
     "exception": false,
     "start_time": "2025-12-10T08:08:25.619024",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.727954124109655"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blended_train_preds_auc = roc_auc_score(train_data[target_col], scaled_blended_train_preds)\n",
    "blended_train_preds_auc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06cb402",
   "metadata": {
    "papermill": {
     "duration": 0.019903,
     "end_time": "2025-12-10T08:08:26.048678",
     "exception": false,
     "start_time": "2025-12-10T08:08:26.028775",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 11. Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3f4cdbdb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T08:08:26.093962Z",
     "iopub.status.busy": "2025-12-10T08:08:26.093062Z",
     "iopub.status.idle": "2025-12-10T08:08:26.767145Z",
     "shell.execute_reply": "2025-12-10T08:08:26.766251Z"
    },
    "papermill": {
     "duration": 0.697824,
     "end_time": "2025-12-10T08:08:26.768806",
     "exception": false,
     "start_time": "2025-12-10T08:08:26.070982",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission file prepared.\n"
     ]
    }
   ],
   "source": [
    "# prepare submission\n",
    "submission = pd.DataFrame({'id': test_data.index, target_col: scaled_blended_test_preds})\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print('Submission file prepared.')"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 14272474,
     "sourceId": 91723,
     "sourceType": "competition"
    },
    {
     "databundleVersionId": 14864042,
     "datasetId": 8925440,
     "sourceId": 14082085,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31192,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 20144.425638,
   "end_time": "2025-12-10T08:08:29.593111",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-12-10T02:32:45.167473",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
