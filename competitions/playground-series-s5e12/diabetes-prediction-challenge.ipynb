{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d21b6ac",
   "metadata": {
    "papermill": {
     "duration": 0.010429,
     "end_time": "2025-12-18T05:01:34.458273",
     "exception": false,
     "start_time": "2025-12-18T05:01:34.447844",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 1. Overview\n",
    "\n",
    "This is a notebook for training models to submit predictions to the \"Diabetes Prediction Challenge\" Kaggle competition ([playground-series-s5e12](https://www.kaggle.com/competitions/playground-series-s5e12)).\n",
    "\n",
    "Synthetic data is used for this playground competition, and the objective is to, for each patient in the test set, predict the probability that the patient will be diagnosed with diabetes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3fd1d05",
   "metadata": {
    "papermill": {
     "duration": 0.008546,
     "end_time": "2025-12-18T05:01:34.475527",
     "exception": false,
     "start_time": "2025-12-18T05:01:34.466981",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 2. Setup\n",
    "\n",
    "## 2.1 Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d65df47e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-18T05:01:34.494121Z",
     "iopub.status.busy": "2025-12-18T05:01:34.493855Z",
     "iopub.status.idle": "2025-12-18T05:01:45.800388Z",
     "shell.execute_reply": "2025-12-18T05:01:45.799583Z"
    },
    "papermill": {
     "duration": 11.317798,
     "end_time": "2025-12-18T05:01:45.801787",
     "exception": false,
     "start_time": "2025-12-18T05:01:34.483989",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import copy\n",
    "import lightgbm as lgb\n",
    "import optuna\n",
    "import os\n",
    "import hashlib as hl # for StackingEstimator\n",
    "import inspect # for StackingEstimator\n",
    "import random\n",
    "import warnings\n",
    "from catboost import CatBoostClassifier\n",
    "from enum import Enum\n",
    "from pathlib import Path # for StackingPredictionsRetriever\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.linear_model import LinearRegression # for meta model\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import OrdinalEncoder, StandardScaler\n",
    "from types import FunctionType\n",
    "from xgboost import XGBClassifier, XGBRFClassifier\n",
    "\n",
    "warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "pd.set_option('display.max_colwidth', None) # Display full column content\n",
    "pd.set_option('display.max_rows', None) # Display all rows\n",
    "pd.set_option('display.width', 1000) # Set larger display width"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04415ed4",
   "metadata": {
    "papermill": {
     "duration": 0.010685,
     "end_time": "2025-12-18T05:01:45.821595",
     "exception": false,
     "start_time": "2025-12-18T05:01:45.810910",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2.2 Reproducibility\n",
    "\n",
    "For reproducibility of results, an arbitrary number will be used for the random seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0cbfc20c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-18T05:01:45.840926Z",
     "iopub.status.busy": "2025-12-18T05:01:45.840437Z",
     "iopub.status.idle": "2025-12-18T05:01:45.891890Z",
     "shell.execute_reply": "2025-12-18T05:01:45.891188Z"
    },
    "papermill": {
     "duration": 0.062855,
     "end_time": "2025-12-18T05:01:45.893096",
     "exception": false,
     "start_time": "2025-12-18T05:01:45.830241",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "RANDOM_SEEDS = [11, 42]\n",
    "random.seed(RANDOM_SEEDS[0])\n",
    "np.random.seed(RANDOM_SEEDS[0])\n",
    "torch.manual_seed(RANDOM_SEEDS[0])\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(RANDOM_SEEDS[0])\n",
    "    torch.cuda.manual_seed_all(RANDOM_SEEDS[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e21f576",
   "metadata": {
    "papermill": {
     "duration": 0.008522,
     "end_time": "2025-12-18T05:01:45.910387",
     "exception": false,
     "start_time": "2025-12-18T05:01:45.901865",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2.3 Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf64fd4a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-18T05:01:45.928379Z",
     "iopub.status.busy": "2025-12-18T05:01:45.928160Z",
     "iopub.status.idle": "2025-12-18T05:01:45.931628Z",
     "shell.execute_reply": "2025-12-18T05:01:45.930936Z"
    },
    "papermill": {
     "duration": 0.013855,
     "end_time": "2025-12-18T05:01:45.932696",
     "exception": false,
     "start_time": "2025-12-18T05:01:45.918841",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97c75a1",
   "metadata": {
    "papermill": {
     "duration": 0.008267,
     "end_time": "2025-12-18T05:01:45.949377",
     "exception": false,
     "start_time": "2025-12-18T05:01:45.941110",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2.4 DataFrames\n",
    "\n",
    "Read the data provided for the competition into dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7c70094",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-18T05:01:45.969624Z",
     "iopub.status.busy": "2025-12-18T05:01:45.969190Z",
     "iopub.status.idle": "2025-12-18T05:01:48.347553Z",
     "shell.execute_reply": "2025-12-18T05:01:48.346968Z"
    },
    "papermill": {
     "duration": 2.391232,
     "end_time": "2025-12-18T05:01:48.348915",
     "exception": false,
     "start_time": "2025-12-18T05:01:45.957683",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "INPUT_DIR = '/kaggle/input'\n",
    "orig_train_data = pd.read_csv(os.path.join(INPUT_DIR, 'playground-series-s5e12/train.csv'))\n",
    "orig_test_data = pd.read_csv(os.path.join(INPUT_DIR, 'playground-series-s5e12/test.csv'))\n",
    "\n",
    "# set index\n",
    "orig_train_data.set_index('id', inplace=True)\n",
    "orig_test_data.set_index('id', inplace=True)\n",
    "\n",
    "# target column\n",
    "target_col = \"diagnosed_diabetes\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ba4bce",
   "metadata": {
    "papermill": {
     "duration": 0.008388,
     "end_time": "2025-12-18T05:01:48.366463",
     "exception": false,
     "start_time": "2025-12-18T05:01:48.358075",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 3. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63a3e338",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-18T05:01:48.384292Z",
     "iopub.status.busy": "2025-12-18T05:01:48.384057Z",
     "iopub.status.idle": "2025-12-18T05:01:48.387041Z",
     "shell.execute_reply": "2025-12-18T05:01:48.386513Z"
    },
    "papermill": {
     "duration": 0.013262,
     "end_time": "2025-12-18T05:01:48.388071",
     "exception": false,
     "start_time": "2025-12-18T05:01:48.374809",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# to skip the generation of plots (e.g. KDE) in this section that take time; set to False to generate the plots \n",
    "SKIP_PLOTS = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65fb13ac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-18T05:01:48.406104Z",
     "iopub.status.busy": "2025-12-18T05:01:48.405871Z",
     "iopub.status.idle": "2025-12-18T05:01:48.884814Z",
     "shell.execute_reply": "2025-12-18T05:01:48.884176Z"
    },
    "papermill": {
     "duration": 0.489357,
     "end_time": "2025-12-18T05:01:48.885959",
     "exception": false,
     "start_time": "2025-12-18T05:01:48.396602",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>alcohol_consumption_per_week</th>\n",
       "      <th>physical_activity_minutes_per_week</th>\n",
       "      <th>diet_score</th>\n",
       "      <th>sleep_hours_per_day</th>\n",
       "      <th>screen_time_hours_per_day</th>\n",
       "      <th>bmi</th>\n",
       "      <th>waist_to_hip_ratio</th>\n",
       "      <th>systolic_bp</th>\n",
       "      <th>diastolic_bp</th>\n",
       "      <th>heart_rate</th>\n",
       "      <th>cholesterol_total</th>\n",
       "      <th>hdl_cholesterol</th>\n",
       "      <th>ldl_cholesterol</th>\n",
       "      <th>triglycerides</th>\n",
       "      <th>family_history_diabetes</th>\n",
       "      <th>hypertension_history</th>\n",
       "      <th>cardiovascular_history</th>\n",
       "      <th>diagnosed_diabetes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>700000.000000</td>\n",
       "      <td>700000.000000</td>\n",
       "      <td>700000.000000</td>\n",
       "      <td>700000.000000</td>\n",
       "      <td>700000.000000</td>\n",
       "      <td>700000.000000</td>\n",
       "      <td>700000.000000</td>\n",
       "      <td>700000.000000</td>\n",
       "      <td>700000.000000</td>\n",
       "      <td>700000.000000</td>\n",
       "      <td>700000.000000</td>\n",
       "      <td>700000.000000</td>\n",
       "      <td>700000.000000</td>\n",
       "      <td>700000.000000</td>\n",
       "      <td>700000.000000</td>\n",
       "      <td>700000.000000</td>\n",
       "      <td>700000.000000</td>\n",
       "      <td>700000.000000</td>\n",
       "      <td>700000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>50.359734</td>\n",
       "      <td>2.072411</td>\n",
       "      <td>80.230803</td>\n",
       "      <td>5.963695</td>\n",
       "      <td>7.002200</td>\n",
       "      <td>6.012733</td>\n",
       "      <td>25.874684</td>\n",
       "      <td>0.858766</td>\n",
       "      <td>116.294193</td>\n",
       "      <td>75.440924</td>\n",
       "      <td>70.167749</td>\n",
       "      <td>186.818801</td>\n",
       "      <td>53.823214</td>\n",
       "      <td>102.905854</td>\n",
       "      <td>123.081850</td>\n",
       "      <td>0.149401</td>\n",
       "      <td>0.181990</td>\n",
       "      <td>0.030324</td>\n",
       "      <td>0.623296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>11.655520</td>\n",
       "      <td>1.048189</td>\n",
       "      <td>51.195071</td>\n",
       "      <td>1.463336</td>\n",
       "      <td>0.901907</td>\n",
       "      <td>2.022707</td>\n",
       "      <td>2.860705</td>\n",
       "      <td>0.037980</td>\n",
       "      <td>11.010390</td>\n",
       "      <td>6.825775</td>\n",
       "      <td>6.938722</td>\n",
       "      <td>16.730832</td>\n",
       "      <td>8.266545</td>\n",
       "      <td>19.022416</td>\n",
       "      <td>24.739397</td>\n",
       "      <td>0.356484</td>\n",
       "      <td>0.385837</td>\n",
       "      <td>0.171478</td>\n",
       "      <td>0.484560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>19.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>3.100000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>15.100000</td>\n",
       "      <td>0.680000</td>\n",
       "      <td>91.000000</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>117.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>42.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>49.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>6.400000</td>\n",
       "      <td>4.600000</td>\n",
       "      <td>23.900000</td>\n",
       "      <td>0.830000</td>\n",
       "      <td>108.000000</td>\n",
       "      <td>71.000000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>175.000000</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>89.000000</td>\n",
       "      <td>106.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>50.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>71.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>25.900000</td>\n",
       "      <td>0.860000</td>\n",
       "      <td>116.000000</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>187.000000</td>\n",
       "      <td>54.000000</td>\n",
       "      <td>103.000000</td>\n",
       "      <td>123.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>58.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>96.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.600000</td>\n",
       "      <td>7.400000</td>\n",
       "      <td>27.800000</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>124.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>199.000000</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>116.000000</td>\n",
       "      <td>139.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>89.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>747.000000</td>\n",
       "      <td>9.900000</td>\n",
       "      <td>9.900000</td>\n",
       "      <td>16.500000</td>\n",
       "      <td>38.400000</td>\n",
       "      <td>1.050000</td>\n",
       "      <td>163.000000</td>\n",
       "      <td>104.000000</td>\n",
       "      <td>101.000000</td>\n",
       "      <td>289.000000</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>205.000000</td>\n",
       "      <td>290.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 age  alcohol_consumption_per_week  physical_activity_minutes_per_week     diet_score  sleep_hours_per_day  screen_time_hours_per_day            bmi  waist_to_hip_ratio    systolic_bp   diastolic_bp     heart_rate  cholesterol_total  hdl_cholesterol  ldl_cholesterol  triglycerides  family_history_diabetes  hypertension_history  cardiovascular_history  diagnosed_diabetes\n",
       "count  700000.000000                 700000.000000                       700000.000000  700000.000000        700000.000000              700000.000000  700000.000000       700000.000000  700000.000000  700000.000000  700000.000000      700000.000000    700000.000000    700000.000000  700000.000000            700000.000000         700000.000000           700000.000000       700000.000000\n",
       "mean       50.359734                      2.072411                           80.230803       5.963695             7.002200                   6.012733      25.874684            0.858766     116.294193      75.440924      70.167749         186.818801        53.823214       102.905854     123.081850                 0.149401              0.181990                0.030324            0.623296\n",
       "std        11.655520                      1.048189                           51.195071       1.463336             0.901907                   2.022707       2.860705            0.037980      11.010390       6.825775       6.938722          16.730832         8.266545        19.022416      24.739397                 0.356484              0.385837                0.171478            0.484560\n",
       "min        19.000000                      1.000000                            1.000000       0.100000             3.100000                   0.600000      15.100000            0.680000      91.000000      51.000000      42.000000         117.000000        21.000000        51.000000      31.000000                 0.000000              0.000000                0.000000            0.000000\n",
       "25%        42.000000                      1.000000                           49.000000       5.000000             6.400000                   4.600000      23.900000            0.830000     108.000000      71.000000      65.000000         175.000000        48.000000        89.000000     106.000000                 0.000000              0.000000                0.000000            0.000000\n",
       "50%        50.000000                      2.000000                           71.000000       6.000000             7.000000                   6.000000      25.900000            0.860000     116.000000      75.000000      70.000000         187.000000        54.000000       103.000000     123.000000                 0.000000              0.000000                0.000000            1.000000\n",
       "75%        58.000000                      3.000000                           96.000000       7.000000             7.600000                   7.400000      27.800000            0.880000     124.000000      80.000000      75.000000         199.000000        59.000000       116.000000     139.000000                 0.000000              0.000000                0.000000            1.000000\n",
       "max        89.000000                      9.000000                          747.000000       9.900000             9.900000                  16.500000      38.400000            1.050000     163.000000     104.000000     101.000000         289.000000        90.000000       205.000000     290.000000                 1.000000              1.000000                1.000000            1.000000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig_train_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b4d34a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-18T05:01:48.905320Z",
     "iopub.status.busy": "2025-12-18T05:01:48.905100Z",
     "iopub.status.idle": "2025-12-18T05:01:49.095424Z",
     "shell.execute_reply": "2025-12-18T05:01:49.094715Z"
    },
    "papermill": {
     "duration": 0.201183,
     "end_time": "2025-12-18T05:01:49.096552",
     "exception": false,
     "start_time": "2025-12-18T05:01:48.895369",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>alcohol_consumption_per_week</th>\n",
       "      <th>physical_activity_minutes_per_week</th>\n",
       "      <th>diet_score</th>\n",
       "      <th>sleep_hours_per_day</th>\n",
       "      <th>screen_time_hours_per_day</th>\n",
       "      <th>bmi</th>\n",
       "      <th>waist_to_hip_ratio</th>\n",
       "      <th>systolic_bp</th>\n",
       "      <th>diastolic_bp</th>\n",
       "      <th>heart_rate</th>\n",
       "      <th>cholesterol_total</th>\n",
       "      <th>hdl_cholesterol</th>\n",
       "      <th>ldl_cholesterol</th>\n",
       "      <th>triglycerides</th>\n",
       "      <th>family_history_diabetes</th>\n",
       "      <th>hypertension_history</th>\n",
       "      <th>cardiovascular_history</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>300000.000000</td>\n",
       "      <td>300000.000000</td>\n",
       "      <td>300000.000000</td>\n",
       "      <td>300000.000000</td>\n",
       "      <td>300000.000000</td>\n",
       "      <td>300000.000000</td>\n",
       "      <td>300000.000000</td>\n",
       "      <td>300000.000000</td>\n",
       "      <td>300000.000000</td>\n",
       "      <td>300000.000000</td>\n",
       "      <td>300000.000000</td>\n",
       "      <td>300000.000000</td>\n",
       "      <td>300000.000000</td>\n",
       "      <td>300000.000000</td>\n",
       "      <td>300000.000000</td>\n",
       "      <td>300000.000000</td>\n",
       "      <td>300000.000000</td>\n",
       "      <td>300000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>50.432397</td>\n",
       "      <td>2.089693</td>\n",
       "      <td>92.349087</td>\n",
       "      <td>5.945838</td>\n",
       "      <td>6.997795</td>\n",
       "      <td>6.011278</td>\n",
       "      <td>25.881906</td>\n",
       "      <td>0.859007</td>\n",
       "      <td>116.374117</td>\n",
       "      <td>75.396013</td>\n",
       "      <td>70.048350</td>\n",
       "      <td>187.308620</td>\n",
       "      <td>53.813557</td>\n",
       "      <td>103.416083</td>\n",
       "      <td>123.538480</td>\n",
       "      <td>0.152920</td>\n",
       "      <td>0.184410</td>\n",
       "      <td>0.033110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>11.938741</td>\n",
       "      <td>1.066214</td>\n",
       "      <td>62.187399</td>\n",
       "      <td>1.481068</td>\n",
       "      <td>0.914693</td>\n",
       "      <td>2.060472</td>\n",
       "      <td>2.894289</td>\n",
       "      <td>0.038523</td>\n",
       "      <td>11.252146</td>\n",
       "      <td>6.950340</td>\n",
       "      <td>7.090543</td>\n",
       "      <td>18.413053</td>\n",
       "      <td>8.398126</td>\n",
       "      <td>20.571855</td>\n",
       "      <td>28.965441</td>\n",
       "      <td>0.359911</td>\n",
       "      <td>0.387819</td>\n",
       "      <td>0.178924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>19.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>3.100000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>15.100000</td>\n",
       "      <td>0.690000</td>\n",
       "      <td>91.000000</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>107.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>42.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>6.400000</td>\n",
       "      <td>4.600000</td>\n",
       "      <td>23.900000</td>\n",
       "      <td>0.830000</td>\n",
       "      <td>108.000000</td>\n",
       "      <td>71.000000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>174.000000</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>89.000000</td>\n",
       "      <td>104.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>50.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>25.900000</td>\n",
       "      <td>0.860000</td>\n",
       "      <td>116.000000</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>187.000000</td>\n",
       "      <td>54.000000</td>\n",
       "      <td>103.000000</td>\n",
       "      <td>123.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>59.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>115.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.600000</td>\n",
       "      <td>7.400000</td>\n",
       "      <td>27.800000</td>\n",
       "      <td>0.890000</td>\n",
       "      <td>124.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>117.000000</td>\n",
       "      <td>142.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>89.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>748.000000</td>\n",
       "      <td>9.900000</td>\n",
       "      <td>9.900000</td>\n",
       "      <td>15.900000</td>\n",
       "      <td>38.300000</td>\n",
       "      <td>1.050000</td>\n",
       "      <td>170.000000</td>\n",
       "      <td>104.000000</td>\n",
       "      <td>101.000000</td>\n",
       "      <td>285.000000</td>\n",
       "      <td>91.000000</td>\n",
       "      <td>226.000000</td>\n",
       "      <td>290.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 age  alcohol_consumption_per_week  physical_activity_minutes_per_week     diet_score  sleep_hours_per_day  screen_time_hours_per_day            bmi  waist_to_hip_ratio    systolic_bp   diastolic_bp     heart_rate  cholesterol_total  hdl_cholesterol  ldl_cholesterol  triglycerides  family_history_diabetes  hypertension_history  cardiovascular_history\n",
       "count  300000.000000                 300000.000000                       300000.000000  300000.000000        300000.000000              300000.000000  300000.000000       300000.000000  300000.000000  300000.000000  300000.000000      300000.000000    300000.000000    300000.000000  300000.000000            300000.000000         300000.000000           300000.000000\n",
       "mean       50.432397                      2.089693                           92.349087       5.945838             6.997795                   6.011278      25.881906            0.859007     116.374117      75.396013      70.048350         187.308620        53.813557       103.416083     123.538480                 0.152920              0.184410                0.033110\n",
       "std        11.938741                      1.066214                           62.187399       1.481068             0.914693                   2.060472       2.894289            0.038523      11.252146       6.950340       7.090543          18.413053         8.398126        20.571855      28.965441                 0.359911              0.387819                0.178924\n",
       "min        19.000000                      1.000000                            1.000000       0.100000             3.100000                   0.600000      15.100000            0.690000      91.000000      51.000000      42.000000         107.000000        22.000000        51.000000      31.000000                 0.000000              0.000000                0.000000\n",
       "25%        42.000000                      1.000000                           51.000000       5.000000             6.400000                   4.600000      23.900000            0.830000     108.000000      71.000000      65.000000         174.000000        48.000000        89.000000     104.000000                 0.000000              0.000000                0.000000\n",
       "50%        50.000000                      2.000000                           77.000000       6.000000             7.000000                   6.000000      25.900000            0.860000     116.000000      75.000000      70.000000         187.000000        54.000000       103.000000     123.000000                 0.000000              0.000000                0.000000\n",
       "75%        59.000000                      3.000000                          115.000000       7.000000             7.600000                   7.400000      27.800000            0.890000     124.000000      80.000000      75.000000         200.000000        60.000000       117.000000     142.000000                 0.000000              0.000000                0.000000\n",
       "max        89.000000                      9.000000                          748.000000       9.900000             9.900000                  15.900000      38.300000            1.050000     170.000000     104.000000     101.000000         285.000000        91.000000       226.000000     290.000000                 1.000000              1.000000                1.000000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig_test_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "585418d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-18T05:01:49.116513Z",
     "iopub.status.busy": "2025-12-18T05:01:49.116277Z",
     "iopub.status.idle": "2025-12-18T05:01:49.234519Z",
     "shell.execute_reply": "2025-12-18T05:01:49.233931Z"
    },
    "papermill": {
     "duration": 0.129523,
     "end_time": "2025-12-18T05:01:49.235854",
     "exception": false,
     "start_time": "2025-12-18T05:01:49.106331",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "numeric_col_names = orig_train_data.select_dtypes(include='number').columns.to_series()\n",
    "categorical_col_names = orig_train_data.select_dtypes(include='object').columns.to_series()\n",
    "assert numeric_col_names.size + categorical_col_names.size == orig_train_data.shape[1]\n",
    "\n",
    "# drop target column from numeric column names\n",
    "numeric_col_names.drop(target_col, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e40b195",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-18T05:01:49.255447Z",
     "iopub.status.busy": "2025-12-18T05:01:49.255186Z",
     "iopub.status.idle": "2025-12-18T05:01:49.532315Z",
     "shell.execute_reply": "2025-12-18T05:01:49.531523Z"
    },
    "papermill": {
     "duration": 0.288162,
     "end_time": "2025-12-18T05:01:49.533464",
     "exception": false,
     "start_time": "2025-12-18T05:01:49.245302",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Train data missing values #####\n",
      "age                                   0\n",
      "alcohol_consumption_per_week          0\n",
      "physical_activity_minutes_per_week    0\n",
      "diet_score                            0\n",
      "sleep_hours_per_day                   0\n",
      "screen_time_hours_per_day             0\n",
      "bmi                                   0\n",
      "waist_to_hip_ratio                    0\n",
      "systolic_bp                           0\n",
      "diastolic_bp                          0\n",
      "heart_rate                            0\n",
      "cholesterol_total                     0\n",
      "hdl_cholesterol                       0\n",
      "ldl_cholesterol                       0\n",
      "triglycerides                         0\n",
      "gender                                0\n",
      "ethnicity                             0\n",
      "education_level                       0\n",
      "income_level                          0\n",
      "smoking_status                        0\n",
      "employment_status                     0\n",
      "family_history_diabetes               0\n",
      "hypertension_history                  0\n",
      "cardiovascular_history                0\n",
      "diagnosed_diabetes                    0\n",
      "dtype: int64\n",
      "\n",
      "##### Test data missing values #####\n",
      "age                                   0\n",
      "alcohol_consumption_per_week          0\n",
      "physical_activity_minutes_per_week    0\n",
      "diet_score                            0\n",
      "sleep_hours_per_day                   0\n",
      "screen_time_hours_per_day             0\n",
      "bmi                                   0\n",
      "waist_to_hip_ratio                    0\n",
      "systolic_bp                           0\n",
      "diastolic_bp                          0\n",
      "heart_rate                            0\n",
      "cholesterol_total                     0\n",
      "hdl_cholesterol                       0\n",
      "ldl_cholesterol                       0\n",
      "triglycerides                         0\n",
      "gender                                0\n",
      "ethnicity                             0\n",
      "education_level                       0\n",
      "income_level                          0\n",
      "smoking_status                        0\n",
      "employment_status                     0\n",
      "family_history_diabetes               0\n",
      "hypertension_history                  0\n",
      "cardiovascular_history                0\n",
      "dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for (dataset_name, dataset) in [('Train data', orig_train_data), ('Test data', orig_test_data)]:\n",
    "    print(f\"##### {dataset_name} missing values #####\")\n",
    "    print(dataset.isnull().sum())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e1afeda6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-18T05:01:49.553537Z",
     "iopub.status.busy": "2025-12-18T05:01:49.552949Z",
     "iopub.status.idle": "2025-12-18T05:01:49.814716Z",
     "shell.execute_reply": "2025-12-18T05:01:49.813864Z"
    },
    "papermill": {
     "duration": 0.2728,
     "end_time": "2025-12-18T05:01:49.815800",
     "exception": false,
     "start_time": "2025-12-18T05:01:49.543000",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Train data categorical cols unique values #####\n",
      "gender:\n",
      "['Female' 'Male' 'Other']\n",
      "ethnicity:\n",
      "['Hispanic' 'White' 'Asian' 'Black' 'Other']\n",
      "education_level:\n",
      "['Highschool' 'Graduate' 'Postgraduate' 'No formal']\n",
      "income_level:\n",
      "['Lower-Middle' 'Upper-Middle' 'Low' 'Middle' 'High']\n",
      "smoking_status:\n",
      "['Current' 'Never' 'Former']\n",
      "employment_status:\n",
      "['Employed' 'Retired' 'Student' 'Unemployed']\n",
      "\n",
      "##### Test data categorical cols unique values #####\n",
      "gender:\n",
      "['Female' 'Male' 'Other']\n",
      "ethnicity:\n",
      "['White' 'Hispanic' 'Black' 'Asian' 'Other']\n",
      "education_level:\n",
      "['Highschool' 'Graduate' 'Postgraduate' 'No formal']\n",
      "income_level:\n",
      "['Middle' 'Low' 'Lower-Middle' 'Upper-Middle' 'High']\n",
      "smoking_status:\n",
      "['Former' 'Never' 'Current']\n",
      "employment_status:\n",
      "['Employed' 'Unemployed' 'Retired' 'Student']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for (dataset_name, dataset) in [('Train data', orig_train_data), ('Test data', orig_test_data)]:\n",
    "    print(f\"##### {dataset_name} categorical cols unique values #####\")\n",
    "    for categorical_col_name in categorical_col_names:\n",
    "        print(f\"{categorical_col_name}:\")\n",
    "        print(dataset[categorical_col_name].unique())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d31b55e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-18T05:01:49.837423Z",
     "iopub.status.busy": "2025-12-18T05:01:49.836979Z",
     "iopub.status.idle": "2025-12-18T05:01:49.841240Z",
     "shell.execute_reply": "2025-12-18T05:01:49.840676Z"
    },
    "papermill": {
     "duration": 0.016724,
     "end_time": "2025-12-18T05:01:49.842380",
     "exception": false,
     "start_time": "2025-12-18T05:01:49.825656",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# KDE plots of target variable and numerical features\n",
    "if not SKIP_PLOTS:\n",
    "    plt.figure(figsize=(12, 24))\n",
    "    kdeplot_col_names = [target_col]\n",
    "    kdeplot_col_names.extend(numeric_col_names)\n",
    "    for i, col in enumerate(kdeplot_col_names, start=1):\n",
    "        plt.subplot(10, 2, i)\n",
    "        sns.kdeplot(data=orig_train_data, x=col, fill=True)\n",
    "        plt.tight_layout()\n",
    "        plt.title(f\"KDE plot of {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "99a67315",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-18T05:01:49.861953Z",
     "iopub.status.busy": "2025-12-18T05:01:49.861503Z",
     "iopub.status.idle": "2025-12-18T05:01:49.865149Z",
     "shell.execute_reply": "2025-12-18T05:01:49.864666Z"
    },
    "papermill": {
     "duration": 0.014606,
     "end_time": "2025-12-18T05:01:49.866174",
     "exception": false,
     "start_time": "2025-12-18T05:01:49.851568",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not SKIP_PLOTS:\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.heatmap(\n",
    "        orig_train_data[numeric_col_names].corr(),\n",
    "        cmap='Reds',\n",
    "        annot=True,\n",
    "        linewidths=2,\n",
    "        fmt='.2f',\n",
    "        vmin=-1,\n",
    "        vmax=1\n",
    "    )\n",
    "    plt.title('Correlation Matrix of Numerical Features', fontsize=18, pad=20)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5394fa73",
   "metadata": {
    "papermill": {
     "duration": 0.009083,
     "end_time": "2025-12-18T05:01:49.884523",
     "exception": false,
     "start_time": "2025-12-18T05:01:49.875440",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 4. Data Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e3ca41e2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-18T05:01:49.903484Z",
     "iopub.status.busy": "2025-12-18T05:01:49.903305Z",
     "iopub.status.idle": "2025-12-18T05:01:50.060944Z",
     "shell.execute_reply": "2025-12-18T05:01:50.060356Z"
    },
    "papermill": {
     "duration": 0.168739,
     "end_time": "2025-12-18T05:01:50.062337",
     "exception": false,
     "start_time": "2025-12-18T05:01:49.893598",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data = orig_train_data.copy()\n",
    "test_data = orig_test_data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6e976a",
   "metadata": {
    "papermill": {
     "duration": 0.00927,
     "end_time": "2025-12-18T05:01:50.081384",
     "exception": false,
     "start_time": "2025-12-18T05:01:50.072114",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 4.1 Ordinal Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ec6d622d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-18T05:01:50.107848Z",
     "iopub.status.busy": "2025-12-18T05:01:50.107605Z",
     "iopub.status.idle": "2025-12-18T05:01:50.991839Z",
     "shell.execute_reply": "2025-12-18T05:01:50.990989Z"
    },
    "papermill": {
     "duration": 0.902118,
     "end_time": "2025-12-18T05:01:50.992970",
     "exception": false,
     "start_time": "2025-12-18T05:01:50.090852",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "education_level_encoded:\n",
      "{'No formal': 0, 'Highschool': 1, 'Graduate': 2, 'Postgraduate': 3}\n",
      "income_level_encoded:\n",
      "{'Low': 0, 'Lower-Middle': 1, 'Middle': 2, 'Upper-Middle': 3, 'High': 4}\n",
      "smoking_status_encoded:\n",
      "{'Never': 0, 'Former': 1, 'Current': 2}\n"
     ]
    }
   ],
   "source": [
    "# education level\n",
    "education_level_encoder = OrdinalEncoder(categories=[['No formal', 'Highschool', 'Graduate', 'Postgraduate']])\n",
    "train_data['education_level_encoded'] = education_level_encoder.fit_transform(train_data[['education_level']])\n",
    "test_data['education_level_encoded'] = education_level_encoder.fit_transform(test_data[['education_level']])\n",
    "\n",
    "# income level\n",
    "income_level_encoder = OrdinalEncoder(categories=[['Low', 'Lower-Middle','Middle', 'Upper-Middle', 'High']])\n",
    "train_data['income_level_encoded'] = income_level_encoder.fit_transform(train_data[['income_level']])\n",
    "test_data['income_level_encoded'] = income_level_encoder.fit_transform(test_data[['income_level']])\n",
    "\n",
    "# smoking status\n",
    "smoking_status_encoder = OrdinalEncoder(categories=[['Never', 'Former', 'Current']])\n",
    "train_data['smoking_status_encoded'] = smoking_status_encoder.fit_transform(train_data[['smoking_status']])\n",
    "test_data['smoking_status_encoded'] = smoking_status_encoder.fit_transform(test_data[['smoking_status']])\n",
    "\n",
    "# drop original cols\n",
    "for col in ['income_level', 'education_level', 'smoking_status']:\n",
    "    train_data.drop(col, axis=1, inplace=True)\n",
    "    test_data.drop(col, axis=1, inplace=True)\n",
    "\n",
    "# print out value maps to check assigned values are as expected\n",
    "for (encoded_col_name, encoder) in [\n",
    "    ('education_level_encoded', education_level_encoder),\n",
    "    ('income_level_encoded', income_level_encoder),\n",
    "    ('smoking_status_encoded', smoking_status_encoder),\n",
    "]:\n",
    "    categories = encoder.categories_[0]\n",
    "    value_map = { category: i for i, category in enumerate(categories) }\n",
    "    print(f\"{encoded_col_name}:\\n{value_map}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b54238",
   "metadata": {
    "papermill": {
     "duration": 0.010634,
     "end_time": "2025-12-18T05:01:51.013785",
     "exception": false,
     "start_time": "2025-12-18T05:01:51.003151",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 4.2 Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9ea441af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-18T05:01:51.033690Z",
     "iopub.status.busy": "2025-12-18T05:01:51.033265Z",
     "iopub.status.idle": "2025-12-18T05:01:51.048649Z",
     "shell.execute_reply": "2025-12-18T05:01:51.048118Z"
    },
    "papermill": {
     "duration": 0.026484,
     "end_time": "2025-12-18T05:01:51.049661",
     "exception": false,
     "start_time": "2025-12-18T05:01:51.023177",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fix_blood_pressure(df):\n",
    "    mask = df['diastolic_bp'] > df['systolic_bp']\n",
    "    df.loc[mask, ['systolic_bp', 'diastolic_bp']] = (\n",
    "        df.loc[mask, ['diastolic_bp', 'systolic_bp']].values\n",
    "    )\n",
    "    return df\n",
    "\n",
    "train_data = fix_blood_pressure(train_data)\n",
    "test_data = fix_blood_pressure(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9206a7",
   "metadata": {
    "papermill": {
     "duration": 0.009317,
     "end_time": "2025-12-18T05:01:51.068294",
     "exception": false,
     "start_time": "2025-12-18T05:01:51.058977",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 4.3 Feature Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fa1061ba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-18T05:01:51.087808Z",
     "iopub.status.busy": "2025-12-18T05:01:51.087606Z",
     "iopub.status.idle": "2025-12-18T05:01:51.095120Z",
     "shell.execute_reply": "2025-12-18T05:01:51.094543Z"
    },
    "papermill": {
     "duration": 0.018667,
     "end_time": "2025-12-18T05:01:51.096106",
     "exception": false,
     "start_time": "2025-12-18T05:01:51.077439",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_generated_features(df):\n",
    "    # log transforms for skewed data\n",
    "    for col in ['triglycerides', 'ldl_cholesterol', 'cholesterol_total']:\n",
    "        df[f'log_{col}'] = np.log1p(df[col])\n",
    "\n",
    "    # medical ratios & interactions\n",
    "    df['cholesterol_ratio'] = df['cholesterol_total'] / (df['hdl_cholesterol'] + 1e-5)\n",
    "    df['ldl_hdl_ratio'] = df['ldl_cholesterol'] / (df['hdl_cholesterol'] + 1e-5)\n",
    "    df['pulse_pressure'] = df['systolic_bp'] - df['diastolic_bp']\n",
    "    df['mean_arterial_pressure'] = (df['systolic_bp'] + 2 * df['diastolic_bp']) / 3\n",
    "    df['age_x_bmi'] = df['age'] * df['bmi']\n",
    "    df['waist_x_bmi'] = df['waist_to_hip_ratio'] * df['bmi']\n",
    "    df['family_history_diabetes_x_log_triglycerides'] = df['family_history_diabetes'] * df['log_triglycerides']\n",
    "    df['hypertension_history_x_systolic_bp'] = df['hypertension_history'] * df['systolic_bp']\n",
    "    df['activity_x_diet'] = df['physical_activity_minutes_per_week'] * df['diet_score']\n",
    "    df['atherogenic_index'] = np.log((df['triglycerides'] / (df['hdl_cholesterol'] + 1e-5)) + 1e-5)\n",
    "    df['non_hdl_cholesterol'] = df['cholesterol_total'] - df['hdl_cholesterol']\n",
    "    df['map_x_bmi'] = df['mean_arterial_pressure'] * df['bmi']\n",
    "    df['lipid_accumulation_proxy'] = df['waist_to_hip_ratio'] * df['log_triglycerides']\n",
    "    df['visceral_adiposity_proxy'] = (df['bmi'] * df['triglycerides']) / (df['hdl_cholesterol'] + 1e-5)\n",
    "\n",
    "    # squared\n",
    "    df['age_sq'] = df['age'] ** 2\n",
    "    df['bmi_sq'] = df['bmi'] ** 2\n",
    "    df['waist_to_hip_ratio_sq'] = df['waist_to_hip_ratio'] ** 2\n",
    "    df['systolic_bp_sq'] = df['systolic_bp'] ** 2\n",
    "\n",
    "    # risk grouping\n",
    "    df['comorbidity_count'] = (\n",
    "        df['hypertension_history'] + \n",
    "        df['cardiovascular_history'] + \n",
    "        df['family_history_diabetes']\n",
    "    )\n",
    "\n",
    "    # binning\n",
    "    df['bmi_cat'] = pd.cut(df['bmi'], bins=[-1, 25, 30, 100], labels=[0, 1, 2]).astype(int)\n",
    "\n",
    "    # relative BMI and BP\n",
    "    df['age_decade'] = (df['age'] // 10).astype(int)\n",
    "    for group_col in ['age_decade', 'hypertension_history']:\n",
    "        for num_col in ['bmi', 'systolic_bp', 'cholesterol_total']:\n",
    "            group_means = df.groupby(group_col)[num_col].transform('mean')\n",
    "            df[f'{num_col}_relative_to_{group_col}'] = df[num_col] - group_means\n",
    "    df.drop(columns=['age_decade'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3ac1ac72",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-18T05:01:51.115824Z",
     "iopub.status.busy": "2025-12-18T05:01:51.115591Z",
     "iopub.status.idle": "2025-12-18T05:01:51.120688Z",
     "shell.execute_reply": "2025-12-18T05:01:51.120042Z"
    },
    "papermill": {
     "duration": 0.016432,
     "end_time": "2025-12-18T05:01:51.121673",
     "exception": false,
     "start_time": "2025-12-18T05:01:51.105241",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_kmeans_features(train_df, test_df, n_clusters):\n",
    "    features_to_cluster = [\n",
    "        'age', 'bmi', 'mean_arterial_pressure', 'cholesterol_ratio', 'log_triglycerides'\n",
    "    ]\n",
    "    \n",
    "    combined = pd.concat([train_df[features_to_cluster], test_df[features_to_cluster]], axis=0)\n",
    "    scaler = StandardScaler()\n",
    "    scaled_data = scaler.fit_transform(combined)\n",
    "    \n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=RANDOM_SEEDS[0], n_init=10)\n",
    "    clusters = kmeans.fit_predict(scaled_data)\n",
    "\n",
    "    train_dists = kmeans.transform(scaled_data[:len(train_df)])\n",
    "    test_dists = kmeans.transform(scaled_data[len(train_df):])\n",
    "\n",
    "    train_df['cluster_label'] = clusters[:len(train_df)].astype(object)\n",
    "    test_df['cluster_label'] = clusters[len(train_df):].astype(object)\n",
    "\n",
    "    for i in range(n_clusters):\n",
    "        train_df[f'dist_to_cluster_{i}'] = train_dists[:, i]\n",
    "        test_df[f'dist_to_cluster_{i}'] = test_dists[:, i]\n",
    "\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0ad84dab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-18T05:01:51.141144Z",
     "iopub.status.busy": "2025-12-18T05:01:51.140698Z",
     "iopub.status.idle": "2025-12-18T05:02:04.161347Z",
     "shell.execute_reply": "2025-12-18T05:02:04.160789Z"
    },
    "papermill": {
     "duration": 13.031746,
     "end_time": "2025-12-18T05:02:04.162629",
     "exception": false,
     "start_time": "2025-12-18T05:01:51.130883",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# add generated features\n",
    "add_generated_features(train_data)\n",
    "add_generated_features(test_data)\n",
    "\n",
    "# apply clustering\n",
    "train_data, test_data = add_kmeans_features(train_data, test_data, n_clusters=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "88182c2b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-18T05:02:04.183110Z",
     "iopub.status.busy": "2025-12-18T05:02:04.182637Z",
     "iopub.status.idle": "2025-12-18T05:02:04.187377Z",
     "shell.execute_reply": "2025-12-18T05:02:04.186841Z"
    },
    "papermill": {
     "duration": 0.015878,
     "end_time": "2025-12-18T05:02:04.188473",
     "exception": false,
     "start_time": "2025-12-18T05:02:04.172595",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['age', 'alcohol_consumption_per_week', 'physical_activity_minutes_per_week', 'diet_score', 'sleep_hours_per_day', 'screen_time_hours_per_day', 'bmi', 'waist_to_hip_ratio', 'systolic_bp', 'diastolic_bp', 'heart_rate', 'cholesterol_total', 'hdl_cholesterol', 'ldl_cholesterol', 'triglycerides', 'gender', 'ethnicity', 'employment_status', 'family_history_diabetes', 'hypertension_history', 'cardiovascular_history', 'diagnosed_diabetes', 'education_level_encoded', 'income_level_encoded', 'smoking_status_encoded', 'log_triglycerides', 'log_ldl_cholesterol', 'log_cholesterol_total', 'cholesterol_ratio', 'ldl_hdl_ratio', 'pulse_pressure', 'mean_arterial_pressure', 'age_x_bmi', 'waist_x_bmi', 'family_history_diabetes_x_log_triglycerides', 'hypertension_history_x_systolic_bp', 'activity_x_diet', 'atherogenic_index', 'non_hdl_cholesterol', 'map_x_bmi', 'lipid_accumulation_proxy', 'visceral_adiposity_proxy', 'age_sq', 'bmi_sq', 'waist_to_hip_ratio_sq', 'systolic_bp_sq', 'comorbidity_count',\n",
       "       'bmi_cat', 'bmi_relative_to_age_decade', 'systolic_bp_relative_to_age_decade', 'cholesterol_total_relative_to_age_decade', 'bmi_relative_to_hypertension_history', 'systolic_bp_relative_to_hypertension_history', 'cholesterol_total_relative_to_hypertension_history', 'cluster_label', 'dist_to_cluster_0', 'dist_to_cluster_1', 'dist_to_cluster_2', 'dist_to_cluster_3', 'dist_to_cluster_4', 'dist_to_cluster_5', 'dist_to_cluster_6'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e71b2d",
   "metadata": {
    "papermill": {
     "duration": 0.009372,
     "end_time": "2025-12-18T05:02:04.207341",
     "exception": false,
     "start_time": "2025-12-18T05:02:04.197969",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 4.4 Remaining Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a7c7c288",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-18T05:02:04.226870Z",
     "iopub.status.busy": "2025-12-18T05:02:04.226694Z",
     "iopub.status.idle": "2025-12-18T05:02:04.702472Z",
     "shell.execute_reply": "2025-12-18T05:02:04.701867Z"
    },
    "papermill": {
     "duration": 0.487152,
     "end_time": "2025-12-18T05:02:04.703810",
     "exception": false,
     "start_time": "2025-12-18T05:02:04.216658",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cat_features = train_data.drop(target_col, axis=1).select_dtypes(include='object').columns.to_list()\n",
    "if len(cat_features) > 0:\n",
    "    for col in cat_features:\n",
    "        train_data[col] = train_data[col].astype('category')\n",
    "        test_data[col] = test_data[col].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b08be71",
   "metadata": {
    "papermill": {
     "duration": 0.009304,
     "end_time": "2025-12-18T05:02:04.723072",
     "exception": false,
     "start_time": "2025-12-18T05:02:04.713768",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 5. Stacking Initial Setup\n",
    "\n",
    "We'll use stacking, an [ensemble learning](https://en.wikipedia.org/wiki/Ensemble_learning) strategy, to generate the predictions. As we'll need to gather predictions from various base models (a.k.a. level-0 models) to feed as input features to a meta model (a.k.a. level-1 model), in order to streamline the process of experimenting with different combinations of base models, some helper classes will be defined in this section. These classes can also be found [here](https://github.com/chuo-v/machine-learning-utils/blob/master/ensemble-learning/stacking/stacking_predictions_retriever.py) at one of my GitHub repositories used to organize some utilities I implemented for machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2ee07a44",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-18T05:02:04.743073Z",
     "iopub.status.busy": "2025-12-18T05:02:04.742715Z",
     "iopub.status.idle": "2025-12-18T05:02:04.768026Z",
     "shell.execute_reply": "2025-12-18T05:02:04.767331Z"
    },
    "papermill": {
     "duration": 0.036702,
     "end_time": "2025-12-18T05:02:04.769063",
     "exception": false,
     "start_time": "2025-12-18T05:02:04.732361",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class StackingEstimator:\n",
    "    \"\"\"\n",
    "    A class representing an estimator that will be used for stacking, an ensemble learning strategy.\n",
    "\n",
    "    Intended to be used in conjunction with the `StackingPredictionsRetriever` class, which helps\n",
    "    retrieve predictions for multiple instances of `StackingEstimator`; as the predictions are saved\n",
    "    in files, on subsequent requests to retrieve predictions, even as the set of estimators has been\n",
    "    modified, the `StackingPredictionsRetriever` class can determine the predictions of estimators\n",
    "    that are non-stale and available (if any) by using the `get_hash` method of the `StackingEstimator`\n",
    "    class to determine the relevance and staleness of any saved predictions.\n",
    "\n",
    "    Proper usage of this class requires one important condition to be satisfied: the predictions made\n",
    "    using the estimator are determinstic, i.e. they are exactly the same everytime the estimator is\n",
    "    run with the same inputs (`name`, `params_dict`, `feature_names`, `get_predictions`).\n",
    "    \"\"\"\n",
    "    name = \"\"\n",
    "    params_dict = {}\n",
    "    feature_names = []\n",
    "    get_predictions = lambda: None\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        name: str,\n",
    "        feature_names: [str],\n",
    "        params_dict: {},\n",
    "        get_preds: FunctionType\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializes a new instance of `StackingEstimator`.\n",
    "\n",
    "        :param name:\n",
    "            A string representing a name for the estimator. It is used for the column names of\n",
    "            the training and test predictions for each estimator, and is also used as an input\n",
    "            to calculate a hash value for the estimator. It is recommended to use a different\n",
    "            name from the names used for other estimators passed to `StackingPredictionsRetriever`.\n",
    "        :param feature_names:\n",
    "            A list of strings representing the names of the features that will be used for the\n",
    "            estimator. It will be passed as an argument to `get_preds`. Internally, it is only\n",
    "            used as an input to calculate a hash value for the estimator.\n",
    "        :param params_dict:\n",
    "            A dictionary of parameters that will be specified for the estimator. It will be\n",
    "            passed as an argument to `get_preds`. Internally, it is only used as an input\n",
    "            to calculate a hash value for the estimator.\n",
    "        :param get_preds:\n",
    "            A function for getting the predictions for the estimator. It should only take two\n",
    "            arguments: 'params_dict' and 'feature_names', and should return predictions for\n",
    "            the training and test data (in that order) as a tuple of two `pandas.Series`.\n",
    "        \"\"\"\n",
    "        # parameter check\n",
    "        if not isinstance(name, str):\n",
    "            raise ValueError(\"`name` argument should be of type `str`\")\n",
    "        if not isinstance(feature_names, list):\n",
    "            raise ValueError(f\"`feature_names` argument for estimator \\\"{name}\\\" should be of type `list`\")\n",
    "        elif not all(isinstance(feature_name, str) for feature_name in feature_names):\n",
    "            raise ValueError(f\"`feature_names` argument for estimator \\\"{name}\\\" should only contain instances of `str`\")\n",
    "        if not isinstance(params_dict, dict):\n",
    "            raise ValueError(f\"`params_dict` argument for estimator \\\"{name}\\\" should be of type `dict`\")\n",
    "        get_preds_params = inspect.signature(get_preds).parameters.values()\n",
    "        get_preds_param_names = [param.name for param in get_preds_params]\n",
    "        if len(get_preds_param_names) != 2:\n",
    "            raise ValueError(f\"`get_preds` function for estimator \\\"{name}\\\" should take two arguments\")\n",
    "        elif \"params_dict\" not in get_preds_param_names:\n",
    "            raise ValueError(f\"`get_preds` function for estimator \\\"{name}\\\" should take a \\\"params_dict\\\" argument\")\n",
    "        elif \"feature_names\" not in get_preds_param_names:\n",
    "            raise ValueError(f\"`get_preds` function for estimator \\\"{name}\\\" should take a \\\"feature_names\\\" argument\")\n",
    "\n",
    "        self.name = name\n",
    "        self.feature_names = feature_names\n",
    "        self.params_dict = params_dict\n",
    "        self.get_preds = get_preds\n",
    "\n",
    "    def get_hash_value(self):\n",
    "        \"\"\"\n",
    "        Calculates and returns a hash value for the estimator using\n",
    "        `name`, `feature_names` and `params_dict` as inputs.\n",
    "        \"\"\"\n",
    "        feature_names_str = \"_\".join(sorted(self.feature_names))\n",
    "        params_dict_str = \"_\".join(f\"{key}-{value}\" for (key, value) in sorted(self.params_dict.items()))\n",
    "        hash_input_str = \"_\".join([self.name, feature_names_str, params_dict_str])\n",
    "        md5_hash = hl.md5(hash_input_str.encode('utf-8')).hexdigest()\n",
    "        return md5_hash\n",
    "\n",
    "class StackingPredictionsRetriever:\n",
    "    \"\"\"\n",
    "    A class for streamlining stacking (an ensemble learning strategy) that saves predictions\n",
    "    from estimators to file so that when trying out different combinations of (base) estimators,\n",
    "    the predictions that are not stale can be reused, saving the time of having the estimators\n",
    "    make predictions again.\n",
    "\n",
    "    Intended to be used in conjunction with the `StackingEstimator` class. The `hash_value` of\n",
    "    `StackingEstimator` is used to determine the staleness and relevance of the predictions for\n",
    "    an estimator. The implementation for making predictions using an estimator needs to be\n",
    "    provided as a function to `get_preds` for `StackingEstimator`; when predictions need to be\n",
    "    made using an estimator, this class will call `get_preds` for the `StackingEstimator` instance.\n",
    "\n",
    "    Proper usage of this class requires one important condition to be satisfied: the predictions made\n",
    "    using the estimators are determinstic, i.e. they are exactly the same everytime a\n",
    "    `StackingEstimator` instance is run with the same inputs.\n",
    "    \"\"\"\n",
    "    estimators = []\n",
    "    working_dir_path = \"\"\n",
    "    train_preds_filename = \"\"\n",
    "    test_preds_filename = \"\"\n",
    "    preds_save_interval = 0\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        estimators: [StackingEstimator],\n",
    "        working_dir_path: str,\n",
    "        train_preds_filename: str = \"train_preds\",\n",
    "        test_preds_filename: str = \"test_preds\",\n",
    "        preds_save_interval: int = 5,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializes a new instance of `StackingPredictionsRetriever`.\n",
    "\n",
    "        :param estimators:\n",
    "            A list of `StackingEstimator` instances for which the class will retrieve predictions.\n",
    "        :param working_dir_path:\n",
    "            The path for the working directory where the files with predictions will be saved.\n",
    "        :param train_preds_filename:\n",
    "            The name of the file in which predictions for the training set will be stored.\n",
    "        :param test_preds_filename:\n",
    "            The name of the file in which predictions for the test set will be stored.\n",
    "        :param preds_save_interval:\n",
    "            An integer which specifies the interval at which predictions will be saved when\n",
    "            `get_preds` is called, corresponding to the number of estimators whose predictions\n",
    "            have been retrieved since the predictions were previously saved. Any estimators\n",
    "            whose predictions are not stale and therefore were not required to make predictions\n",
    "            again are not included in this number.\n",
    "        \"\"\"\n",
    "        # parameter check\n",
    "        if not isinstance(estimators, list):\n",
    "            raise ValueError(\"`estimators` must be passed as a list\")\n",
    "        if not all(isinstance(e, StackingEstimator) for e in estimators):\n",
    "            raise ValueError(\"`estimators` should only contain instances of `StackingEstimator`\")\n",
    "        if not isinstance(working_dir_path, str):\n",
    "            raise ValueError(\"`working_dir_path` argument should be of type `str`\")\n",
    "        if not isinstance(preds_save_interval, int):\n",
    "            raise ValueError(\"`preds_save_interval` argument should be of type `int`\")\n",
    "\n",
    "        self.estimators = estimators\n",
    "        self.working_dir_path = working_dir_path\n",
    "        self.train_preds_filename = train_preds_filename\n",
    "        self.test_preds_filename = test_preds_filename\n",
    "        self.preds_save_interval = preds_save_interval\n",
    "\n",
    "    def get_train_preds_file_path(self):\n",
    "        \"\"\"\n",
    "        Returns the file path for storing predictions for training data.\n",
    "        \"\"\"\n",
    "        return Path(f\"{self.working_dir_path}/{self.train_preds_filename}.csv\")\n",
    "\n",
    "    def get_test_preds_file_path(self):\n",
    "        \"\"\"\n",
    "        Returns the file path for storing predictions for test data.\n",
    "        \"\"\"\n",
    "        return Path(f\"{self.working_dir_path}/{self.test_preds_filename}.csv\")\n",
    "\n",
    "    def get_current_train_and_test_preds(self):\n",
    "        \"\"\"\n",
    "        Returns the current predictions for training and test data (in that order)\n",
    "        as a tuple of two `pandas.DataFrame`.\n",
    "\n",
    "        The predictions are attempted to be retrieved from the file paths returned\n",
    "        by `get_train_preds_file_path` and `get_test_preds_file_path`; if there are\n",
    "        any issues with doing so (e.g. file does not exist, dataframe is empty),\n",
    "        empty dataframes will be returned instead.\n",
    "        In the case an `pandas.errors.EmptyDataError` exception is raised when\n",
    "        reading from a file, the corresponding file will be removed.\n",
    "        \"\"\"\n",
    "        curr_train_preds = pd.DataFrame()\n",
    "        curr_test_preds = pd.DataFrame()\n",
    "        train_preds_file_path = self.get_train_preds_file_path()\n",
    "        test_preds_file_path = self.get_test_preds_file_path()\n",
    "\n",
    "        if train_preds_file_path.is_file():\n",
    "            try:\n",
    "                curr_train_preds = pd.read_csv(train_preds_file_path)\n",
    "            except pd.errors.EmptyDataError:\n",
    "                train_preds_file_path.unlink()\n",
    "        if test_preds_file_path.is_file():\n",
    "            try:\n",
    "                curr_test_preds = pd.read_csv(test_preds_file_path)\n",
    "            except pd.errors.EmptyDataError:\n",
    "                test_preds_file_path.unlink()\n",
    "\n",
    "        return curr_train_preds, curr_test_preds\n",
    "\n",
    "    def get_preds(self):\n",
    "        \"\"\"\n",
    "        Retrieves predictions from all estimators in `estimators`, storing them in\n",
    "        two files at the file paths specified by `working_dir_path`,\n",
    "        `train_preds_filename` and `test_preds_filename`.\n",
    "\n",
    "        If non-stale (relevant) predictions are found for an estimator, retrieval\n",
    "        of predictions by calling `get_preds` on the estimator will be skipped,\n",
    "        and the existing predictions for the estimator will be kept.\n",
    "        \"\"\"\n",
    "        print(\"[INFO] Getting predictions..\")\n",
    "        curr_train_preds, curr_test_preds = self.get_current_train_and_test_preds()\n",
    "\n",
    "        preds_retrieved_count = 0\n",
    "        num_preds_retrieved_but_not_yet_saved = 0\n",
    "        estimators_skipped = []\n",
    "\n",
    "        for estimator in self.estimators:\n",
    "            estimator_hash_value = estimator.get_hash_value()\n",
    "            estimator_name = f\"{estimator.name} ({estimator_hash_value})\"\n",
    "\n",
    "            # skip retrieving predictions for estimator if non-stale predictions are already available\n",
    "            train_preds_available = any(estimator_hash_value in col_name for col_name in curr_train_preds.columns)\n",
    "            test_preds_available = any(estimator_hash_value in col_name for col_name in curr_test_preds.columns)\n",
    "            if train_preds_available and test_preds_available:\n",
    "                estimators_skipped += [estimator_name]\n",
    "                continue\n",
    "\n",
    "            print(f\"[INFO] Getting predictions for estimator {estimator_name}\")\n",
    "            train_preds, test_preds = estimator.get_preds(estimator.params_dict, estimator.feature_names)\n",
    "            if not isinstance(train_preds, pd.core.series.Series):\n",
    "                raise ValueError(\"`train_preds` should be of type `pandas.Series`\")\n",
    "            if not isinstance(test_preds, pd.core.series.Series):\n",
    "                raise ValueError(\"`test_preds` should be of type `pandas.Series`\")\n",
    "            curr_train_preds[estimator_name] = train_preds\n",
    "            curr_test_preds[estimator_name] = test_preds\n",
    "            preds_retrieved_count += 1\n",
    "\n",
    "            # save predictions at an interval of `preds_save_interval`\n",
    "            if preds_retrieved_count % self.preds_save_interval == 0:\n",
    "                curr_train_preds.sort_index(axis=1).to_csv(self.get_train_preds_file_path(), index=False)\n",
    "                curr_test_preds.sort_index(axis=1).to_csv(self.get_test_preds_file_path(), index=False)\n",
    "                num_preds_retrieved_but_not_yet_saved = 0\n",
    "                print(\"[INFO] Saved predictions\")\n",
    "            else:\n",
    "                num_preds_retrieved_but_not_yet_saved += 1\n",
    "\n",
    "        if estimators_skipped:\n",
    "            estimators_skipped.sort()\n",
    "            formatted_estimators = \", \".join(estimators_skipped)\n",
    "            print(f\"[INFO] Skipped retrieving predictions for following estimators as their current ones are not stale:\\n{formatted_estimators}\")\n",
    "\n",
    "        if num_preds_retrieved_but_not_yet_saved != 0:\n",
    "            curr_train_preds.sort_index(axis=1).to_csv(self.get_train_preds_file_path(), index=False)\n",
    "            curr_test_preds.sort_index(axis=1).to_csv(self.get_test_preds_file_path(), index=False)\n",
    "            print(\"[INFO] Saved predictions\")\n",
    "\n",
    "        print(\"[INFO] Finished getting all predictions\")\n",
    "\n",
    "    def sync_preds(self):\n",
    "        \"\"\"\n",
    "        Syncs the predictions stored at the two file paths specified by\n",
    "        `working_dir_path`, `train_preds_filename` and `test_preds_filename` by\n",
    "        removing predictions for any estimator that is not currently in `estimators`.\n",
    "\n",
    "        Note that new predictions for estimators that do not currently have predictions\n",
    "        in the files will not be added; `get_preds` should be used for this purpose\n",
    "        instead.\n",
    "        \"\"\"\n",
    "        print(\"[INFO] Syncing predictions..\")\n",
    "        estimator_hash_values = [estimator.get_hash_value() for estimator in self.estimators]\n",
    "        should_remove_col = lambda col_name: not any(hash_value in col_name for hash_value in estimator_hash_values)\n",
    "\n",
    "        curr_train_preds, curr_test_preds = self.get_current_train_and_test_preds()\n",
    "\n",
    "        if not curr_train_preds.empty:\n",
    "            col_names_to_remove = [col_name for col_name in curr_train_preds.columns if should_remove_col(col_name)]\n",
    "            if col_names_to_remove:\n",
    "                print(f\"[INFO] Dropping columns for following estimators from training predictions:\\n{col_names_to_remove}\")\n",
    "                curr_train_preds.drop(columns=col_names_to_remove, inplace=True)\n",
    "                curr_train_preds.to_csv(self.get_train_preds_file_path(), index=False)\n",
    "            else:\n",
    "                print(f\"[INFO] No columns for training predictions were dropped\")\n",
    "        if not curr_test_preds.empty:\n",
    "            col_names_to_remove = [col_name for col_name in curr_test_preds.columns if should_remove_col(col_name)]\n",
    "            if col_names_to_remove:\n",
    "                print(f\"[INFO] Dropping columns for following estimators from test predictions:\\n{col_names_to_remove}\")\n",
    "                curr_test_preds.drop(columns=col_names_to_remove, inplace=True)\n",
    "                curr_test_preds.to_csv(self.get_test_preds_file_path(), index=False)\n",
    "            else:\n",
    "                print(f\"[INFO] No columns for test predictions were dropped\")\n",
    "\n",
    "        print(\"[INFO] Finished syncing predictions\")\n",
    "\n",
    "    def import_preds(self, input_dir_path):\n",
    "        \"\"\"\n",
    "        Imports predictions stored at the two file paths at `input_dir_path` with\n",
    "        `train_preds_filename` and `test_preds_filename` as their filenames. If no\n",
    "        such files are found, no predictions will be imported.\n",
    "\n",
    "        Only predictions for estimators specified in `estimators` will be imported.\n",
    "        Any predictions for estimators that were already available will be overwritten\n",
    "        with predictions for the same estimators found in the files at `input_dir_path`.\n",
    "\n",
    "        :param input_dir_path:\n",
    "            The path to the directory for the training and test predictions files.\n",
    "            The file names are expected to be the same as `train_preds_filename`\n",
    "            and `test_preds_filename`\n",
    "        \"\"\"\n",
    "        print(\"[INFO] Importing predictions..\")\n",
    "        curr_train_preds, curr_test_preds = self.get_current_train_and_test_preds()\n",
    "        input_train_preds = pd.DataFrame()\n",
    "        input_test_preds = pd.DataFrame()\n",
    "\n",
    "        input_train_preds_path = Path(f\"{input_dir_path}/{self.train_preds_filename}.csv\")\n",
    "        input_test_preds_path = Path(f\"{input_dir_path}/{self.test_preds_filename}.csv\")\n",
    "        if input_train_preds_path.is_file():\n",
    "            try:\n",
    "                input_train_preds = pd.read_csv(input_train_preds_path)\n",
    "            except: pass\n",
    "        if input_test_preds_path.is_file():\n",
    "            try:\n",
    "                input_test_preds = pd.read_csv(input_test_preds_path)\n",
    "            except: pass\n",
    "\n",
    "        estimators_with_imported_train_preds = []\n",
    "        estimators_with_imported_test_preds = []\n",
    "        for estimator in self.estimators:\n",
    "            estimator_hash_value = estimator.get_hash_value()\n",
    "            estimator_name = f\"{estimator.name} ({estimator_hash_value})\"\n",
    "            train_preds_available = any(estimator_hash_value in col_name for col_name in input_train_preds.columns)\n",
    "            test_preds_available = any(estimator_hash_value in col_name for col_name in input_test_preds.columns)\n",
    "\n",
    "            if train_preds_available:\n",
    "                curr_train_preds[estimator_name] = input_train_preds[estimator_name]\n",
    "                estimators_with_imported_train_preds += [estimator_name]\n",
    "            if test_preds_available:\n",
    "                curr_test_preds[estimator_name] = input_test_preds[estimator_name]\n",
    "                estimators_with_imported_test_preds += [estimator_name]\n",
    "\n",
    "        if not estimators_with_imported_train_preds:\n",
    "            print(\"[INFO] No train predictions were imported\")\n",
    "        else:\n",
    "            curr_train_preds.sort_index(axis=1).to_csv(self.get_train_preds_file_path(), index=False)\n",
    "            formatted_estimators = \", \".join(estimators_with_imported_train_preds)\n",
    "            print(f\"[INFO] {len(estimators_with_imported_train_preds)} train predictions were imported:\\n{formatted_estimators}\")\n",
    "        if not estimators_with_imported_test_preds:\n",
    "            print(\"[INFO] No test predictions were imported\")\n",
    "        else:\n",
    "            curr_test_preds.sort_index(axis=1).to_csv(self.get_test_preds_file_path(), index=False)\n",
    "            formatted_estimators = \", \".join(estimators_with_imported_test_preds)\n",
    "            print(f\"[INFO] {len(estimators_with_imported_test_preds)} test predictions were imported:\\n{formatted_estimators}\")\n",
    "        \n",
    "        print(\"[INFO] Finished importing predictions\")\n",
    "\n",
    "    def clear_preds(self):\n",
    "        \"\"\"\n",
    "        Removes all stored predictions by deleting the two files at filepaths specified\n",
    "        by `working_dir_path`, `train_preds_filename` and `test_preds_filename`.\n",
    "        \"\"\"\n",
    "        train_preds_file_path = self.get_train_preds_file_path()\n",
    "        test_preds_file_path = self.get_test_preds_file_path()\n",
    "\n",
    "        if train_preds_file_path.is_file():\n",
    "            train_preds_file_path.unlink()\n",
    "        if test_preds_file_path.is_file():\n",
    "            test_preds_file_path.unlink()\n",
    "\n",
    "        print(\"[INFO] Finished clearing predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef183a14",
   "metadata": {
    "papermill": {
     "duration": 0.009602,
     "end_time": "2025-12-18T05:02:04.787925",
     "exception": false,
     "start_time": "2025-12-18T05:02:04.778323",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Next, we'll simply create a variable for storing the estimators (`StackingEstimator` instances) that we'll pass to the `StackingPredictionsRetriever` class for getting all the predictions from our base models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8765d4a0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-18T05:02:04.807600Z",
     "iopub.status.busy": "2025-12-18T05:02:04.807409Z",
     "iopub.status.idle": "2025-12-18T05:02:04.810617Z",
     "shell.execute_reply": "2025-12-18T05:02:04.809917Z"
    },
    "papermill": {
     "duration": 0.014385,
     "end_time": "2025-12-18T05:02:04.811678",
     "exception": false,
     "start_time": "2025-12-18T05:02:04.797293",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "estimators = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cca22c4",
   "metadata": {
    "papermill": {
     "duration": 0.009363,
     "end_time": "2025-12-18T05:02:04.830389",
     "exception": false,
     "start_time": "2025-12-18T05:02:04.821026",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 6. Feature Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5d2671c9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-18T05:02:04.850081Z",
     "iopub.status.busy": "2025-12-18T05:02:04.849851Z",
     "iopub.status.idle": "2025-12-18T05:02:04.855125Z",
     "shell.execute_reply": "2025-12-18T05:02:04.854439Z"
    },
    "papermill": {
     "duration": 0.016611,
     "end_time": "2025-12-18T05:02:04.856259",
     "exception": false,
     "start_time": "2025-12-18T05:02:04.839648",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "FEATURE_SET_1 = [\n",
    "    'age', 'alcohol_consumption_per_week', 'physical_activity_minutes_per_week',\n",
    "    'diet_score', 'sleep_hours_per_day', 'screen_time_hours_per_day',\n",
    "    'bmi', 'waist_to_hip_ratio', 'systolic_bp',\n",
    "    'diastolic_bp', 'heart_rate', 'cholesterol_total',\n",
    "    'hdl_cholesterol', 'ldl_cholesterol', 'triglycerides',\n",
    "    'gender', 'ethnicity', 'employment_status',\n",
    "    'family_history_diabetes', 'hypertension_history', 'cardiovascular_history',\n",
    "    'education_level_encoded', 'income_level_encoded', 'smoking_status_encoded',\n",
    "    'log_triglycerides', 'log_ldl_cholesterol', 'log_cholesterol_total',\n",
    "    'cholesterol_ratio', 'ldl_hdl_ratio', 'pulse_pressure',\n",
    "    'mean_arterial_pressure', 'age_x_bmi', 'waist_x_bmi',\n",
    "    'family_history_diabetes_x_log_triglycerides', 'hypertension_history_x_systolic_bp', 'activity_x_diet',\n",
    "    'age_sq', 'bmi_sq', 'waist_to_hip_ratio_sq',\n",
    "    'systolic_bp_sq', 'comorbidity_count', 'bmi_cat',\n",
    "    'cluster_label',\n",
    "]\n",
    "\n",
    "FEATURE_SET_2 = [\n",
    "    'age', 'alcohol_consumption_per_week', 'physical_activity_minutes_per_week',\n",
    "    'diet_score', 'sleep_hours_per_day', 'screen_time_hours_per_day',\n",
    "    'bmi', 'waist_to_hip_ratio', 'systolic_bp',\n",
    "    'diastolic_bp', 'heart_rate', 'cholesterol_total',\n",
    "    'hdl_cholesterol', 'ldl_cholesterol', 'triglycerides',\n",
    "    'gender', 'ethnicity', 'employment_status',\n",
    "    'family_history_diabetes', 'hypertension_history', 'cardiovascular_history',\n",
    "    'education_level_encoded', 'income_level_encoded', 'smoking_status_encoded',\n",
    "    'log_triglycerides', 'log_ldl_cholesterol', 'log_cholesterol_total',\n",
    "    'cholesterol_ratio', 'ldl_hdl_ratio', 'pulse_pressure',\n",
    "    'mean_arterial_pressure', 'age_x_bmi', 'waist_x_bmi',\n",
    "    'family_history_diabetes_x_log_triglycerides', 'hypertension_history_x_systolic_bp', 'activity_x_diet',\n",
    "    'atherogenic_index', 'non_hdl_cholesterol', 'map_x_bmi',\n",
    "    'lipid_accumulation_proxy', 'visceral_adiposity_proxy', 'age_sq',\n",
    "    'bmi_sq', 'waist_to_hip_ratio_sq', 'systolic_bp_sq',\n",
    "    'comorbidity_count', 'bmi_cat', 'bmi_relative_to_age_decade',\n",
    "    'systolic_bp_relative_to_age_decade', 'cholesterol_total_relative_to_age_decade', 'bmi_relative_to_hypertension_history',\n",
    "    'systolic_bp_relative_to_hypertension_history', 'cholesterol_total_relative_to_hypertension_history', 'cluster_label',\n",
    "    'dist_to_cluster_0', 'dist_to_cluster_1', 'dist_to_cluster_2',\n",
    "    'dist_to_cluster_3', 'dist_to_cluster_4', 'dist_to_cluster_5',\n",
    "    'dist_to_cluster_6',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd4d674",
   "metadata": {
    "papermill": {
     "duration": 0.011633,
     "end_time": "2025-12-18T05:02:04.877419",
     "exception": false,
     "start_time": "2025-12-18T05:02:04.865786",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 7. Base Model Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ea1337c1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-18T05:02:04.897341Z",
     "iopub.status.busy": "2025-12-18T05:02:04.896892Z",
     "iopub.status.idle": "2025-12-18T05:02:04.899798Z",
     "shell.execute_reply": "2025-12-18T05:02:04.899250Z"
    },
    "papermill": {
     "duration": 0.013983,
     "end_time": "2025-12-18T05:02:04.900837",
     "exception": false,
     "start_time": "2025-12-18T05:02:04.886854",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# to skip hyperparameter tuning when it's not needed; set to `False` to do the tuning\n",
    "SKIP_BASE_MODEL_HYPERPARAMETER_TUNING = False\n",
    "\n",
    "# value set for early stopping for base models that support it; this value will be used for actual model training as well\n",
    "BASE_MODEL_EARLY_STOPPING_ROUNDS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7dfd0da7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-18T05:02:04.920817Z",
     "iopub.status.busy": "2025-12-18T05:02:04.920646Z",
     "iopub.status.idle": "2025-12-18T05:02:04.924205Z",
     "shell.execute_reply": "2025-12-18T05:02:04.923526Z"
    },
    "papermill": {
     "duration": 0.015011,
     "end_time": "2025-12-18T05:02:04.925220",
     "exception": false,
     "start_time": "2025-12-18T05:02:04.910209",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BaseModelOptunaStudyEstimator(Enum):\n",
    "    CATBOOSTCLASSIFIER = \"CatBoostClassifier\"\n",
    "    LGBMCLASSIFIER = \"LGBMClassifier\"\n",
    "    XGBCLASSIFIER = \"XGBClassifier\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a801fcab",
   "metadata": {
    "papermill": {
     "duration": 0.009609,
     "end_time": "2025-12-18T05:02:04.944135",
     "exception": false,
     "start_time": "2025-12-18T05:02:04.934526",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Manually configure the values for the following variables for different studies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0b7938c1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-18T05:02:04.963662Z",
     "iopub.status.busy": "2025-12-18T05:02:04.963459Z",
     "iopub.status.idle": "2025-12-18T05:02:04.966574Z",
     "shell.execute_reply": "2025-12-18T05:02:04.966053Z"
    },
    "papermill": {
     "duration": 0.014032,
     "end_time": "2025-12-18T05:02:04.967569",
     "exception": false,
     "start_time": "2025-12-18T05:02:04.953537",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# estimator to use for Optuna study\n",
    "BASE_MODEL_OPTUNA_STUDY_ESTIMATOR = BaseModelOptunaStudyEstimator.XGBCLASSIFIER\n",
    "\n",
    "# feature set to use for Optuna study\n",
    "BASE_MODEL_OPTUNA_STUDY_FEATURE_SET = FEATURE_SET_1\n",
    "\n",
    "# maximum number of trials Optuna will conduct for the optimization\n",
    "BASE_MODEL_OPTUNA_STUDY_NUM_TRIALS = 100\n",
    "\n",
    "# number of splits to use for Stratified K-Fold Cross-Validation for Optuna study\n",
    "BASE_MODEL_OPTUNA_STUDY_KFOLD_NUM_SPLITS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4d60fb6b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-18T05:02:04.987690Z",
     "iopub.status.busy": "2025-12-18T05:02:04.987512Z",
     "iopub.status.idle": "2025-12-18T05:02:05.002394Z",
     "shell.execute_reply": "2025-12-18T05:02:05.001716Z"
    },
    "papermill": {
     "duration": 0.026494,
     "end_time": "2025-12-18T05:02:05.003486",
     "exception": false,
     "start_time": "2025-12-18T05:02:04.976992",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_base_model_optuna_params(trial, study_estimator):\n",
    "    if study_estimator == BaseModelOptunaStudyEstimator.CATBOOSTCLASSIFIER:\n",
    "        return {\n",
    "            \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.005, 0.1, log=True),\n",
    "            \"depth\": trial.suggest_int(\"depth\", 3, 10),\n",
    "            \"l2_leaf_reg\": trial.suggest_float(\"l2_leaf_reg\", 1, 30),\n",
    "            'bagging_temperature': trial.suggest_float('bagging_temperature', 0, 30),\n",
    "            'random_strength': trial.suggest_float('random_strength', 0, 20),\n",
    "            'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 1, 20),\n",
    "        }\n",
    "    elif study_estimator == BaseModelOptunaStudyEstimator.LGBMCLASSIFIER:\n",
    "        if BASE_MODEL_OPTUNA_STUDY_FEATURE_SET == FEATURE_SET_1:\n",
    "            return {\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 0.004, 0.04),\n",
    "                'num_leaves': trial.suggest_int('num_leaves', 25, 80),\n",
    "                'min_child_samples': trial.suggest_int('min_child_samples', 100, 1000),\n",
    "                'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.05, 0.7),\n",
    "                'reg_alpha': trial.suggest_float('reg_alpha', 0.5, 15.0, log=True),\n",
    "                'reg_lambda': trial.suggest_float('reg_lambda', 1.0, 10.0, log=True),\n",
    "            }\n",
    "        elif BASE_MODEL_OPTUNA_STUDY_FEATURE_SET == FEATURE_SET_2:\n",
    "            return {\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1),\n",
    "                'num_leaves': trial.suggest_int('num_leaves', 10, 100),\n",
    "                'min_child_samples': trial.suggest_int('min_child_samples', 20, 100),\n",
    "                'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 0.95),\n",
    "                'reg_alpha': trial.suggest_float('reg_alpha', 1.0, 100.0, log=True),\n",
    "                'reg_lambda': trial.suggest_float('reg_lambda', 0.1, 10.0, log=True),\n",
    "            }\n",
    "        else:\n",
    "            raise ValueError(f\"Search space for feature set for Optuna study not yet specified for LGBMClassifier.\")\n",
    "    elif study_estimator == BaseModelOptunaStudyEstimator.XGBCLASSIFIER:\n",
    "        if BASE_MODEL_OPTUNA_STUDY_FEATURE_SET == FEATURE_SET_1:\n",
    "            return {\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 0.002, 0.02),\n",
    "                'max_depth': trial.suggest_int('max_depth', 4, 10),\n",
    "                'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.05, 0.6),\n",
    "                'alpha': trial.suggest_float('alpha', 1e-3, 20.0, log=True),\n",
    "                'gamma': trial.suggest_float('gamma', 0.1, 1.0),\n",
    "                'lambda': trial.suggest_float('lambda', 1e-3, 10.0, log=True),\n",
    "                'min_child_weight': trial.suggest_int('min_child_weight', 50, 200),\n",
    "            }\n",
    "        elif BASE_MODEL_OPTUNA_STUDY_FEATURE_SET == FEATURE_SET_2:\n",
    "            return {\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.05, log=True),\n",
    "                'max_depth': trial.suggest_int('max_depth', 3, 6),\n",
    "                'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.7, 1.0),\n",
    "                'alpha': trial.suggest_float('alpha', 0.1, 50.0, log=True),\n",
    "                'gamma': trial.suggest_float('gamma', 0.0, 0.5),\n",
    "                'lambda': trial.suggest_float('lambda', 1.0, 10.0, log=True),\n",
    "                'min_child_weight': trial.suggest_int('min_child_weight', 10, 50),\n",
    "            }\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported optuna study estimator\")\n",
    "\n",
    "def get_base_model_predictions(study_estimator, trial_params, X_train_fold, y_train_fold, X_validation_fold, y_validation_fold):\n",
    "    if study_estimator == BaseModelOptunaStudyEstimator.CATBOOSTCLASSIFIER:\n",
    "        model = CatBoostClassifier(\n",
    "            **trial_params,\n",
    "            iterations=30000,\n",
    "            use_best_model=True,\n",
    "            cat_features=cat_features,\n",
    "            loss_function='Logloss',\n",
    "            eval_metric='AUC',\n",
    "            task_type='GPU' if torch.cuda.is_available() else 'CPU',\n",
    "            devices='0',\n",
    "            metric_period=1000,\n",
    "            random_seed=RANDOM_SEEDS[0],\n",
    "            verbose=False,\n",
    "            allow_writing_files=False\n",
    "        )\n",
    "        model.fit(\n",
    "            X_train_fold, y_train_fold,\n",
    "            eval_set=(X_validation_fold, y_validation_fold),\n",
    "            early_stopping_rounds=BASE_MODEL_EARLY_STOPPING_ROUNDS\n",
    "        )\n",
    "        return model.predict_proba(X_validation_fold)[:, 1]\n",
    "    elif study_estimator == BaseModelOptunaStudyEstimator.LGBMCLASSIFIER:\n",
    "        model = lgb.LGBMClassifier(\n",
    "            **trial_params,\n",
    "            n_estimators=30000,\n",
    "            objective='binary',\n",
    "            metric='auc',\n",
    "            verbose=-1,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        model.fit(\n",
    "            X_train_fold, y_train_fold,\n",
    "            eval_set=(X_validation_fold, y_validation_fold),\n",
    "            callbacks=[lgb.early_stopping(stopping_rounds=BASE_MODEL_EARLY_STOPPING_ROUNDS, verbose=0)]\n",
    "        )\n",
    "        return model.predict_proba(X_validation_fold)[:, 1]\n",
    "    elif study_estimator == BaseModelOptunaStudyEstimator.XGBCLASSIFIER:\n",
    "        model = XGBClassifier(\n",
    "            **trial_params,\n",
    "            n_estimators=30000,\n",
    "            tree_method='hist' if torch.cuda.is_available() else 'auto',\n",
    "            device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "            enable_categorical=True,\n",
    "            objective='binary:logistic',\n",
    "            eval_metric='auc',\n",
    "            early_stopping_rounds=BASE_MODEL_EARLY_STOPPING_ROUNDS,\n",
    "            n_jobs=-1,\n",
    "            random_state=RANDOM_SEEDS[0],\n",
    "            verbosity=0\n",
    "        )\n",
    "        model.fit(\n",
    "            X_train_fold, y_train_fold,\n",
    "            eval_set=[(X_validation_fold, y_validation_fold)],\n",
    "            verbose=False\n",
    "        )\n",
    "        return model.predict_proba(X_validation_fold)[:, 1]\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported optuna study estimator\")\n",
    "\n",
    "def base_model_optuna_study_objective(trial):\n",
    "    base_model_params = get_base_model_optuna_params(trial, BASE_MODEL_OPTUNA_STUDY_ESTIMATOR)\n",
    "\n",
    "    X_train = train_data[BASE_MODEL_OPTUNA_STUDY_FEATURE_SET]\n",
    "    y_train = train_data[target_col]\n",
    "\n",
    "    base_model_optuna_study_skf = StratifiedKFold(n_splits=BASE_MODEL_OPTUNA_STUDY_KFOLD_NUM_SPLITS, shuffle=True, random_state=RANDOM_SEEDS[0])\n",
    "    base_model_optuna_study_skf_splits = base_model_optuna_study_skf.split(X_train, y_train)\n",
    "    base_model_optuna_study_skf_enumeration = enumerate(base_model_optuna_study_skf_splits)\n",
    "\n",
    "    total_roc_auc = 0\n",
    "\n",
    "    for fold, (train_indices, validation_indices) in base_model_optuna_study_skf_enumeration:\n",
    "        X_train_fold = X_train.iloc[train_indices]\n",
    "        X_validation_fold = X_train.iloc[validation_indices]\n",
    "        y_train_fold = y_train.iloc[train_indices]\n",
    "        y_validation_fold = y_train.iloc[validation_indices]\n",
    "\n",
    "        y_validation_pred_proba = get_base_model_predictions(\n",
    "            BASE_MODEL_OPTUNA_STUDY_ESTIMATOR,\n",
    "            base_model_params,\n",
    "            X_train_fold, y_train_fold,\n",
    "            X_validation_fold, y_validation_fold\n",
    "        )\n",
    "        roc_auc_fold = roc_auc_score(y_validation_fold, y_validation_pred_proba)\n",
    "        total_roc_auc += roc_auc_fold\n",
    "\n",
    "        trial.report(roc_auc_fold, step=fold)\n",
    "        if trial.should_prune():\n",
    "            raise optuna.TrialPruned()\n",
    "\n",
    "    average_roc_auc = total_roc_auc / BASE_MODEL_OPTUNA_STUDY_KFOLD_NUM_SPLITS\n",
    "    return average_roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "463fb958",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-18T05:02:05.023203Z",
     "iopub.status.busy": "2025-12-18T05:02:05.022741Z",
     "iopub.status.idle": "2025-12-18T13:04:57.158321Z",
     "shell.execute_reply": "2025-12-18T13:04:57.157576Z"
    },
    "papermill": {
     "duration": 28972.160716,
     "end_time": "2025-12-18T13:04:57.173490",
     "exception": false,
     "start_time": "2025-12-18T05:02:05.012774",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/optuna/_experimental.py:32: ExperimentalWarning: Argument ``multivariate`` is an experimental feature. The interface can change in the future.\n",
      "  warnings.warn(\n",
      "[I 2025-12-18 05:02:05,025] A new study created in memory with name: no-name-cbe5d5e5-3f3b-4fe6-8108-a9b0fc63388d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started base model hyperparameter tuning for XGBClassifier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-18 05:05:16,303] Trial 0 finished with value: 0.726469465401638 and parameters: {'learning_rate': 0.017513121705893635, 'max_depth': 7, 'subsample': 0.7813921853049104, 'colsample_bytree': 0.3895203640508679, 'alpha': 0.2981821337990834, 'gamma': 0.22090788831277716, 'lambda': 7.088329751497953, 'min_child_weight': 130}. Best is trial 0 with value: 0.726469465401638.\n",
      "[I 2025-12-18 05:15:23,398] Trial 1 finished with value: 0.7261269080943916 and parameters: {'learning_rate': 0.005780052702528054, 'max_depth': 5, 'subsample': 0.6058735092514047, 'colsample_bytree': 0.5025790007263533, 'alpha': 1.617322225139358, 'gamma': 0.12400362414325146, 'lambda': 1.6327783915889695, 'min_child_weight': 184}. Best is trial 0 with value: 0.726469465401638.\n",
      "[I 2025-12-18 05:17:53,966] Trial 2 finished with value: 0.7265668611229827 and parameters: {'learning_rate': 0.018267969094363137, 'max_depth': 10, 'subsample': 0.6918137303303191, 'colsample_bytree': 0.17107839508640071, 'alpha': 2.793496376269867, 'gamma': 0.8210297704105332, 'lambda': 0.0010284934964233004, 'min_child_weight': 168}. Best is trial 2 with value: 0.7265668611229827.\n",
      "[I 2025-12-18 05:20:20,532] Trial 3 finished with value: 0.726177831070621 and parameters: {'learning_rate': 0.01952345703384379, 'max_depth': 10, 'subsample': 0.7103594917664878, 'colsample_bytree': 0.3149712814008872, 'alpha': 0.771357665617766, 'gamma': 0.95710020692935, 'lambda': 0.0012957173366443604, 'min_child_weight': 95}. Best is trial 2 with value: 0.7265668611229827.\n",
      "[I 2025-12-18 05:24:49,265] Trial 4 finished with value: 0.7264086893760598 and parameters: {'learning_rate': 0.009225074295110988, 'max_depth': 8, 'subsample': 0.7865186452499526, 'colsample_bytree': 0.37860625412733595, 'alpha': 0.11444989252052812, 'gamma': 0.541810650662391, 'lambda': 0.13550560176876658, 'min_child_weight': 64}. Best is trial 2 with value: 0.7265668611229827.\n",
      "[I 2025-12-18 05:26:29,279] Trial 5 pruned. \n",
      "[I 2025-12-18 05:41:35,392] Trial 6 pruned. \n",
      "[I 2025-12-18 05:43:28,071] Trial 7 pruned. \n",
      "[I 2025-12-18 05:45:36,185] Trial 8 pruned. \n",
      "[I 2025-12-18 05:46:51,121] Trial 9 pruned. \n",
      "[I 2025-12-18 05:49:54,077] Trial 10 finished with value: 0.7269128175828138 and parameters: {'learning_rate': 0.01610930059816441, 'max_depth': 10, 'subsample': 0.7718827488389586, 'colsample_bytree': 0.1250630687891849, 'alpha': 0.9196461939179202, 'gamma': 0.44886646421546805, 'lambda': 0.0010672212929638747, 'min_child_weight': 163}. Best is trial 10 with value: 0.7269128175828138.\n",
      "[I 2025-12-18 05:53:00,638] Trial 11 finished with value: 0.7268067919767703 and parameters: {'learning_rate': 0.01642098069981444, 'max_depth': 9, 'subsample': 0.765183105033339, 'colsample_bytree': 0.1613793058103612, 'alpha': 2.114287989936129, 'gamma': 0.7487108015281395, 'lambda': 0.002102158064040601, 'min_child_weight': 185}. Best is trial 10 with value: 0.7269128175828138.\n",
      "[I 2025-12-18 05:53:08,659] Trial 12 pruned. \n",
      "[I 2025-12-18 05:54:07,785] Trial 13 pruned. \n",
      "[I 2025-12-18 05:58:05,158] Trial 14 finished with value: 0.7265917074382834 and parameters: {'learning_rate': 0.011667874675617848, 'max_depth': 10, 'subsample': 0.8373270341474333, 'colsample_bytree': 0.29721261551433376, 'alpha': 0.07008217818728849, 'gamma': 0.8034363764118199, 'lambda': 0.0014571736377169055, 'min_child_weight': 178}. Best is trial 10 with value: 0.7269128175828138.\n",
      "[I 2025-12-18 06:00:02,231] Trial 15 pruned. \n",
      "[I 2025-12-18 06:03:33,734] Trial 16 finished with value: 0.7266183007998297 and parameters: {'learning_rate': 0.015598404205599634, 'max_depth': 7, 'subsample': 0.7955253089012312, 'colsample_bytree': 0.29217837480738484, 'alpha': 2.0027681476025245, 'gamma': 0.558515307306207, 'lambda': 0.0012542261231483056, 'min_child_weight': 193}. Best is trial 10 with value: 0.7269128175828138.\n",
      "[I 2025-12-18 06:09:43,043] Trial 17 finished with value: 0.7270233567438554 and parameters: {'learning_rate': 0.011606368295998495, 'max_depth': 7, 'subsample': 0.865293747090882, 'colsample_bytree': 0.1260771614289064, 'alpha': 3.4538904015472442, 'gamma': 0.9286270082539638, 'lambda': 0.0043707887989424795, 'min_child_weight': 192}. Best is trial 17 with value: 0.7270233567438554.\n",
      "[I 2025-12-18 06:16:38,656] Trial 18 finished with value: 0.7268217562665429 and parameters: {'learning_rate': 0.012234962573751151, 'max_depth': 6, 'subsample': 0.9706380045833911, 'colsample_bytree': 0.09138493258834736, 'alpha': 8.83698156511239, 'gamma': 0.9977511361325353, 'lambda': 0.019573713202687887, 'min_child_weight': 184}. Best is trial 17 with value: 0.7270233567438554.\n",
      "[I 2025-12-18 06:28:02,710] Trial 19 finished with value: 0.7273138271672233 and parameters: {'learning_rate': 0.004829339226078259, 'max_depth': 10, 'subsample': 0.9180877359507682, 'colsample_bytree': 0.09338509266808802, 'alpha': 2.490519201678975, 'gamma': 0.9976759239748936, 'lambda': 0.03046468518976221, 'min_child_weight': 186}. Best is trial 19 with value: 0.7273138271672233.\n",
      "[I 2025-12-18 06:28:10,267] Trial 20 pruned. \n",
      "[I 2025-12-18 06:28:17,683] Trial 21 pruned. \n",
      "[I 2025-12-18 06:28:24,349] Trial 22 pruned. \n",
      "[I 2025-12-18 06:31:19,155] Trial 23 pruned. \n",
      "[I 2025-12-18 06:36:08,160] Trial 24 finished with value: 0.7268132999693627 and parameters: {'learning_rate': 0.011455299642819976, 'max_depth': 7, 'subsample': 0.7907863264857864, 'colsample_bytree': 0.22891408669701527, 'alpha': 0.84678361714508, 'gamma': 0.9942373205054165, 'lambda': 0.008122166758252263, 'min_child_weight': 175}. Best is trial 19 with value: 0.7273138271672233.\n",
      "[I 2025-12-18 06:39:45,476] Trial 25 pruned. \n",
      "[I 2025-12-18 06:43:00,104] Trial 26 finished with value: 0.7269277008268498 and parameters: {'learning_rate': 0.017000822687063866, 'max_depth': 10, 'subsample': 0.8514527516056101, 'colsample_bytree': 0.14641931531301222, 'alpha': 0.0055322149455574975, 'gamma': 0.316246889174824, 'lambda': 0.0025036098590149325, 'min_child_weight': 185}. Best is trial 19 with value: 0.7273138271672233.\n",
      "[I 2025-12-18 06:45:37,230] Trial 27 finished with value: 0.7268941275190425 and parameters: {'learning_rate': 0.0199960492446218, 'max_depth': 9, 'subsample': 0.8872444366101375, 'colsample_bytree': 0.1372981780689159, 'alpha': 0.0010434273509140539, 'gamma': 0.2150423395137856, 'lambda': 0.0028012832695217473, 'min_child_weight': 197}. Best is trial 19 with value: 0.7273138271672233.\n",
      "[I 2025-12-18 06:46:33,245] Trial 28 pruned. \n",
      "[I 2025-12-18 06:49:18,506] Trial 29 finished with value: 0.7268699110066009 and parameters: {'learning_rate': 0.018116091624259235, 'max_depth': 10, 'subsample': 0.8861702565413514, 'colsample_bytree': 0.20761439920799618, 'alpha': 0.0014027367929174887, 'gamma': 0.581664665271298, 'lambda': 0.0011800115538294173, 'min_child_weight': 129}. Best is trial 19 with value: 0.7273138271672233.\n",
      "[I 2025-12-18 06:55:47,791] Trial 30 finished with value: 0.7270710028244451 and parameters: {'learning_rate': 0.010093407043811526, 'max_depth': 7, 'subsample': 0.9552158789987828, 'colsample_bytree': 0.15343779342071703, 'alpha': 0.026760035144200906, 'gamma': 0.28615149100212406, 'lambda': 0.0017015051821302092, 'min_child_weight': 178}. Best is trial 19 with value: 0.7273138271672233.\n",
      "[I 2025-12-18 07:03:41,731] Trial 31 finished with value: 0.7269677821163502 and parameters: {'learning_rate': 0.008022540648081512, 'max_depth': 7, 'subsample': 0.9126908486701798, 'colsample_bytree': 0.1363564793188437, 'alpha': 0.04198322963696837, 'gamma': 0.4403997694241535, 'lambda': 0.007702659368754814, 'min_child_weight': 195}. Best is trial 19 with value: 0.7273138271672233.\n",
      "[I 2025-12-18 07:03:49,394] Trial 32 pruned. \n",
      "[I 2025-12-18 07:12:35,721] Trial 33 finished with value: 0.7271543552915468 and parameters: {'learning_rate': 0.011051271105415967, 'max_depth': 5, 'subsample': 0.9805441889389717, 'colsample_bytree': 0.13734382670809628, 'alpha': 0.004137216102410966, 'gamma': 0.34251710818144776, 'lambda': 0.0014131304938386467, 'min_child_weight': 126}. Best is trial 19 with value: 0.7273138271672233.\n",
      "[I 2025-12-18 07:22:21,033] Trial 34 finished with value: 0.727142738012474 and parameters: {'learning_rate': 0.009451144370979556, 'max_depth': 4, 'subsample': 0.9660846365129598, 'colsample_bytree': 0.1773590290665463, 'alpha': 0.013533602745043576, 'gamma': 0.3220904520728558, 'lambda': 0.0011169945940491164, 'min_child_weight': 98}. Best is trial 19 with value: 0.7273138271672233.\n",
      "[I 2025-12-18 07:30:32,922] Trial 35 finished with value: 0.7270553069702821 and parameters: {'learning_rate': 0.010557073216554146, 'max_depth': 4, 'subsample': 0.920287857361883, 'colsample_bytree': 0.2246223428908217, 'alpha': 0.030577223401492288, 'gamma': 0.27677397967480416, 'lambda': 0.0014045382740687094, 'min_child_weight': 104}. Best is trial 19 with value: 0.7273138271672233.\n",
      "[I 2025-12-18 07:33:00,511] Trial 36 pruned. \n",
      "[I 2025-12-18 07:46:59,176] Trial 37 finished with value: 0.7270469733554208 and parameters: {'learning_rate': 0.00915035050025416, 'max_depth': 4, 'subsample': 0.9802308832294747, 'colsample_bytree': 0.10472682357152138, 'alpha': 0.010949569240989946, 'gamma': 0.5955792863407494, 'lambda': 0.024812369864689272, 'min_child_weight': 151}. Best is trial 19 with value: 0.7273138271672233.\n",
      "[I 2025-12-18 07:47:08,119] Trial 38 pruned. \n",
      "[I 2025-12-18 07:50:28,006] Trial 39 pruned. \n",
      "[I 2025-12-18 07:56:11,908] Trial 40 finished with value: 0.7270671458267705 and parameters: {'learning_rate': 0.013584909635777536, 'max_depth': 5, 'subsample': 0.9029932298270361, 'colsample_bytree': 0.23566981494632733, 'alpha': 0.0032442909971063067, 'gamma': 0.1417742328125151, 'lambda': 0.0011466631293233125, 'min_child_weight': 198}. Best is trial 19 with value: 0.7273138271672233.\n",
      "[I 2025-12-18 07:57:50,325] Trial 41 pruned. \n",
      "[I 2025-12-18 08:01:46,892] Trial 42 pruned. \n",
      "[I 2025-12-18 08:01:51,867] Trial 43 pruned. \n",
      "[I 2025-12-18 08:09:05,301] Trial 44 finished with value: 0.727103216337241 and parameters: {'learning_rate': 0.006588648828432912, 'max_depth': 9, 'subsample': 0.9925474225134373, 'colsample_bytree': 0.15409115715725652, 'alpha': 4.50710887216596, 'gamma': 0.9332232957785422, 'lambda': 0.02723483186180632, 'min_child_weight': 163}. Best is trial 19 with value: 0.7273138271672233.\n",
      "[I 2025-12-18 08:10:28,770] Trial 45 pruned. \n",
      "[I 2025-12-18 08:13:57,974] Trial 46 pruned. \n",
      "[I 2025-12-18 08:14:02,470] Trial 47 pruned. \n",
      "[I 2025-12-18 08:14:09,296] Trial 48 pruned. \n",
      "[I 2025-12-18 08:15:45,862] Trial 49 pruned. \n",
      "[I 2025-12-18 08:15:53,610] Trial 50 pruned. \n",
      "[I 2025-12-18 08:17:32,141] Trial 51 pruned. \n",
      "[I 2025-12-18 08:17:37,805] Trial 52 pruned. \n",
      "[I 2025-12-18 08:22:43,594] Trial 53 finished with value: 0.7271525519310984 and parameters: {'learning_rate': 0.017938555321277333, 'max_depth': 4, 'subsample': 0.96733349237106, 'colsample_bytree': 0.25400692857684853, 'alpha': 0.018472375104364884, 'gamma': 0.18516898711793298, 'lambda': 0.002104031141345454, 'min_child_weight': 159}. Best is trial 19 with value: 0.7273138271672233.\n",
      "[I 2025-12-18 08:24:04,960] Trial 54 pruned. \n",
      "[I 2025-12-18 08:27:31,073] Trial 55 finished with value: 0.7270804015365 and parameters: {'learning_rate': 0.016215773536992086, 'max_depth': 7, 'subsample': 0.9770441969163745, 'colsample_bytree': 0.2028449023876886, 'alpha': 0.05835925281248451, 'gamma': 0.2590811064647622, 'lambda': 0.0025656164827570793, 'min_child_weight': 140}. Best is trial 19 with value: 0.7273138271672233.\n",
      "[I 2025-12-18 08:28:47,329] Trial 56 pruned. \n",
      "[I 2025-12-18 08:33:40,576] Trial 57 finished with value: 0.7271010311418012 and parameters: {'learning_rate': 0.01987250594468485, 'max_depth': 4, 'subsample': 0.9577002707078125, 'colsample_bytree': 0.18331725731381965, 'alpha': 0.039647232265071074, 'gamma': 0.22401575989997136, 'lambda': 0.0010544496833610622, 'min_child_weight': 129}. Best is trial 19 with value: 0.7273138271672233.\n",
      "[I 2025-12-18 08:34:53,019] Trial 58 pruned. \n",
      "[I 2025-12-18 08:40:34,636] Trial 59 finished with value: 0.7272067057760805 and parameters: {'learning_rate': 0.018213999555157716, 'max_depth': 4, 'subsample': 0.9845703977116733, 'colsample_bytree': 0.16776992313927575, 'alpha': 0.025944579125161283, 'gamma': 0.3113731003459015, 'lambda': 0.001987499846227266, 'min_child_weight': 148}. Best is trial 19 with value: 0.7273138271672233.\n",
      "[I 2025-12-18 08:45:30,556] Trial 60 finished with value: 0.727091142594051 and parameters: {'learning_rate': 0.016872118791230316, 'max_depth': 4, 'subsample': 0.9844861355698669, 'colsample_bytree': 0.2566371171314419, 'alpha': 0.06072396288405693, 'gamma': 0.3659573417776989, 'lambda': 0.03551575833572986, 'min_child_weight': 163}. Best is trial 19 with value: 0.7273138271672233.\n",
      "[I 2025-12-18 08:47:18,325] Trial 61 pruned. \n",
      "[I 2025-12-18 08:48:56,744] Trial 62 pruned. \n",
      "[I 2025-12-18 08:50:28,605] Trial 63 pruned. \n",
      "[I 2025-12-18 08:51:59,322] Trial 64 pruned. \n",
      "[I 2025-12-18 08:53:59,053] Trial 65 pruned. \n",
      "[I 2025-12-18 08:57:13,309] Trial 66 pruned. \n",
      "[I 2025-12-18 09:08:42,998] Trial 67 finished with value: 0.7272400250600676 and parameters: {'learning_rate': 0.008194780139832073, 'max_depth': 4, 'subsample': 0.9935852513951617, 'colsample_bytree': 0.19388659583697923, 'alpha': 0.012024890856514786, 'gamma': 0.15490534604096387, 'lambda': 0.0042214687866083764, 'min_child_weight': 78}. Best is trial 19 with value: 0.7273138271672233.\n",
      "[I 2025-12-18 09:15:59,637] Trial 68 pruned. \n",
      "[I 2025-12-18 09:18:51,164] Trial 69 pruned. \n",
      "[I 2025-12-18 09:24:04,274] Trial 70 pruned. \n",
      "[I 2025-12-18 09:39:54,655] Trial 71 finished with value: 0.7271303622924506 and parameters: {'learning_rate': 0.005799749105533087, 'max_depth': 4, 'subsample': 0.9083268597738682, 'colsample_bytree': 0.27955770716470135, 'alpha': 0.0027839240995111997, 'gamma': 0.3394569410630792, 'lambda': 0.004342744111885273, 'min_child_weight': 121}. Best is trial 19 with value: 0.7273138271672233.\n",
      "[I 2025-12-18 09:47:25,488] Trial 72 pruned. \n",
      "[I 2025-12-18 10:01:17,918] Trial 73 finished with value: 0.7271015366651171 and parameters: {'learning_rate': 0.006387846064762351, 'max_depth': 4, 'subsample': 0.9415585570936225, 'colsample_bytree': 0.3625206630566554, 'alpha': 0.014793321684545472, 'gamma': 0.4435633841036908, 'lambda': 0.0013406799307655562, 'min_child_weight': 125}. Best is trial 19 with value: 0.7273138271672233.\n",
      "[I 2025-12-18 10:12:57,897] Trial 74 finished with value: 0.7272017826924411 and parameters: {'learning_rate': 0.008723887316030954, 'max_depth': 4, 'subsample': 0.9857556727526053, 'colsample_bytree': 0.17352511655126107, 'alpha': 0.005136991631839401, 'gamma': 0.2408039592240421, 'lambda': 0.013568135240960432, 'min_child_weight': 119}. Best is trial 19 with value: 0.7273138271672233.\n",
      "[I 2025-12-18 10:24:08,998] Trial 75 finished with value: 0.7271666780080307 and parameters: {'learning_rate': 0.007856214995090868, 'max_depth': 4, 'subsample': 0.9957035655685136, 'colsample_bytree': 0.277689427006099, 'alpha': 0.002033250290520477, 'gamma': 0.11319339512943813, 'lambda': 0.0027485410190826736, 'min_child_weight': 128}. Best is trial 19 with value: 0.7273138271672233.\n",
      "[I 2025-12-18 10:26:38,375] Trial 76 pruned. \n",
      "[I 2025-12-18 10:42:05,572] Trial 77 finished with value: 0.7272012709267068 and parameters: {'learning_rate': 0.006417746510653317, 'max_depth': 4, 'subsample': 0.980538483644372, 'colsample_bytree': 0.17333106357456438, 'alpha': 0.003585729625265178, 'gamma': 0.17651798904121305, 'lambda': 0.0054027594549133666, 'min_child_weight': 110}. Best is trial 19 with value: 0.7273138271672233.\n",
      "[I 2025-12-18 10:55:19,888] Trial 78 finished with value: 0.7271832360745334 and parameters: {'learning_rate': 0.006386172063180294, 'max_depth': 4, 'subsample': 0.9836992783088729, 'colsample_bytree': 0.28700413717235573, 'alpha': 0.0026003016364698666, 'gamma': 0.15070345633961735, 'lambda': 0.006143509855222345, 'min_child_weight': 109}. Best is trial 19 with value: 0.7273138271672233.\n",
      "[I 2025-12-18 11:04:38,174] Trial 79 finished with value: 0.7271517401148744 and parameters: {'learning_rate': 0.01003634728014615, 'max_depth': 4, 'subsample': 0.9945900438040329, 'colsample_bytree': 0.1804016378854268, 'alpha': 0.03584808010012044, 'gamma': 0.10623655385176495, 'lambda': 0.27772748048566265, 'min_child_weight': 119}. Best is trial 19 with value: 0.7273138271672233.\n",
      "[I 2025-12-18 11:10:34,148] Trial 80 pruned. \n",
      "[I 2025-12-18 11:14:17,799] Trial 81 pruned. \n",
      "[I 2025-12-18 11:17:40,495] Trial 82 pruned. \n",
      "[I 2025-12-18 11:21:43,522] Trial 83 pruned. \n",
      "[I 2025-12-18 11:34:18,580] Trial 84 finished with value: 0.7272386721393476 and parameters: {'learning_rate': 0.006904737040460974, 'max_depth': 4, 'subsample': 0.9922124321624518, 'colsample_bytree': 0.2204135110662262, 'alpha': 0.0036424088851005646, 'gamma': 0.22490965702409924, 'lambda': 0.004113630439838606, 'min_child_weight': 104}. Best is trial 19 with value: 0.7273138271672233.\n",
      "[I 2025-12-18 11:44:13,779] Trial 85 finished with value: 0.727132316711249 and parameters: {'learning_rate': 0.008798823790671935, 'max_depth': 4, 'subsample': 0.9486602819278938, 'colsample_bytree': 0.2588572110510472, 'alpha': 0.002182726259566085, 'gamma': 0.15261327508079015, 'lambda': 0.0014323362704595974, 'min_child_weight': 90}. Best is trial 19 with value: 0.7273138271672233.\n",
      "[I 2025-12-18 11:51:27,043] Trial 86 pruned. \n",
      "[I 2025-12-18 11:58:07,489] Trial 87 finished with value: 0.7271638780846644 and parameters: {'learning_rate': 0.0081063602239408, 'max_depth': 10, 'subsample': 0.8751610211471222, 'colsample_bytree': 0.1390515311719082, 'alpha': 3.767728892104124, 'gamma': 0.9406436013967637, 'lambda': 0.1256816197948056, 'min_child_weight': 191}. Best is trial 19 with value: 0.7273138271672233.\n",
      "[I 2025-12-18 11:59:34,868] Trial 88 pruned. \n",
      "[I 2025-12-18 12:01:23,248] Trial 89 pruned. \n",
      "[I 2025-12-18 12:01:28,885] Trial 90 pruned. \n",
      "[I 2025-12-18 12:07:25,837] Trial 91 pruned. \n",
      "[I 2025-12-18 12:09:29,759] Trial 92 pruned. \n",
      "[I 2025-12-18 12:11:53,424] Trial 93 pruned. \n",
      "[I 2025-12-18 12:16:03,528] Trial 94 pruned. \n",
      "[I 2025-12-18 12:18:23,430] Trial 95 pruned. \n",
      "[I 2025-12-18 12:22:10,301] Trial 96 pruned. \n",
      "[I 2025-12-18 12:32:13,618] Trial 97 finished with value: 0.7272713043110408 and parameters: {'learning_rate': 0.009529308126434627, 'max_depth': 4, 'subsample': 0.9760712008549881, 'colsample_bytree': 0.20030064697051828, 'alpha': 0.0029192827174491526, 'gamma': 0.3772961680582061, 'lambda': 0.00904754506537308, 'min_child_weight': 112}. Best is trial 19 with value: 0.7273138271672233.\n",
      "[I 2025-12-18 12:47:21,763] Trial 98 finished with value: 0.7272394991915504 and parameters: {'learning_rate': 0.005704847892694589, 'max_depth': 4, 'subsample': 0.9849703220547251, 'colsample_bytree': 0.23529764827824312, 'alpha': 0.002148634610557025, 'gamma': 0.17113211764441308, 'lambda': 0.0018329740303084188, 'min_child_weight': 123}. Best is trial 19 with value: 0.7273138271672233.\n",
      "[I 2025-12-18 13:04:57,145] Trial 99 finished with value: 0.7271472070913013 and parameters: {'learning_rate': 0.004904391329455796, 'max_depth': 4, 'subsample': 0.9785396063762473, 'colsample_bytree': 0.23561720576709488, 'alpha': 0.003921593513563104, 'gamma': 0.16827448164718797, 'lambda': 0.003023730836123198, 'min_child_weight': 157}. Best is trial 19 with value: 0.7273138271672233.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# trials finished: 100\n",
      "Best trial AUC: 0.7273138271672233\n",
      "Best trial params:\n",
      "- learning_rate: 0.004829339226078259\n",
      "- max_depth: 10\n",
      "- subsample: 0.9180877359507682\n",
      "- colsample_bytree: 0.09338509266808802\n",
      "- alpha: 2.490519201678975\n",
      "- gamma: 0.9976759239748936\n",
      "- lambda: 0.03046468518976221\n",
      "- min_child_weight: 186\n"
     ]
    }
   ],
   "source": [
    "if SKIP_BASE_MODEL_HYPERPARAMETER_TUNING:\n",
    "    print(\"Skipped base model hyperparameter tuning\")\n",
    "else:\n",
    "    print(f\"Started base model hyperparameter tuning for {BASE_MODEL_OPTUNA_STUDY_ESTIMATOR.value}\")\n",
    "    sampler = optuna.samplers.TPESampler(n_ei_candidates=48, multivariate=True)\n",
    "    study = optuna.create_study(sampler=sampler, direction=\"maximize\")\n",
    "    study.optimize(base_model_optuna_study_objective, n_trials=BASE_MODEL_OPTUNA_STUDY_NUM_TRIALS)\n",
    "    \n",
    "    print(f\"# trials finished: {len(study.trials)}\")\n",
    "    trial = study.best_trial\n",
    "    print(f\"Best trial AUC: {trial.value}\")\n",
    "    print(f\"Best trial params:\")\n",
    "    for param_key, param_value in trial.params.items():\n",
    "        print(f\"- {param_key}: {param_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6b8e21",
   "metadata": {
    "papermill": {
     "duration": 0.013645,
     "end_time": "2025-12-18T13:04:57.200575",
     "exception": false,
     "start_time": "2025-12-18T13:04:57.186930",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 8. Base Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4bdd446a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-18T13:04:57.228186Z",
     "iopub.status.busy": "2025-12-18T13:04:57.227755Z",
     "iopub.status.idle": "2025-12-18T13:04:57.231207Z",
     "shell.execute_reply": "2025-12-18T13:04:57.230540Z"
    },
    "papermill": {
     "duration": 0.018741,
     "end_time": "2025-12-18T13:04:57.232302",
     "exception": false,
     "start_time": "2025-12-18T13:04:57.213561",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# number of splits to use for Stratified K-Fold Cross-Validation for base models\n",
    "BASE_MODEL_KFOLD_NUM_SPLITS = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a129311",
   "metadata": {
    "papermill": {
     "duration": 0.013051,
     "end_time": "2025-12-18T13:04:57.258425",
     "exception": false,
     "start_time": "2025-12-18T13:04:57.245374",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 8.1 CatBoostClassifier\n",
    "\n",
    "### 8.1.1 Helper Methods (CatBoostClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3df8df29",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-18T13:04:57.374142Z",
     "iopub.status.busy": "2025-12-18T13:04:57.373862Z",
     "iopub.status.idle": "2025-12-18T13:04:57.381125Z",
     "shell.execute_reply": "2025-12-18T13:04:57.380587Z"
    },
    "papermill": {
     "duration": 0.110763,
     "end_time": "2025-12-18T13:04:57.382200",
     "exception": false,
     "start_time": "2025-12-18T13:04:57.271437",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_catboostclassifier_preds(params_dict, feature_names):\n",
    "    oof_preds_accumulator = np.zeros(len(train_data))\n",
    "    test_preds_accumulator = np.zeros(len(test_data))\n",
    "\n",
    "    X_train = train_data[feature_names]\n",
    "    y_train = train_data[target_col]\n",
    "\n",
    "    for random_seed in RANDOM_SEEDS:\n",
    "        skf = StratifiedKFold(n_splits=BASE_MODEL_KFOLD_NUM_SPLITS, shuffle=True, random_state=random_seed)\n",
    "        skf_splits = skf.split(train_data[feature_names], train_data[target_col])\n",
    "        skf_enumeration = enumerate(skf_splits)\n",
    "\n",
    "        seed_oof_preds = np.zeros(len(train_data))\n",
    "    \n",
    "        for fold, (train_indices, validation_indices) in skf_enumeration:\n",
    "            X_train_fold = X_train.iloc[train_indices]\n",
    "            X_validation_fold = X_train.iloc[validation_indices]\n",
    "            y_train_fold = y_train.iloc[train_indices]\n",
    "            y_validation_fold = y_train.iloc[validation_indices]\n",
    "        \n",
    "            model = CatBoostClassifier(\n",
    "                **params_dict,\n",
    "                use_best_model=True,\n",
    "                cat_features=cat_features,\n",
    "                loss_function='Logloss',\n",
    "                eval_metric='AUC',\n",
    "                task_type='GPU' if torch.cuda.is_available() else 'CPU',\n",
    "                devices='0',\n",
    "                metric_period=1000,\n",
    "                random_seed=random_seed,\n",
    "                verbose=False,\n",
    "                allow_writing_files=False\n",
    "            )\n",
    "            model.fit(\n",
    "                X_train_fold, y_train_fold,\n",
    "                eval_set=(X_validation_fold, y_validation_fold),\n",
    "                early_stopping_rounds=BASE_MODEL_EARLY_STOPPING_ROUNDS\n",
    "            )\n",
    "\n",
    "            y_validation_pred_proba = model.predict_proba(X_validation_fold)[:, 1]\n",
    "            y_test_pred_proba = model.predict_proba(test_data[feature_names])[:, 1]\n",
    "            seed_oof_preds[validation_indices] = np.array(y_validation_pred_proba)\n",
    "            test_preds_accumulator += np.array(y_test_pred_proba)\n",
    "\n",
    "        oof_preds_accumulator += seed_oof_preds\n",
    "\n",
    "    final_oof_preds = oof_preds_accumulator / len(RANDOM_SEEDS)\n",
    "    final_test_preds = test_preds_accumulator / (BASE_MODEL_KFOLD_NUM_SPLITS * len(RANDOM_SEEDS))\n",
    "    return pd.Series(final_oof_preds), pd.Series(final_test_preds)\n",
    "\n",
    "def get_catboostclassifier_stacking_estimator(index, params_dict, feature_names):\n",
    "    return StackingEstimator(\n",
    "        name=f\"CatBoostClassifier_{index}\",\n",
    "        params_dict=params_dict,\n",
    "        feature_names=feature_names,\n",
    "        get_preds=get_catboostclassifier_preds\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60ac967",
   "metadata": {
    "papermill": {
     "duration": 0.013301,
     "end_time": "2025-12-18T13:04:57.409399",
     "exception": false,
     "start_time": "2025-12-18T13:04:57.396098",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 8.1.2 Add Estimators (CatBoostClassifier)\n",
    "\n",
    "Add CatBoostClassifier estimators to list that StackingPredictionsRetriever will process. Hyperparameters were found using Optuna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "924350f3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-18T13:04:57.437309Z",
     "iopub.status.busy": "2025-12-18T13:04:57.436690Z",
     "iopub.status.idle": "2025-12-18T13:04:57.442396Z",
     "shell.execute_reply": "2025-12-18T13:04:57.441705Z"
    },
    "papermill": {
     "duration": 0.0209,
     "end_time": "2025-12-18T13:04:57.443532",
     "exception": false,
     "start_time": "2025-12-18T13:04:57.422632",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# CatBoostClassifier base models using FEATURE_SET_1\n",
    "estimators += [\n",
    "    get_catboostclassifier_stacking_estimator(\n",
    "        index=1,\n",
    "        params_dict={ # Optuna study AUC: 0.7261767336235222\n",
    "            'iterations': 30000,\n",
    "            'learning_rate': 0.03933473509871599,\n",
    "            'depth': 3,\n",
    "            'l2_leaf_reg': 14.932109771039046,\n",
    "            'bagging_temperature': 0.13345806085697987,\n",
    "            'random_strength': 7.486374538597635,\n",
    "            'min_data_in_leaf': 2,\n",
    "        },\n",
    "        feature_names=FEATURE_SET_1\n",
    "    ),\n",
    "    get_catboostclassifier_stacking_estimator(\n",
    "        index=2,\n",
    "        params_dict={ # Optuna study AUC: 0.725842155230371\n",
    "            'iterations': 30000,\n",
    "            'learning_rate': 0.041779205681346576,\n",
    "            'depth': 4,\n",
    "            'l2_leaf_reg': 3.628892496718331,\n",
    "            'bagging_temperature': 0.1922242909320177,\n",
    "            'random_strength': 8.464699585881778,\n",
    "            'min_data_in_leaf': 5,\n",
    "        },\n",
    "        feature_names=FEATURE_SET_1\n",
    "    ),\n",
    "     get_catboostclassifier_stacking_estimator(\n",
    "        index=3,\n",
    "        params_dict={ # Optuna study AUC: 0.7257614687804782\n",
    "            'iterations': 30000,\n",
    "            'learning_rate': 0.08955773312600926,\n",
    "            'depth': 4,\n",
    "            'l2_leaf_reg': 8.952470035979275,\n",
    "            'bagging_temperature': 0.21150772067613666,\n",
    "            'random_strength': 14.741499198080962,\n",
    "            'min_data_in_leaf': 1,\n",
    "        },\n",
    "        feature_names=FEATURE_SET_1\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7287a39",
   "metadata": {
    "papermill": {
     "duration": 0.013111,
     "end_time": "2025-12-18T13:04:57.469810",
     "exception": false,
     "start_time": "2025-12-18T13:04:57.456699",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 8.2 LGBMClassifier\n",
    "\n",
    "### 8.2.1 Helper Methods (LGBMClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c3287d58",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-18T13:04:57.497341Z",
     "iopub.status.busy": "2025-12-18T13:04:57.496895Z",
     "iopub.status.idle": "2025-12-18T13:04:57.504348Z",
     "shell.execute_reply": "2025-12-18T13:04:57.503712Z"
    },
    "papermill": {
     "duration": 0.022497,
     "end_time": "2025-12-18T13:04:57.505518",
     "exception": false,
     "start_time": "2025-12-18T13:04:57.483021",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_lgbmclassifier_preds(params_dict, feature_names):\n",
    "    oof_preds_accumulator = np.zeros(len(train_data))\n",
    "    test_preds_accumulator = np.zeros(len(test_data))\n",
    "\n",
    "    X_train = train_data[feature_names]\n",
    "    y_train = train_data[target_col]\n",
    "\n",
    "    for random_seed in RANDOM_SEEDS:\n",
    "        skf = StratifiedKFold(n_splits=BASE_MODEL_KFOLD_NUM_SPLITS, shuffle=True, random_state=random_seed)\n",
    "        skf_splits = skf.split(train_data[feature_names], train_data[target_col])\n",
    "        skf_enumeration = enumerate(skf_splits)\n",
    "\n",
    "        seed_oof_preds = np.zeros(len(train_data))\n",
    "\n",
    "        for fold, (train_indices, validation_indices) in skf_enumeration:\n",
    "            X_train_fold = X_train.iloc[train_indices]\n",
    "            X_validation_fold = X_train.iloc[validation_indices]\n",
    "            y_train_fold = y_train.iloc[train_indices]\n",
    "            y_validation_fold = y_train.iloc[validation_indices]\n",
    "\n",
    "            model = lgb.LGBMClassifier(\n",
    "                **params_dict,\n",
    "                n_estimators=30000,\n",
    "                objective='binary',\n",
    "                metric='auc',\n",
    "                verbose=-1,\n",
    "                n_jobs=-1\n",
    "            )\n",
    "            model.fit(\n",
    "                X_train_fold, y_train_fold,\n",
    "                eval_set=(X_validation_fold, y_validation_fold),\n",
    "                callbacks=[lgb.early_stopping(stopping_rounds=BASE_MODEL_EARLY_STOPPING_ROUNDS, verbose=0)]\n",
    "            )\n",
    "\n",
    "            y_validation_pred_proba = model.predict_proba(X_validation_fold)[:, 1]\n",
    "            y_test_pred_proba = model.predict_proba(test_data[feature_names])[:, 1]\n",
    "            seed_oof_preds[validation_indices] = np.array(y_validation_pred_proba)\n",
    "            test_preds_accumulator += np.array(y_test_pred_proba)\n",
    "\n",
    "        oof_preds_accumulator += seed_oof_preds\n",
    "\n",
    "    final_oof_preds = oof_preds_accumulator / len(RANDOM_SEEDS)\n",
    "    final_test_preds = test_preds_accumulator / (BASE_MODEL_KFOLD_NUM_SPLITS * len(RANDOM_SEEDS))\n",
    "    return pd.Series(final_oof_preds), pd.Series(final_test_preds)\n",
    "\n",
    "def get_lgbmclassifier_stacking_estimator(index, params_dict, feature_names):\n",
    "    return StackingEstimator(\n",
    "        name=f\"LGBMClassifier_{index}\",\n",
    "        params_dict=params_dict,\n",
    "        feature_names=feature_names,\n",
    "        get_preds=get_lgbmclassifier_preds\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316eff6d",
   "metadata": {
    "papermill": {
     "duration": 0.01423,
     "end_time": "2025-12-18T13:04:57.532920",
     "exception": false,
     "start_time": "2025-12-18T13:04:57.518690",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 8.2.2 Add Estimators (LGBMClassifier)\n",
    "\n",
    "Add XGBClassifier estimators to list that StackingPredictionsRetriever will process. Hyperparameters were found using Optuna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c201212f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-18T13:04:57.575599Z",
     "iopub.status.busy": "2025-12-18T13:04:57.575357Z",
     "iopub.status.idle": "2025-12-18T13:04:57.584712Z",
     "shell.execute_reply": "2025-12-18T13:04:57.583989Z"
    },
    "papermill": {
     "duration": 0.031064,
     "end_time": "2025-12-18T13:04:57.585854",
     "exception": false,
     "start_time": "2025-12-18T13:04:57.554790",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# LGBMClassifier base models using FEATURE_SET_1\n",
    "estimators += [\n",
    "    get_lgbmclassifier_stacking_estimator(\n",
    "        index=1,\n",
    "        params_dict={ # Optuna study AUC: 0.7276280474276193\n",
    "            'learning_rate': 0.01125492919087445,\n",
    "            'num_leaves': 18,\n",
    "            'min_child_samples': 11,\n",
    "            'subsample': 0.999676392356712,\n",
    "            'colsample_bytree': 0.5319864181594173,\n",
    "            'reg_alpha': 9.584221562909311,\n",
    "            'reg_lambda': 3.3831986318550724,\n",
    "        },\n",
    "        feature_names=FEATURE_SET_1\n",
    "    ),\n",
    "    get_lgbmclassifier_stacking_estimator(\n",
    "        index=2,\n",
    "        params_dict={ # Optuna study AUC: 0.7277311734147552\n",
    "            'learning_rate': 0.060449260107967834,\n",
    "            'num_leaves': 7,\n",
    "            'min_child_samples': 55,\n",
    "            'subsample': 0.923934087842396,\n",
    "            'colsample_bytree': 0.5313004056194756,\n",
    "            'reg_alpha': 9.481239127684901,\n",
    "            'reg_lambda': 0.001336263986782526,\n",
    "        },\n",
    "        feature_names=FEATURE_SET_1\n",
    "    ),\n",
    "    get_lgbmclassifier_stacking_estimator(\n",
    "        index=3,\n",
    "        params_dict={ # Optuna study AUC: 0.7277063286096981\n",
    "            'learning_rate': 0.028103753111304447,\n",
    "            'num_leaves': 8,\n",
    "            'min_child_samples': 50,\n",
    "            'subsample': 0.5917000582350134,\n",
    "            'colsample_bytree': 0.4222659398825859,\n",
    "            'reg_alpha': 17.056836702128017,\n",
    "            'reg_lambda': 0.005161430844595434,\n",
    "        },\n",
    "        feature_names=FEATURE_SET_1\n",
    "    ),\n",
    "    get_lgbmclassifier_stacking_estimator(\n",
    "        index=4,\n",
    "        params_dict={ # Optuna study AUC: 0.7277637392447089\n",
    "            'learning_rate': 0.021776422228844104,\n",
    "            'num_leaves': 34,\n",
    "            'min_child_samples': 124,\n",
    "            'subsample': 0.7672542347544175,\n",
    "            'colsample_bytree': 0.40540262525095094,\n",
    "            'reg_alpha': 13.154165547218854,\n",
    "            'reg_lambda': 1.6421360904189628,\n",
    "        },\n",
    "        feature_names=FEATURE_SET_1\n",
    "    ),\n",
    "    get_lgbmclassifier_stacking_estimator(\n",
    "        index=5,\n",
    "        params_dict={ # Optuna study AUC: 0.7280265610367231\n",
    "            'learning_rate': 0.005372538919315431,\n",
    "            'num_leaves': 40,\n",
    "            'min_child_samples': 494,\n",
    "            'subsample': 0.9625276144224274,\n",
    "            'colsample_bytree': 0.3279562120965377,\n",
    "            'reg_alpha': 1.5142522813882282,\n",
    "            'reg_lambda': 4.088524578916788,\n",
    "        },\n",
    "        feature_names=FEATURE_SET_1\n",
    "    ),\n",
    "    get_lgbmclassifier_stacking_estimator(\n",
    "        index=6,\n",
    "        params_dict={ # Optuna study AUC: 0.7285206901236\n",
    "            'learning_rate': 0.010514959295597044,\n",
    "            'num_leaves': 59,\n",
    "            'min_child_samples': 459,\n",
    "            'subsample': 0.9801456440779619,\n",
    "            'colsample_bytree': 0.21788689229472508,\n",
    "            'reg_alpha': 1.0879584848361499,\n",
    "            'reg_lambda': 2.824035430134392,\n",
    "        },\n",
    "        feature_names=FEATURE_SET_1\n",
    "    ),\n",
    "    get_lgbmclassifier_stacking_estimator(\n",
    "        index=7,\n",
    "        params_dict={ # Optuna study AUC: 0.7284107120755644\n",
    "            'learning_rate': 0.005337402645739864,\n",
    "            'num_leaves': 63,\n",
    "            'min_child_samples': 164,\n",
    "            'subsample': 0.8344370152901462,\n",
    "            'colsample_bytree': 0.24907941694655586,\n",
    "            'reg_alpha': 11.033723247954274,\n",
    "            'reg_lambda': 7.232987300526449,\n",
    "        },\n",
    "        feature_names=FEATURE_SET_1\n",
    "    ),\n",
    "]\n",
    "\n",
    "# LGBMClassifier base models using FEATURE_SET_2\n",
    "estimators += [\n",
    "    get_lgbmclassifier_stacking_estimator(\n",
    "        index=100,\n",
    "        params_dict={ # Optuna study AUC: 0.7267113594654058\n",
    "            'learning_rate': 0.08030213631902122,\n",
    "            'num_leaves': 10,\n",
    "            'min_child_samples': 36,\n",
    "            'subsample': 0.9736572187439243,\n",
    "            'colsample_bytree': 0.6862436315985779,\n",
    "            'reg_alpha': 19.485115558852872,\n",
    "            'reg_lambda': 0.11379340962435365,\n",
    "        },\n",
    "        feature_names=FEATURE_SET_2\n",
    "    ),\n",
    "    get_lgbmclassifier_stacking_estimator(\n",
    "        index=101,\n",
    "        params_dict={ # Optuna study AUC: 0.7267412352672019\n",
    "            'learning_rate': 0.04180757514768784,\n",
    "            'num_leaves': 13,\n",
    "            'min_child_samples': 15,\n",
    "            'subsample': 0.7602307843844437,\n",
    "            'colsample_bytree': 0.6064219998341494,\n",
    "            'reg_alpha': 11.137420503168325,\n",
    "            'reg_lambda': 0.1206423560374285,\n",
    "        },\n",
    "        feature_names=FEATURE_SET_2\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80b5f72",
   "metadata": {
    "papermill": {
     "duration": 0.01317,
     "end_time": "2025-12-18T13:04:57.612666",
     "exception": false,
     "start_time": "2025-12-18T13:04:57.599496",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 8.3 XGBClassifier\n",
    "\n",
    "### 8.3.1 Helper Methods (XGBClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a81570e0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-18T13:04:57.641112Z",
     "iopub.status.busy": "2025-12-18T13:04:57.640471Z",
     "iopub.status.idle": "2025-12-18T13:04:57.648174Z",
     "shell.execute_reply": "2025-12-18T13:04:57.647453Z"
    },
    "papermill": {
     "duration": 0.02294,
     "end_time": "2025-12-18T13:04:57.649387",
     "exception": false,
     "start_time": "2025-12-18T13:04:57.626447",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_xgbclassifier_preds(params_dict, feature_names):\n",
    "    oof_preds_accumulator = np.zeros(len(train_data))\n",
    "    test_preds_accumulator = np.zeros(len(test_data))\n",
    "\n",
    "    X_train = train_data[feature_names]\n",
    "    y_train = train_data[target_col]\n",
    "\n",
    "    for random_seed in RANDOM_SEEDS:\n",
    "        skf = StratifiedKFold(n_splits=BASE_MODEL_KFOLD_NUM_SPLITS, shuffle=True, random_state=random_seed)\n",
    "        skf_splits = skf.split(X_train, y_train)\n",
    "        skf_enumeration = enumerate(skf_splits)\n",
    "\n",
    "        seed_oof_preds = np.zeros(len(train_data))\n",
    "\n",
    "        for fold, (train_indices, validation_indices) in skf_enumeration:\n",
    "            X_train_fold = X_train.iloc[train_indices]\n",
    "            X_validation_fold = X_train.iloc[validation_indices]\n",
    "            y_train_fold = y_train.iloc[train_indices]\n",
    "            y_validation_fold = y_train.iloc[validation_indices]\n",
    "\n",
    "            model = XGBClassifier(\n",
    "                **params_dict,\n",
    "                tree_method='hist' if torch.cuda.is_available() else 'auto',\n",
    "                device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "                enable_categorical=True,\n",
    "                objective='binary:logistic',\n",
    "                eval_metric='auc',\n",
    "                early_stopping_rounds=BASE_MODEL_EARLY_STOPPING_ROUNDS,\n",
    "                n_jobs=-1,\n",
    "                random_state=random_seed,\n",
    "                verbosity=0\n",
    "            )\n",
    "            model.fit(\n",
    "                X_train_fold, y_train_fold,\n",
    "                eval_set=[(X_validation_fold, y_validation_fold)],\n",
    "                verbose=False\n",
    "            )\n",
    "\n",
    "            y_validation_pred_proba = model.predict_proba(X_validation_fold)[:, 1]\n",
    "            y_test_pred_proba = model.predict_proba(test_data[feature_names])[:, 1]\n",
    "            seed_oof_preds[validation_indices] = np.array(y_validation_pred_proba)\n",
    "            test_preds_accumulator += np.array(y_test_pred_proba)\n",
    "\n",
    "        oof_preds_accumulator += seed_oof_preds\n",
    "\n",
    "    final_oof_preds = oof_preds_accumulator / len(RANDOM_SEEDS)\n",
    "    final_test_preds = test_preds_accumulator / (BASE_MODEL_KFOLD_NUM_SPLITS * len(RANDOM_SEEDS))\n",
    "    return pd.Series(final_oof_preds), pd.Series(final_test_preds)\n",
    "\n",
    "def get_xgbclassifier_stacking_estimator(index, params_dict, feature_names):\n",
    "    return StackingEstimator(\n",
    "        name=f\"XGBClassifier_{index}\",\n",
    "        params_dict=params_dict,\n",
    "        feature_names=feature_names,\n",
    "        get_preds=get_xgbclassifier_preds\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a764f154",
   "metadata": {
    "papermill": {
     "duration": 0.013279,
     "end_time": "2025-12-18T13:04:57.676254",
     "exception": false,
     "start_time": "2025-12-18T13:04:57.662975",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 8.3.2 Add Estimators (XGBClassifier)\n",
    "\n",
    "Add XGBClassifier estimators to list that StackingPredictionsRetriever will process. Hyperparameters were found using Optuna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8b379cfc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-18T13:04:57.703944Z",
     "iopub.status.busy": "2025-12-18T13:04:57.703535Z",
     "iopub.status.idle": "2025-12-18T13:04:57.711699Z",
     "shell.execute_reply": "2025-12-18T13:04:57.711038Z"
    },
    "papermill": {
     "duration": 0.023454,
     "end_time": "2025-12-18T13:04:57.712803",
     "exception": false,
     "start_time": "2025-12-18T13:04:57.689349",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# XGBClassifier base models using FEATURE_SET_1\n",
    "estimators += [\n",
    "    get_xgbclassifier_stacking_estimator(\n",
    "        index=1,\n",
    "        params_dict={ # Optuna study AUC: 0.7275219804910846\n",
    "            'n_estimators': 30000,\n",
    "            'learning_rate': 0.00985498815107458,\n",
    "            'max_depth': 3,\n",
    "            'subsample': 0.975836120137461,\n",
    "            'colsample_bytree': 0.5411854284303592,\n",
    "            'alpha': 9.940781978752474,\n",
    "            'gamma': 0.008422323405815038,\n",
    "            'lambda': 0.025214960531620187,\n",
    "            'min_child_weight': 12,\n",
    "        },\n",
    "        feature_names=FEATURE_SET_1\n",
    "    ),\n",
    "    get_xgbclassifier_stacking_estimator(\n",
    "        index=2,\n",
    "        params_dict={ # Optuna study AUC: 0.7273817150393508\n",
    "            'n_estimators': 30000,\n",
    "            'learning_rate': 0.047179227853488916,\n",
    "            'max_depth': 3,\n",
    "            'subsample': 0.9561594029099818,\n",
    "            'colsample_bytree': 0.5200809916944509,\n",
    "            'alpha': 9.323686821094613,\n",
    "            'gamma': 0.06513704074541844,\n",
    "            'lambda': 0.07573405175712218,\n",
    "            'min_child_weight': 14,\n",
    "        },\n",
    "        feature_names=FEATURE_SET_1\n",
    "    ),\n",
    "    get_xgbclassifier_stacking_estimator(\n",
    "        index=3,\n",
    "        params_dict={ # Optuna study AUC: 0.7274144144696422\n",
    "            'n_estimators': 30000,\n",
    "            'learning_rate': 0.06778303256075534,\n",
    "            'max_depth': 3,\n",
    "            'subsample': 0.9750702612583769,\n",
    "            'colsample_bytree': 0.5164463777572837,\n",
    "            'alpha': 6.677223824702266,\n",
    "            'gamma': 0.06627215758548254,\n",
    "            'lambda': 0.10239210156952944,\n",
    "            'min_child_weight': 17,\n",
    "        },\n",
    "        feature_names=FEATURE_SET_1\n",
    "    ),\n",
    "    get_xgbclassifier_stacking_estimator(\n",
    "        index=4,\n",
    "        params_dict={ # Optuna study AUC: 0.7263868488191946\n",
    "            'n_estimators': 30000,\n",
    "            'learning_rate': 0.00992002978574334,\n",
    "            'max_depth': 6,\n",
    "            'subsample': 0.6885700003314461,\n",
    "            'colsample_bytree': 0.5082842329050175,\n",
    "            'alpha': 4.042835803115786,\n",
    "            'gamma': 0.19033575052721494,\n",
    "            'lambda': 1.4531584526994292,\n",
    "            'min_child_weight': 79,\n",
    "        },\n",
    "        feature_names=FEATURE_SET_1\n",
    "    ),\n",
    "    get_xgbclassifier_stacking_estimator(\n",
    "        index=5,\n",
    "        params_dict={ # Optuna study AUC: 0.7261995858097773\n",
    "            'n_estimators': 30000,\n",
    "            'learning_rate': 0.005092159244819224,\n",
    "            'max_depth': 8,\n",
    "            'subsample': 0.6985482460232558,\n",
    "            'colsample_bytree': 0.5002716122370332,\n",
    "            'alpha': 0.5442317401534714,\n",
    "            'gamma': 0.9101677712528158,\n",
    "            'lambda': 1.4849248721792976,\n",
    "            'min_child_weight': 86,\n",
    "        },\n",
    "        feature_names=FEATURE_SET_1\n",
    "    ),\n",
    "    # get_xgbclassifier_stacking_estimator(\n",
    "    #     index=6,\n",
    "    #     params_dict={ # Optuna study AUC: 0.7263827153203234\n",
    "    #         'n_estimators': 30000,\n",
    "    #         'learning_rate': 0.016719342630848243,\n",
    "    #         'max_depth': 4,\n",
    "    #         'subsample': 0.6941658041689057,\n",
    "    #         'colsample_bytree': 0.8090538581001316,\n",
    "    #         'alpha': 0.007129383729049109,\n",
    "    #         'gamma': 0.48079353784117146,\n",
    "    #         'lambda': 0.00101543996647542,\n",
    "    #         'min_child_weight': 50,\n",
    "    #     },\n",
    "    #     feature_names=FEATURE_SET_1\n",
    "    # ),\n",
    "]\n",
    "\n",
    "# XGBClassifier base models using FEATURE_SET_2\n",
    "estimators += [\n",
    "    get_xgbclassifier_stacking_estimator(\n",
    "        index=100,\n",
    "        params_dict={ # Optuna study AUC: 0.7262159108318079\n",
    "            'n_estimators': 30000,\n",
    "            'learning_rate': 0.011859808032021718,\n",
    "            'max_depth': 3,\n",
    "            'subsample': 0.9372116555018073,\n",
    "            'colsample_bytree': 0.9755650095828481,\n",
    "            'alpha': 11.83079224267289,\n",
    "            'gamma': 0.47957759727475824,\n",
    "            'lambda': 1.5631226520724053,\n",
    "            'min_child_weight': 38,\n",
    "        },\n",
    "        feature_names=FEATURE_SET_2\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346d4c8c",
   "metadata": {
    "papermill": {
     "duration": 0.013275,
     "end_time": "2025-12-18T13:04:57.739683",
     "exception": false,
     "start_time": "2025-12-18T13:04:57.726408",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 9. Base Model Predictions\n",
    "\n",
    "## 9.1 Get Base Model Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bbdf0547",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-18T13:04:57.767669Z",
     "iopub.status.busy": "2025-12-18T13:04:57.767087Z",
     "iopub.status.idle": "2025-12-18T13:05:38.348001Z",
     "shell.execute_reply": "2025-12-18T13:05:38.347382Z"
    },
    "papermill": {
     "duration": 40.596668,
     "end_time": "2025-12-18T13:05:38.349385",
     "exception": false,
     "start_time": "2025-12-18T13:04:57.752717",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Importing predictions..\n",
      "[INFO] 18 train predictions were imported:\n",
      "CatBoostClassifier_1 (5b3133609cb7932f4c272f1494d1f6b0), CatBoostClassifier_2 (c5054ebf7585fcc187c40442e3cbbcff), CatBoostClassifier_3 (a2e94c97067190eb52e25c8f0d0b1b81), LGBMClassifier_1 (5e025e207cf4283c7e28cd00096712fe), LGBMClassifier_2 (756b644eceb4b410ad34d055d7a2a7ad), LGBMClassifier_3 (12e279a3f79f03854fcc391dd7dbd023), LGBMClassifier_4 (fa1a2bd980655d692c10eb2a327f6445), LGBMClassifier_5 (f70e8914ef00881b6e0ca60a3f6c8f70), LGBMClassifier_6 (a0c02e5dec4bd4467961cd96901740a9), LGBMClassifier_7 (0d5c160d0c75a01607ee65e1309f0f3e), LGBMClassifier_100 (e9308d5ef0fe3d2e21b23b76c24abf10), LGBMClassifier_101 (4184d68f723388204df6096a194619e3), XGBClassifier_1 (648af2669b3c1b6d36d968e260379dee), XGBClassifier_2 (f9455e9875a464c78cd90b60584d3f1c), XGBClassifier_3 (4a02ebc078c643d903a00ddcfb0f3301), XGBClassifier_4 (41fcdf694bf252b540f6c6d28d63eb45), XGBClassifier_5 (04de438dafcf1a1b67a0dbfa0e2bc0d1), XGBClassifier_100 (a0e17bbe4a8e31c8ed28c4ff78fd8568)\n",
      "[INFO] 18 test predictions were imported:\n",
      "CatBoostClassifier_1 (5b3133609cb7932f4c272f1494d1f6b0), CatBoostClassifier_2 (c5054ebf7585fcc187c40442e3cbbcff), CatBoostClassifier_3 (a2e94c97067190eb52e25c8f0d0b1b81), LGBMClassifier_1 (5e025e207cf4283c7e28cd00096712fe), LGBMClassifier_2 (756b644eceb4b410ad34d055d7a2a7ad), LGBMClassifier_3 (12e279a3f79f03854fcc391dd7dbd023), LGBMClassifier_4 (fa1a2bd980655d692c10eb2a327f6445), LGBMClassifier_5 (f70e8914ef00881b6e0ca60a3f6c8f70), LGBMClassifier_6 (a0c02e5dec4bd4467961cd96901740a9), LGBMClassifier_7 (0d5c160d0c75a01607ee65e1309f0f3e), LGBMClassifier_100 (e9308d5ef0fe3d2e21b23b76c24abf10), LGBMClassifier_101 (4184d68f723388204df6096a194619e3), XGBClassifier_1 (648af2669b3c1b6d36d968e260379dee), XGBClassifier_2 (f9455e9875a464c78cd90b60584d3f1c), XGBClassifier_3 (4a02ebc078c643d903a00ddcfb0f3301), XGBClassifier_4 (41fcdf694bf252b540f6c6d28d63eb45), XGBClassifier_5 (04de438dafcf1a1b67a0dbfa0e2bc0d1), XGBClassifier_100 (a0e17bbe4a8e31c8ed28c4ff78fd8568)\n",
      "[INFO] Finished importing predictions\n",
      "[INFO] Syncing predictions..\n",
      "[INFO] No columns for training predictions were dropped\n",
      "[INFO] No columns for test predictions were dropped\n",
      "[INFO] Finished syncing predictions\n",
      "[INFO] Getting predictions..\n",
      "[INFO] Skipped retrieving predictions for following estimators as their current ones are not stale:\n",
      "CatBoostClassifier_1 (5b3133609cb7932f4c272f1494d1f6b0), CatBoostClassifier_2 (c5054ebf7585fcc187c40442e3cbbcff), CatBoostClassifier_3 (a2e94c97067190eb52e25c8f0d0b1b81), LGBMClassifier_1 (5e025e207cf4283c7e28cd00096712fe), LGBMClassifier_100 (e9308d5ef0fe3d2e21b23b76c24abf10), LGBMClassifier_101 (4184d68f723388204df6096a194619e3), LGBMClassifier_2 (756b644eceb4b410ad34d055d7a2a7ad), LGBMClassifier_3 (12e279a3f79f03854fcc391dd7dbd023), LGBMClassifier_4 (fa1a2bd980655d692c10eb2a327f6445), LGBMClassifier_5 (f70e8914ef00881b6e0ca60a3f6c8f70), LGBMClassifier_6 (a0c02e5dec4bd4467961cd96901740a9), LGBMClassifier_7 (0d5c160d0c75a01607ee65e1309f0f3e), XGBClassifier_1 (648af2669b3c1b6d36d968e260379dee), XGBClassifier_100 (a0e17bbe4a8e31c8ed28c4ff78fd8568), XGBClassifier_2 (f9455e9875a464c78cd90b60584d3f1c), XGBClassifier_3 (4a02ebc078c643d903a00ddcfb0f3301), XGBClassifier_4 (41fcdf694bf252b540f6c6d28d63eb45), XGBClassifier_5 (04de438dafcf1a1b67a0dbfa0e2bc0d1)\n",
      "[INFO] Finished getting all predictions\n"
     ]
    }
   ],
   "source": [
    "stacking_preds_retriever = StackingPredictionsRetriever(\n",
    "    estimators=estimators,\n",
    "    working_dir_path=\"/kaggle/working/\",\n",
    "    train_preds_filename=\"base_models_train_preds\",\n",
    "    test_preds_filename=\"base_models_test_preds\",\n",
    "    preds_save_interval=1\n",
    ")\n",
    "stacking_preds_retriever.import_preds(\"/kaggle/input/diabetes-prediction-challenge-base-model-preds/\")\n",
    "stacking_preds_retriever.sync_preds()\n",
    "stacking_preds_retriever.get_preds()\n",
    "\n",
    "base_model_train_preds, base_model_test_preds = stacking_preds_retriever.get_current_train_and_test_preds()\n",
    "base_model_train_preds.sort_index(axis=1, inplace=True, key=lambda index: index.map(lambda col_name: (col_name.split(\"_\")[0], int(col_name.split()[0].split(\"_\")[-1]))))\n",
    "base_model_test_preds.sort_index(axis=1, inplace=True, key=lambda index: index.map(lambda col_name: (col_name.split(\"_\")[0], int(col_name.split()[0].split(\"_\")[-1]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41624e5d",
   "metadata": {
    "papermill": {
     "duration": 0.013593,
     "end_time": "2025-12-18T13:05:38.376953",
     "exception": false,
     "start_time": "2025-12-18T13:05:38.363360",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 9.2 Base Models AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6aa7c2e2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-18T13:05:38.405539Z",
     "iopub.status.busy": "2025-12-18T13:05:38.405290Z",
     "iopub.status.idle": "2025-12-18T13:05:41.367270Z",
     "shell.execute_reply": "2025-12-18T13:05:41.366536Z"
    },
    "papermill": {
     "duration": 2.978174,
     "end_time": "2025-12-18T13:05:41.368469",
     "exception": false,
     "start_time": "2025-12-18T13:05:38.390295",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMClassifier_6 (a0c02e5dec4bd4467961cd96901740a9)        0.729329\n",
       "LGBMClassifier_7 (0d5c160d0c75a01607ee65e1309f0f3e)        0.729191\n",
       "LGBMClassifier_5 (f70e8914ef00881b6e0ca60a3f6c8f70)        0.728832\n",
       "LGBMClassifier_4 (fa1a2bd980655d692c10eb2a327f6445)        0.728675\n",
       "LGBMClassifier_3 (12e279a3f79f03854fcc391dd7dbd023)        0.728647\n",
       "LGBMClassifier_2 (756b644eceb4b410ad34d055d7a2a7ad)        0.728547\n",
       "LGBMClassifier_1 (5e025e207cf4283c7e28cd00096712fe)        0.728484\n",
       "XGBClassifier_2 (f9455e9875a464c78cd90b60584d3f1c)         0.727895\n",
       "XGBClassifier_1 (648af2669b3c1b6d36d968e260379dee)         0.727848\n",
       "XGBClassifier_3 (4a02ebc078c643d903a00ddcfb0f3301)         0.727831\n",
       "LGBMClassifier_101 (4184d68f723388204df6096a194619e3)      0.727692\n",
       "LGBMClassifier_100 (e9308d5ef0fe3d2e21b23b76c24abf10)      0.727588\n",
       "XGBClassifier_4 (41fcdf694bf252b540f6c6d28d63eb45)         0.727212\n",
       "XGBClassifier_5 (04de438dafcf1a1b67a0dbfa0e2bc0d1)         0.727053\n",
       "XGBClassifier_100 (a0e17bbe4a8e31c8ed28c4ff78fd8568)       0.726944\n",
       "CatBoostClassifier_3 (a2e94c97067190eb52e25c8f0d0b1b81)    0.726661\n",
       "CatBoostClassifier_1 (5b3133609cb7932f4c272f1494d1f6b0)    0.726533\n",
       "CatBoostClassifier_2 (c5054ebf7585fcc187c40442e3cbbcff)    0.726503\n",
       "dtype: float64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model_auc = pd.Series()\n",
    "for estimator in base_model_train_preds.columns:\n",
    "    base_model_auc[estimator] = roc_auc_score(train_data[target_col], base_model_train_preds[estimator])\n",
    "base_model_auc.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "091dac28",
   "metadata": {
    "papermill": {
     "duration": 0.013543,
     "end_time": "2025-12-18T13:05:41.396348",
     "exception": false,
     "start_time": "2025-12-18T13:05:41.382805",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 10. Meta-Model\n",
    "\n",
    "## 10.1 Meta-Model Hyperparameter Tuning\n",
    "\n",
    "### 10.1.1 Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3e86ab89",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-18T13:05:41.424777Z",
     "iopub.status.busy": "2025-12-18T13:05:41.424293Z",
     "iopub.status.idle": "2025-12-18T13:05:41.428257Z",
     "shell.execute_reply": "2025-12-18T13:05:41.427532Z"
    },
    "papermill": {
     "duration": 0.019642,
     "end_time": "2025-12-18T13:05:41.429384",
     "exception": false,
     "start_time": "2025-12-18T13:05:41.409742",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# to skip hyperparameter tuning when it's not needed; set to `False` to do the tuning & selection\n",
    "SKIP_META_MODEL_HYPERPARAMETER_TUNING = True\n",
    "\n",
    "# maximum number of trials Optuna will conduct for the optimization\n",
    "META_MODEL_OPTUNA_STUDY_NUM_TRIALS = 50\n",
    "\n",
    "# number of splits to use for K-Fold Cross-Validation\n",
    "META_MODEL_KFOLD_NUM_SPLITS = 5\n",
    "\n",
    "# use different random seeds from ones used to train base models to avoid\n",
    "# potential leakage or alignment artifacts from original splits\n",
    "META_MODEL_RANDOM_SEEDS = [77, 99]\n",
    "\n",
    "# optuna study best parameters for meta model\n",
    "meta_model_optuna_study_best_params = {}\n",
    "\n",
    "# parameters selected for meta model\n",
    "meta_model_params = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2caf96e5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-18T13:05:41.457688Z",
     "iopub.status.busy": "2025-12-18T13:05:41.457465Z",
     "iopub.status.idle": "2025-12-18T13:05:41.464877Z",
     "shell.execute_reply": "2025-12-18T13:05:41.464206Z"
    },
    "papermill": {
     "duration": 0.023081,
     "end_time": "2025-12-18T13:05:41.465974",
     "exception": false,
     "start_time": "2025-12-18T13:05:41.442893",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_meta_model_optuna_params(trial):\n",
    "    return {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 150, 400),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.05),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 30, 63),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 30, 80),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0.1, 15.0, log=True),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0.1, 10.0, log=True),\n",
    "    }\n",
    "\n",
    "def meta_model_optuna_study_objective(trial):\n",
    "    meta_model_params = get_meta_model_optuna_params(trial)\n",
    "\n",
    "    meta_oof_preds_accumulator = np.zeros(len(train_data))\n",
    "\n",
    "    for random_seed in META_MODEL_RANDOM_SEEDS:\n",
    "        meta_skf = StratifiedKFold(n_splits=META_MODEL_KFOLD_NUM_SPLITS, shuffle=True, random_state=random_seed)\n",
    "        meta_skf_splits = meta_skf.split(base_model_train_preds, train_data[target_col])\n",
    "        meta_skf_enumeration = enumerate(meta_skf_splits)\n",
    "    \n",
    "        seed_oof_preds = np.zeros(len(train_data))\n",
    "    \n",
    "        for fold, (train_indices, validation_indices) in meta_skf_enumeration:\n",
    "            X_train_fold = base_model_train_preds.iloc[train_indices]\n",
    "            y_train_fold = train_data[target_col].iloc[train_indices]\n",
    "            X_validation_fold = base_model_train_preds.iloc[validation_indices]\n",
    "            y_validation_fold = train_data[target_col].iloc[validation_indices]\n",
    "    \n",
    "            model = lgb.LGBMClassifier(\n",
    "                **meta_model_params,\n",
    "                objective='binary',\n",
    "                metric='auc',\n",
    "                verbose=-1,\n",
    "                n_jobs=-1\n",
    "            )\n",
    "            model.fit(\n",
    "                X_train_fold, y_train_fold,\n",
    "                eval_set=(X_validation_fold, y_validation_fold)\n",
    "            )\n",
    "    \n",
    "            y_validation_pred_proba = model.predict_proba(X_validation_fold)[:, 1]\n",
    "            seed_oof_preds[validation_indices] = y_validation_pred_proba\n",
    "    \n",
    "        meta_oof_preds_accumulator += seed_oof_preds\n",
    "    \n",
    "    final_meta_oof_preds = meta_oof_preds_accumulator / len(META_MODEL_RANDOM_SEEDS)\n",
    "\n",
    "    return roc_auc_score(train_data[target_col], final_meta_oof_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3d974a90",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-18T13:05:41.493976Z",
     "iopub.status.busy": "2025-12-18T13:05:41.493771Z",
     "iopub.status.idle": "2025-12-18T13:05:41.498938Z",
     "shell.execute_reply": "2025-12-18T13:05:41.498238Z"
    },
    "papermill": {
     "duration": 0.020564,
     "end_time": "2025-12-18T13:05:41.500054",
     "exception": false,
     "start_time": "2025-12-18T13:05:41.479490",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped hyperparameter tuning for meta model\n"
     ]
    }
   ],
   "source": [
    "if SKIP_META_MODEL_HYPERPARAMETER_TUNING:\n",
    "    print(\"Skipped hyperparameter tuning for meta model\")\n",
    "else:\n",
    "    print(\"Started hyperparameter tuning for meta model\")\n",
    "    sampler = optuna.samplers.TPESampler(n_ei_candidates=48, multivariate=True)\n",
    "    study = optuna.create_study(sampler=sampler, direction=\"maximize\")\n",
    "    study.optimize(meta_model_optuna_study_objective, n_trials=META_MODEL_OPTUNA_STUDY_NUM_TRIALS)\n",
    "    \n",
    "    print(f\"# trials finished: {len(study.trials)}\")\n",
    "    trial = study.best_trial\n",
    "    meta_model_optuna_study_best_params = study.best_params\n",
    "    print(f\"Best trial AUC: {trial.value}\")\n",
    "    print(f\"Best trial params:\")\n",
    "    for param_key, param_value in meta_model_optuna_study_best_params.items():\n",
    "        print(f\"- {param_key}: {param_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f121706a",
   "metadata": {
    "papermill": {
     "duration": 0.013673,
     "end_time": "2025-12-18T13:05:41.527407",
     "exception": false,
     "start_time": "2025-12-18T13:05:41.513734",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 10.1.2 Tuned Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "03db55be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-18T13:05:41.555619Z",
     "iopub.status.busy": "2025-12-18T13:05:41.555376Z",
     "iopub.status.idle": "2025-12-18T13:05:41.561659Z",
     "shell.execute_reply": "2025-12-18T13:05:41.560861Z"
    },
    "papermill": {
     "duration": 0.02185,
     "end_time": "2025-12-18T13:05:41.562692",
     "exception": false,
     "start_time": "2025-12-18T13:05:41.540842",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following tuned parameters will be used for the meta model:\n",
      "- n_estimators: 381\n",
      "- learning_rate: 0.022384407942929303\n",
      "- num_leaves: 32\n",
      "- min_child_samples: 66\n",
      "- subsample: 0.732532371499966\n",
      "- colsample_bytree: 0.8509773544372147\n",
      "- reg_alpha: 7.966984554886289\n",
      "- reg_lambda: 2.745546755231143\n"
     ]
    }
   ],
   "source": [
    "# default values (most found from previous tuning/selection)\n",
    "META_MODEL_DEFAULT_N_ESTIMATORS = 381\n",
    "META_MODEL_DEFAULT_LEARNING_RATE = 0.022384407942929303\n",
    "META_MODEL_DEFAULT_NUM_LEAVES = 32\n",
    "META_MODEL_DEFAULT_MIN_CHILD_SAMPLES = 66\n",
    "META_MODEL_DEFAULT_SUBSAMPLE = 0.732532371499966\n",
    "META_MODEL_DEFAULT_COLSAMPLE_BY_TREE = 0.8509773544372147\n",
    "META_MODEL_DEFAULT_REG_ALPHA = 7.966984554886289\n",
    "META_MODEL_DEFAULT_REG_LAMBDA = 2.745546755231143\n",
    "\n",
    "# meta model parameters\n",
    "meta_model_params['n_estimators'] = meta_model_optuna_study_best_params.get('n_estimators', META_MODEL_DEFAULT_N_ESTIMATORS)\n",
    "meta_model_params['learning_rate'] = meta_model_optuna_study_best_params.get('learning_rate', META_MODEL_DEFAULT_LEARNING_RATE)\n",
    "meta_model_params['num_leaves'] = meta_model_optuna_study_best_params.get('num_leaves', META_MODEL_DEFAULT_NUM_LEAVES)\n",
    "meta_model_params['min_child_samples'] = meta_model_optuna_study_best_params.get('min_child_samples', META_MODEL_DEFAULT_MIN_CHILD_SAMPLES)\n",
    "meta_model_params['subsample'] = meta_model_optuna_study_best_params.get('subsample', META_MODEL_DEFAULT_SUBSAMPLE)\n",
    "meta_model_params['colsample_bytree'] = meta_model_optuna_study_best_params.get('colsample_bytree', META_MODEL_DEFAULT_COLSAMPLE_BY_TREE)\n",
    "meta_model_params['reg_alpha'] = meta_model_optuna_study_best_params.get('reg_alpha', META_MODEL_DEFAULT_REG_ALPHA)\n",
    "meta_model_params['reg_lambda'] = meta_model_optuna_study_best_params.get('reg_lambda', META_MODEL_DEFAULT_REG_LAMBDA)\n",
    "print(f\"The following tuned parameters will be used for the meta model:\")\n",
    "for param_key, param_value in meta_model_params.items():\n",
    "        print(f\"- {param_key}: {param_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c37950",
   "metadata": {
    "papermill": {
     "duration": 0.01344,
     "end_time": "2025-12-18T13:05:41.589801",
     "exception": false,
     "start_time": "2025-12-18T13:05:41.576361",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 10.2 Meta-Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a92e1052",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-18T13:05:41.618275Z",
     "iopub.status.busy": "2025-12-18T13:05:41.617648Z",
     "iopub.status.idle": "2025-12-18T13:08:22.875931Z",
     "shell.execute_reply": "2025-12-18T13:08:22.875286Z"
    },
    "papermill": {
     "duration": 161.274041,
     "end_time": "2025-12-18T13:08:22.877441",
     "exception": false,
     "start_time": "2025-12-18T13:05:41.603400",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "meta_oof_preds_accumulator = np.zeros(len(train_data))\n",
    "meta_test_preds_accumulator = np.zeros(len(test_data))\n",
    "meta_train_feature_importances_accumulator = np.zeros(len(base_model_train_preds.columns))\n",
    "\n",
    "for random_seed in META_MODEL_RANDOM_SEEDS:\n",
    "    meta_skf = StratifiedKFold(n_splits=META_MODEL_KFOLD_NUM_SPLITS, shuffle=True, random_state=random_seed)\n",
    "    meta_skf_splits = meta_skf.split(base_model_train_preds, train_data[target_col])\n",
    "    meta_skf_enumeration = enumerate(meta_skf_splits)\n",
    "\n",
    "    seed_oof_preds = np.zeros(len(train_data))\n",
    "\n",
    "    for fold, (train_indices, validation_indices) in meta_skf_enumeration:\n",
    "        X_train_fold = base_model_train_preds.iloc[train_indices]\n",
    "        y_train_fold = train_data[target_col].iloc[train_indices]\n",
    "        X_validation_fold = base_model_train_preds.iloc[validation_indices]\n",
    "        y_validation_fold = train_data[target_col].iloc[validation_indices]\n",
    "\n",
    "        meta_model = lgb.LGBMClassifier(\n",
    "            **meta_model_params,\n",
    "            objective='binary',\n",
    "            metric='auc',\n",
    "            verbose=-1,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        meta_model.fit(\n",
    "            X_train_fold, y_train_fold,\n",
    "            eval_set=(X_validation_fold, y_validation_fold)\n",
    "        )\n",
    "\n",
    "        y_validation_pred_proba = meta_model.predict_proba(X_validation_fold)[:, 1]\n",
    "        y_test_pred_proba = meta_model.predict_proba(base_model_test_preds)[:, 1]\n",
    "        seed_oof_preds[validation_indices] = np.array(y_validation_pred_proba)\n",
    "        meta_test_preds_accumulator += np.array(y_test_pred_proba)\n",
    "        meta_train_feature_importances_accumulator += np.array(meta_model.feature_importances_)\n",
    "\n",
    "    meta_oof_preds_accumulator += seed_oof_preds\n",
    "\n",
    "final_meta_oof_preds = meta_oof_preds_accumulator / len(META_MODEL_RANDOM_SEEDS)\n",
    "final_meta_test_preds = meta_test_preds_accumulator / (META_MODEL_KFOLD_NUM_SPLITS * len(META_MODEL_RANDOM_SEEDS))\n",
    "meta_train_feature_importances = meta_train_feature_importances_accumulator / (META_MODEL_KFOLD_NUM_SPLITS * len(META_MODEL_RANDOM_SEEDS))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a347e1",
   "metadata": {
    "papermill": {
     "duration": 0.013946,
     "end_time": "2025-12-18T13:08:22.906327",
     "exception": false,
     "start_time": "2025-12-18T13:08:22.892381",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 10.3 Meta-Model Feature Importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8f975c75",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-18T13:08:22.934758Z",
     "iopub.status.busy": "2025-12-18T13:08:22.934522Z",
     "iopub.status.idle": "2025-12-18T13:08:22.940907Z",
     "shell.execute_reply": "2025-12-18T13:08:22.940353Z"
    },
    "papermill": {
     "duration": 0.022111,
     "end_time": "2025-12-18T13:08:22.941984",
     "exception": false,
     "start_time": "2025-12-18T13:08:22.919873",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMClassifier_6 (a0c02e5dec4bd4467961cd96901740a9)        1241.7\n",
       "LGBMClassifier_7 (0d5c160d0c75a01607ee65e1309f0f3e)         988.8\n",
       "XGBClassifier_3 (4a02ebc078c643d903a00ddcfb0f3301)          793.1\n",
       "XGBClassifier_100 (a0e17bbe4a8e31c8ed28c4ff78fd8568)        722.3\n",
       "XGBClassifier_2 (f9455e9875a464c78cd90b60584d3f1c)          706.0\n",
       "XGBClassifier_5 (04de438dafcf1a1b67a0dbfa0e2bc0d1)          698.3\n",
       "XGBClassifier_4 (41fcdf694bf252b540f6c6d28d63eb45)          695.9\n",
       "CatBoostClassifier_2 (c5054ebf7585fcc187c40442e3cbbcff)     627.0\n",
       "CatBoostClassifier_1 (5b3133609cb7932f4c272f1494d1f6b0)     623.6\n",
       "LGBMClassifier_2 (756b644eceb4b410ad34d055d7a2a7ad)         609.3\n",
       "LGBMClassifier_3 (12e279a3f79f03854fcc391dd7dbd023)         592.8\n",
       "LGBMClassifier_101 (4184d68f723388204df6096a194619e3)       561.6\n",
       "CatBoostClassifier_3 (a2e94c97067190eb52e25c8f0d0b1b81)     540.5\n",
       "LGBMClassifier_100 (e9308d5ef0fe3d2e21b23b76c24abf10)       522.9\n",
       "XGBClassifier_1 (648af2669b3c1b6d36d968e260379dee)          483.8\n",
       "LGBMClassifier_4 (fa1a2bd980655d692c10eb2a327f6445)         472.6\n",
       "LGBMClassifier_1 (5e025e207cf4283c7e28cd00096712fe)         466.5\n",
       "LGBMClassifier_5 (f70e8914ef00881b6e0ca60a3f6c8f70)         464.3\n",
       "dtype: float64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_model_feature_importances = pd.Series(meta_train_feature_importances)\n",
    "meta_model_feature_importances.index = base_model_train_preds.columns\n",
    "meta_model_feature_importances.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69cc5619",
   "metadata": {
    "papermill": {
     "duration": 0.013696,
     "end_time": "2025-12-18T13:08:22.969834",
     "exception": false,
     "start_time": "2025-12-18T13:08:22.956138",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 10.4 Final Adjustments to Test Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9cbe6eb8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-18T13:08:22.998518Z",
     "iopub.status.busy": "2025-12-18T13:08:22.997862Z",
     "iopub.status.idle": "2025-12-18T13:08:23.009333Z",
     "shell.execute_reply": "2025-12-18T13:08:23.008742Z"
    },
    "papermill": {
     "duration": 0.026964,
     "end_time": "2025-12-18T13:08:23.010427",
     "exception": false,
     "start_time": "2025-12-18T13:08:22.983463",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def min_max_scale(preds):\n",
    "    min_val = preds.min()\n",
    "    max_val = preds.max()\n",
    "    if max_val > min_val:\n",
    "        return (preds - min_val) / (max_val - min_val)\n",
    "    return preds\n",
    "\n",
    "# scale final meta oof/test preds\n",
    "scaled_final_meta_oof_preds = min_max_scale(final_meta_oof_preds)\n",
    "scaled_final_meta_test_preds = min_max_scale(final_meta_test_preds)\n",
    "\n",
    "# just in case floating point math leaves values very slightly below 0 or above 1\n",
    "scaled_final_meta_oof_preds = np.clip(scaled_final_meta_oof_preds, 0, 1)\n",
    "scaled_final_meta_test_preds = np.clip(scaled_final_meta_test_preds, 0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3926f9d0",
   "metadata": {
    "papermill": {
     "duration": 0.013972,
     "end_time": "2025-12-18T13:08:23.038459",
     "exception": false,
     "start_time": "2025-12-18T13:08:23.024487",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 10.5 Meta-Model AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "dfe339bb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-18T13:08:23.068141Z",
     "iopub.status.busy": "2025-12-18T13:08:23.067841Z",
     "iopub.status.idle": "2025-12-18T13:08:23.231881Z",
     "shell.execute_reply": "2025-12-18T13:08:23.231072Z"
    },
    "papermill": {
     "duration": 0.179873,
     "end_time": "2025-12-18T13:08:23.233027",
     "exception": false,
     "start_time": "2025-12-18T13:08:23.053154",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7295286624473414\n"
     ]
    }
   ],
   "source": [
    "meta_model_auc = roc_auc_score(train_data[target_col], scaled_final_meta_oof_preds)\n",
    "print(meta_model_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae1745d",
   "metadata": {
    "papermill": {
     "duration": 0.013904,
     "end_time": "2025-12-18T13:08:23.261275",
     "exception": false,
     "start_time": "2025-12-18T13:08:23.247371",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 11. Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6bd60b4c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-18T13:08:23.289467Z",
     "iopub.status.busy": "2025-12-18T13:08:23.289251Z",
     "iopub.status.idle": "2025-12-18T13:08:23.845635Z",
     "shell.execute_reply": "2025-12-18T13:08:23.844829Z"
    },
    "papermill": {
     "duration": 0.571866,
     "end_time": "2025-12-18T13:08:23.846731",
     "exception": false,
     "start_time": "2025-12-18T13:08:23.274865",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission file prepared.\n"
     ]
    }
   ],
   "source": [
    "# prepare submission\n",
    "submission = pd.DataFrame({'id': test_data.index, target_col: scaled_final_meta_test_preds})\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print('Submission file prepared.')"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 14272474,
     "sourceId": 91723,
     "sourceType": "competition"
    },
    {
     "databundleVersionId": 15000119,
     "datasetId": 8925440,
     "sourceId": 14205181,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31192,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 29214.497046,
   "end_time": "2025-12-18T13:08:25.381619",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-12-18T05:01:30.884573",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
