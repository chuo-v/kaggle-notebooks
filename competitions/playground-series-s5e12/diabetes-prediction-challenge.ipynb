{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5657ca32",
   "metadata": {
    "papermill": {
     "duration": 0.010984,
     "end_time": "2025-12-26T16:31:36.537120",
     "exception": false,
     "start_time": "2025-12-26T16:31:36.526136",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 1. Overview\n",
    "\n",
    "This is a notebook for training models to submit predictions to the \"Diabetes Prediction Challenge\" Kaggle competition ([playground-series-s5e12](https://www.kaggle.com/competitions/playground-series-s5e12)).\n",
    "\n",
    "Synthetic data is used for this playground competition, and the objective is to, for each patient in the test set, predict the probability that the patient will be diagnosed with diabetes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87392d6f",
   "metadata": {
    "papermill": {
     "duration": 0.009482,
     "end_time": "2025-12-26T16:31:36.556089",
     "exception": false,
     "start_time": "2025-12-26T16:31:36.546607",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 2. Setup\n",
    "\n",
    "## 2.1 Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17805284",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-26T16:31:36.576583Z",
     "iopub.status.busy": "2025-12-26T16:31:36.575903Z",
     "iopub.status.idle": "2025-12-26T16:31:47.842310Z",
     "shell.execute_reply": "2025-12-26T16:31:47.841406Z"
    },
    "papermill": {
     "duration": 11.279739,
     "end_time": "2025-12-26T16:31:47.845016",
     "exception": false,
     "start_time": "2025-12-26T16:31:36.565277",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import copy\n",
    "import lightgbm as lgb\n",
    "import optuna\n",
    "import os\n",
    "import hashlib as hl # for StackingEstimator\n",
    "import inspect # for StackingEstimator\n",
    "import random\n",
    "import warnings\n",
    "from catboost import CatBoostClassifier\n",
    "from enum import Enum\n",
    "from pathlib import Path # for StackingPredictionsRetriever\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.linear_model import LinearRegression # for meta model\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import OrdinalEncoder, StandardScaler\n",
    "from types import FunctionType\n",
    "from xgboost import XGBClassifier, XGBRFClassifier\n",
    "\n",
    "warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "pd.set_option('display.max_colwidth', None) # Display full column content\n",
    "pd.set_option('display.max_rows', None) # Display all rows\n",
    "pd.set_option('display.width', 1000) # Set larger display width"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9dd8c7",
   "metadata": {
    "papermill": {
     "duration": 0.014752,
     "end_time": "2025-12-26T16:31:47.877669",
     "exception": false,
     "start_time": "2025-12-26T16:31:47.862917",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2.2 Reproducibility\n",
    "\n",
    "For reproducibility of results, an arbitrary number will be used for the random seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13ea8f09",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-26T16:31:47.904567Z",
     "iopub.status.busy": "2025-12-26T16:31:47.904020Z",
     "iopub.status.idle": "2025-12-26T16:31:47.953916Z",
     "shell.execute_reply": "2025-12-26T16:31:47.953314Z"
    },
    "papermill": {
     "duration": 0.062389,
     "end_time": "2025-12-26T16:31:47.955306",
     "exception": false,
     "start_time": "2025-12-26T16:31:47.892917",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "RANDOM_SEEDS = [11, 42]\n",
    "random.seed(RANDOM_SEEDS[0])\n",
    "np.random.seed(RANDOM_SEEDS[0])\n",
    "torch.manual_seed(RANDOM_SEEDS[0])\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(RANDOM_SEEDS[0])\n",
    "    torch.cuda.manual_seed_all(RANDOM_SEEDS[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28befb10",
   "metadata": {
    "papermill": {
     "duration": 0.009228,
     "end_time": "2025-12-26T16:31:47.974058",
     "exception": false,
     "start_time": "2025-12-26T16:31:47.964830",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2.3 Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64852c5f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-26T16:31:47.993696Z",
     "iopub.status.busy": "2025-12-26T16:31:47.993172Z",
     "iopub.status.idle": "2025-12-26T16:31:47.996617Z",
     "shell.execute_reply": "2025-12-26T16:31:47.996079Z"
    },
    "papermill": {
     "duration": 0.014552,
     "end_time": "2025-12-26T16:31:47.997658",
     "exception": false,
     "start_time": "2025-12-26T16:31:47.983106",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb98911",
   "metadata": {
    "papermill": {
     "duration": 0.00927,
     "end_time": "2025-12-26T16:31:48.016117",
     "exception": false,
     "start_time": "2025-12-26T16:31:48.006847",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2.4 DataFrames\n",
    "\n",
    "Read the data provided for the competition into dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f173d04",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-26T16:31:48.035734Z",
     "iopub.status.busy": "2025-12-26T16:31:48.035157Z",
     "iopub.status.idle": "2025-12-26T16:31:50.347493Z",
     "shell.execute_reply": "2025-12-26T16:31:50.346578Z"
    },
    "papermill": {
     "duration": 2.32357,
     "end_time": "2025-12-26T16:31:50.348936",
     "exception": false,
     "start_time": "2025-12-26T16:31:48.025366",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "INPUT_DIR = '/kaggle/input'\n",
    "orig_train_data = pd.read_csv(os.path.join(INPUT_DIR, 'playground-series-s5e12/train.csv'))\n",
    "orig_test_data = pd.read_csv(os.path.join(INPUT_DIR, 'playground-series-s5e12/test.csv'))\n",
    "\n",
    "# set index\n",
    "orig_train_data.set_index('id', inplace=True)\n",
    "orig_test_data.set_index('id', inplace=True)\n",
    "\n",
    "# target column\n",
    "target_col = \"diagnosed_diabetes\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff666fe7",
   "metadata": {
    "papermill": {
     "duration": 0.009705,
     "end_time": "2025-12-26T16:31:50.368951",
     "exception": false,
     "start_time": "2025-12-26T16:31:50.359246",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 3. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d3ee08a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-26T16:31:50.389977Z",
     "iopub.status.busy": "2025-12-26T16:31:50.389297Z",
     "iopub.status.idle": "2025-12-26T16:31:50.392542Z",
     "shell.execute_reply": "2025-12-26T16:31:50.392016Z"
    },
    "papermill": {
     "duration": 0.014933,
     "end_time": "2025-12-26T16:31:50.393653",
     "exception": false,
     "start_time": "2025-12-26T16:31:50.378720",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# to skip the generation of plots (e.g. KDE) in this section that take time; set to False to generate the plots \n",
    "SKIP_PLOTS = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1649d004",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-26T16:31:50.414334Z",
     "iopub.status.busy": "2025-12-26T16:31:50.413885Z",
     "iopub.status.idle": "2025-12-26T16:31:50.892491Z",
     "shell.execute_reply": "2025-12-26T16:31:50.891807Z"
    },
    "papermill": {
     "duration": 0.490325,
     "end_time": "2025-12-26T16:31:50.893743",
     "exception": false,
     "start_time": "2025-12-26T16:31:50.403418",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>alcohol_consumption_per_week</th>\n",
       "      <th>physical_activity_minutes_per_week</th>\n",
       "      <th>diet_score</th>\n",
       "      <th>sleep_hours_per_day</th>\n",
       "      <th>screen_time_hours_per_day</th>\n",
       "      <th>bmi</th>\n",
       "      <th>waist_to_hip_ratio</th>\n",
       "      <th>systolic_bp</th>\n",
       "      <th>diastolic_bp</th>\n",
       "      <th>heart_rate</th>\n",
       "      <th>cholesterol_total</th>\n",
       "      <th>hdl_cholesterol</th>\n",
       "      <th>ldl_cholesterol</th>\n",
       "      <th>triglycerides</th>\n",
       "      <th>family_history_diabetes</th>\n",
       "      <th>hypertension_history</th>\n",
       "      <th>cardiovascular_history</th>\n",
       "      <th>diagnosed_diabetes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>700000.000000</td>\n",
       "      <td>700000.000000</td>\n",
       "      <td>700000.000000</td>\n",
       "      <td>700000.000000</td>\n",
       "      <td>700000.000000</td>\n",
       "      <td>700000.000000</td>\n",
       "      <td>700000.000000</td>\n",
       "      <td>700000.000000</td>\n",
       "      <td>700000.000000</td>\n",
       "      <td>700000.000000</td>\n",
       "      <td>700000.000000</td>\n",
       "      <td>700000.000000</td>\n",
       "      <td>700000.000000</td>\n",
       "      <td>700000.000000</td>\n",
       "      <td>700000.000000</td>\n",
       "      <td>700000.000000</td>\n",
       "      <td>700000.000000</td>\n",
       "      <td>700000.000000</td>\n",
       "      <td>700000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>50.359734</td>\n",
       "      <td>2.072411</td>\n",
       "      <td>80.230803</td>\n",
       "      <td>5.963695</td>\n",
       "      <td>7.002200</td>\n",
       "      <td>6.012733</td>\n",
       "      <td>25.874684</td>\n",
       "      <td>0.858766</td>\n",
       "      <td>116.294193</td>\n",
       "      <td>75.440924</td>\n",
       "      <td>70.167749</td>\n",
       "      <td>186.818801</td>\n",
       "      <td>53.823214</td>\n",
       "      <td>102.905854</td>\n",
       "      <td>123.081850</td>\n",
       "      <td>0.149401</td>\n",
       "      <td>0.181990</td>\n",
       "      <td>0.030324</td>\n",
       "      <td>0.623296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>11.655520</td>\n",
       "      <td>1.048189</td>\n",
       "      <td>51.195071</td>\n",
       "      <td>1.463336</td>\n",
       "      <td>0.901907</td>\n",
       "      <td>2.022707</td>\n",
       "      <td>2.860705</td>\n",
       "      <td>0.037980</td>\n",
       "      <td>11.010390</td>\n",
       "      <td>6.825775</td>\n",
       "      <td>6.938722</td>\n",
       "      <td>16.730832</td>\n",
       "      <td>8.266545</td>\n",
       "      <td>19.022416</td>\n",
       "      <td>24.739397</td>\n",
       "      <td>0.356484</td>\n",
       "      <td>0.385837</td>\n",
       "      <td>0.171478</td>\n",
       "      <td>0.484560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>19.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>3.100000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>15.100000</td>\n",
       "      <td>0.680000</td>\n",
       "      <td>91.000000</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>117.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>42.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>49.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>6.400000</td>\n",
       "      <td>4.600000</td>\n",
       "      <td>23.900000</td>\n",
       "      <td>0.830000</td>\n",
       "      <td>108.000000</td>\n",
       "      <td>71.000000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>175.000000</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>89.000000</td>\n",
       "      <td>106.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>50.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>71.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>25.900000</td>\n",
       "      <td>0.860000</td>\n",
       "      <td>116.000000</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>187.000000</td>\n",
       "      <td>54.000000</td>\n",
       "      <td>103.000000</td>\n",
       "      <td>123.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>58.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>96.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.600000</td>\n",
       "      <td>7.400000</td>\n",
       "      <td>27.800000</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>124.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>199.000000</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>116.000000</td>\n",
       "      <td>139.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>89.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>747.000000</td>\n",
       "      <td>9.900000</td>\n",
       "      <td>9.900000</td>\n",
       "      <td>16.500000</td>\n",
       "      <td>38.400000</td>\n",
       "      <td>1.050000</td>\n",
       "      <td>163.000000</td>\n",
       "      <td>104.000000</td>\n",
       "      <td>101.000000</td>\n",
       "      <td>289.000000</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>205.000000</td>\n",
       "      <td>290.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 age  alcohol_consumption_per_week  physical_activity_minutes_per_week     diet_score  sleep_hours_per_day  screen_time_hours_per_day            bmi  waist_to_hip_ratio    systolic_bp   diastolic_bp     heart_rate  cholesterol_total  hdl_cholesterol  ldl_cholesterol  triglycerides  family_history_diabetes  hypertension_history  cardiovascular_history  diagnosed_diabetes\n",
       "count  700000.000000                 700000.000000                       700000.000000  700000.000000        700000.000000              700000.000000  700000.000000       700000.000000  700000.000000  700000.000000  700000.000000      700000.000000    700000.000000    700000.000000  700000.000000            700000.000000         700000.000000           700000.000000       700000.000000\n",
       "mean       50.359734                      2.072411                           80.230803       5.963695             7.002200                   6.012733      25.874684            0.858766     116.294193      75.440924      70.167749         186.818801        53.823214       102.905854     123.081850                 0.149401              0.181990                0.030324            0.623296\n",
       "std        11.655520                      1.048189                           51.195071       1.463336             0.901907                   2.022707       2.860705            0.037980      11.010390       6.825775       6.938722          16.730832         8.266545        19.022416      24.739397                 0.356484              0.385837                0.171478            0.484560\n",
       "min        19.000000                      1.000000                            1.000000       0.100000             3.100000                   0.600000      15.100000            0.680000      91.000000      51.000000      42.000000         117.000000        21.000000        51.000000      31.000000                 0.000000              0.000000                0.000000            0.000000\n",
       "25%        42.000000                      1.000000                           49.000000       5.000000             6.400000                   4.600000      23.900000            0.830000     108.000000      71.000000      65.000000         175.000000        48.000000        89.000000     106.000000                 0.000000              0.000000                0.000000            0.000000\n",
       "50%        50.000000                      2.000000                           71.000000       6.000000             7.000000                   6.000000      25.900000            0.860000     116.000000      75.000000      70.000000         187.000000        54.000000       103.000000     123.000000                 0.000000              0.000000                0.000000            1.000000\n",
       "75%        58.000000                      3.000000                           96.000000       7.000000             7.600000                   7.400000      27.800000            0.880000     124.000000      80.000000      75.000000         199.000000        59.000000       116.000000     139.000000                 0.000000              0.000000                0.000000            1.000000\n",
       "max        89.000000                      9.000000                          747.000000       9.900000             9.900000                  16.500000      38.400000            1.050000     163.000000     104.000000     101.000000         289.000000        90.000000       205.000000     290.000000                 1.000000              1.000000                1.000000            1.000000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig_train_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51f8096e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-26T16:31:50.914986Z",
     "iopub.status.busy": "2025-12-26T16:31:50.914423Z",
     "iopub.status.idle": "2025-12-26T16:31:51.112726Z",
     "shell.execute_reply": "2025-12-26T16:31:51.112002Z"
    },
    "papermill": {
     "duration": 0.20996,
     "end_time": "2025-12-26T16:31:51.113972",
     "exception": false,
     "start_time": "2025-12-26T16:31:50.904012",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>alcohol_consumption_per_week</th>\n",
       "      <th>physical_activity_minutes_per_week</th>\n",
       "      <th>diet_score</th>\n",
       "      <th>sleep_hours_per_day</th>\n",
       "      <th>screen_time_hours_per_day</th>\n",
       "      <th>bmi</th>\n",
       "      <th>waist_to_hip_ratio</th>\n",
       "      <th>systolic_bp</th>\n",
       "      <th>diastolic_bp</th>\n",
       "      <th>heart_rate</th>\n",
       "      <th>cholesterol_total</th>\n",
       "      <th>hdl_cholesterol</th>\n",
       "      <th>ldl_cholesterol</th>\n",
       "      <th>triglycerides</th>\n",
       "      <th>family_history_diabetes</th>\n",
       "      <th>hypertension_history</th>\n",
       "      <th>cardiovascular_history</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>300000.000000</td>\n",
       "      <td>300000.000000</td>\n",
       "      <td>300000.000000</td>\n",
       "      <td>300000.000000</td>\n",
       "      <td>300000.000000</td>\n",
       "      <td>300000.000000</td>\n",
       "      <td>300000.000000</td>\n",
       "      <td>300000.000000</td>\n",
       "      <td>300000.000000</td>\n",
       "      <td>300000.000000</td>\n",
       "      <td>300000.000000</td>\n",
       "      <td>300000.000000</td>\n",
       "      <td>300000.000000</td>\n",
       "      <td>300000.000000</td>\n",
       "      <td>300000.000000</td>\n",
       "      <td>300000.000000</td>\n",
       "      <td>300000.000000</td>\n",
       "      <td>300000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>50.432397</td>\n",
       "      <td>2.089693</td>\n",
       "      <td>92.349087</td>\n",
       "      <td>5.945838</td>\n",
       "      <td>6.997795</td>\n",
       "      <td>6.011278</td>\n",
       "      <td>25.881906</td>\n",
       "      <td>0.859007</td>\n",
       "      <td>116.374117</td>\n",
       "      <td>75.396013</td>\n",
       "      <td>70.048350</td>\n",
       "      <td>187.308620</td>\n",
       "      <td>53.813557</td>\n",
       "      <td>103.416083</td>\n",
       "      <td>123.538480</td>\n",
       "      <td>0.152920</td>\n",
       "      <td>0.184410</td>\n",
       "      <td>0.033110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>11.938741</td>\n",
       "      <td>1.066214</td>\n",
       "      <td>62.187399</td>\n",
       "      <td>1.481068</td>\n",
       "      <td>0.914693</td>\n",
       "      <td>2.060472</td>\n",
       "      <td>2.894289</td>\n",
       "      <td>0.038523</td>\n",
       "      <td>11.252146</td>\n",
       "      <td>6.950340</td>\n",
       "      <td>7.090543</td>\n",
       "      <td>18.413053</td>\n",
       "      <td>8.398126</td>\n",
       "      <td>20.571855</td>\n",
       "      <td>28.965441</td>\n",
       "      <td>0.359911</td>\n",
       "      <td>0.387819</td>\n",
       "      <td>0.178924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>19.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>3.100000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>15.100000</td>\n",
       "      <td>0.690000</td>\n",
       "      <td>91.000000</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>107.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>42.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>6.400000</td>\n",
       "      <td>4.600000</td>\n",
       "      <td>23.900000</td>\n",
       "      <td>0.830000</td>\n",
       "      <td>108.000000</td>\n",
       "      <td>71.000000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>174.000000</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>89.000000</td>\n",
       "      <td>104.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>50.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>25.900000</td>\n",
       "      <td>0.860000</td>\n",
       "      <td>116.000000</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>187.000000</td>\n",
       "      <td>54.000000</td>\n",
       "      <td>103.000000</td>\n",
       "      <td>123.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>59.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>115.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.600000</td>\n",
       "      <td>7.400000</td>\n",
       "      <td>27.800000</td>\n",
       "      <td>0.890000</td>\n",
       "      <td>124.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>117.000000</td>\n",
       "      <td>142.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>89.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>748.000000</td>\n",
       "      <td>9.900000</td>\n",
       "      <td>9.900000</td>\n",
       "      <td>15.900000</td>\n",
       "      <td>38.300000</td>\n",
       "      <td>1.050000</td>\n",
       "      <td>170.000000</td>\n",
       "      <td>104.000000</td>\n",
       "      <td>101.000000</td>\n",
       "      <td>285.000000</td>\n",
       "      <td>91.000000</td>\n",
       "      <td>226.000000</td>\n",
       "      <td>290.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 age  alcohol_consumption_per_week  physical_activity_minutes_per_week     diet_score  sleep_hours_per_day  screen_time_hours_per_day            bmi  waist_to_hip_ratio    systolic_bp   diastolic_bp     heart_rate  cholesterol_total  hdl_cholesterol  ldl_cholesterol  triglycerides  family_history_diabetes  hypertension_history  cardiovascular_history\n",
       "count  300000.000000                 300000.000000                       300000.000000  300000.000000        300000.000000              300000.000000  300000.000000       300000.000000  300000.000000  300000.000000  300000.000000      300000.000000    300000.000000    300000.000000  300000.000000            300000.000000         300000.000000           300000.000000\n",
       "mean       50.432397                      2.089693                           92.349087       5.945838             6.997795                   6.011278      25.881906            0.859007     116.374117      75.396013      70.048350         187.308620        53.813557       103.416083     123.538480                 0.152920              0.184410                0.033110\n",
       "std        11.938741                      1.066214                           62.187399       1.481068             0.914693                   2.060472       2.894289            0.038523      11.252146       6.950340       7.090543          18.413053         8.398126        20.571855      28.965441                 0.359911              0.387819                0.178924\n",
       "min        19.000000                      1.000000                            1.000000       0.100000             3.100000                   0.600000      15.100000            0.690000      91.000000      51.000000      42.000000         107.000000        22.000000        51.000000      31.000000                 0.000000              0.000000                0.000000\n",
       "25%        42.000000                      1.000000                           51.000000       5.000000             6.400000                   4.600000      23.900000            0.830000     108.000000      71.000000      65.000000         174.000000        48.000000        89.000000     104.000000                 0.000000              0.000000                0.000000\n",
       "50%        50.000000                      2.000000                           77.000000       6.000000             7.000000                   6.000000      25.900000            0.860000     116.000000      75.000000      70.000000         187.000000        54.000000       103.000000     123.000000                 0.000000              0.000000                0.000000\n",
       "75%        59.000000                      3.000000                          115.000000       7.000000             7.600000                   7.400000      27.800000            0.890000     124.000000      80.000000      75.000000         200.000000        60.000000       117.000000     142.000000                 0.000000              0.000000                0.000000\n",
       "max        89.000000                      9.000000                          748.000000       9.900000             9.900000                  15.900000      38.300000            1.050000     170.000000     104.000000     101.000000         285.000000        91.000000       226.000000     290.000000                 1.000000              1.000000                1.000000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig_test_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49a26eb4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-26T16:31:51.135668Z",
     "iopub.status.busy": "2025-12-26T16:31:51.135241Z",
     "iopub.status.idle": "2025-12-26T16:31:51.255526Z",
     "shell.execute_reply": "2025-12-26T16:31:51.254954Z"
    },
    "papermill": {
     "duration": 0.132361,
     "end_time": "2025-12-26T16:31:51.256845",
     "exception": false,
     "start_time": "2025-12-26T16:31:51.124484",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "numeric_col_names = orig_train_data.select_dtypes(include='number').columns.to_series()\n",
    "categorical_col_names = orig_train_data.select_dtypes(include='object').columns.to_series()\n",
    "assert numeric_col_names.size + categorical_col_names.size == orig_train_data.shape[1]\n",
    "\n",
    "# drop target column from numeric column names\n",
    "numeric_col_names.drop(target_col, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b65caa88",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-26T16:31:51.278141Z",
     "iopub.status.busy": "2025-12-26T16:31:51.277935Z",
     "iopub.status.idle": "2025-12-26T16:31:51.557069Z",
     "shell.execute_reply": "2025-12-26T16:31:51.556192Z"
    },
    "papermill": {
     "duration": 0.290956,
     "end_time": "2025-12-26T16:31:51.558216",
     "exception": false,
     "start_time": "2025-12-26T16:31:51.267260",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Train data missing values #####\n",
      "age                                   0\n",
      "alcohol_consumption_per_week          0\n",
      "physical_activity_minutes_per_week    0\n",
      "diet_score                            0\n",
      "sleep_hours_per_day                   0\n",
      "screen_time_hours_per_day             0\n",
      "bmi                                   0\n",
      "waist_to_hip_ratio                    0\n",
      "systolic_bp                           0\n",
      "diastolic_bp                          0\n",
      "heart_rate                            0\n",
      "cholesterol_total                     0\n",
      "hdl_cholesterol                       0\n",
      "ldl_cholesterol                       0\n",
      "triglycerides                         0\n",
      "gender                                0\n",
      "ethnicity                             0\n",
      "education_level                       0\n",
      "income_level                          0\n",
      "smoking_status                        0\n",
      "employment_status                     0\n",
      "family_history_diabetes               0\n",
      "hypertension_history                  0\n",
      "cardiovascular_history                0\n",
      "diagnosed_diabetes                    0\n",
      "dtype: int64\n",
      "\n",
      "##### Test data missing values #####\n",
      "age                                   0\n",
      "alcohol_consumption_per_week          0\n",
      "physical_activity_minutes_per_week    0\n",
      "diet_score                            0\n",
      "sleep_hours_per_day                   0\n",
      "screen_time_hours_per_day             0\n",
      "bmi                                   0\n",
      "waist_to_hip_ratio                    0\n",
      "systolic_bp                           0\n",
      "diastolic_bp                          0\n",
      "heart_rate                            0\n",
      "cholesterol_total                     0\n",
      "hdl_cholesterol                       0\n",
      "ldl_cholesterol                       0\n",
      "triglycerides                         0\n",
      "gender                                0\n",
      "ethnicity                             0\n",
      "education_level                       0\n",
      "income_level                          0\n",
      "smoking_status                        0\n",
      "employment_status                     0\n",
      "family_history_diabetes               0\n",
      "hypertension_history                  0\n",
      "cardiovascular_history                0\n",
      "dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for (dataset_name, dataset) in [('Train data', orig_train_data), ('Test data', orig_test_data)]:\n",
    "    print(f\"##### {dataset_name} missing values #####\")\n",
    "    print(dataset.isnull().sum())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "097bbe8b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-26T16:31:51.579411Z",
     "iopub.status.busy": "2025-12-26T16:31:51.579187Z",
     "iopub.status.idle": "2025-12-26T16:31:51.840303Z",
     "shell.execute_reply": "2025-12-26T16:31:51.839605Z"
    },
    "papermill": {
     "duration": 0.272881,
     "end_time": "2025-12-26T16:31:51.841403",
     "exception": false,
     "start_time": "2025-12-26T16:31:51.568522",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Train data categorical cols unique values #####\n",
      "gender:\n",
      "['Female' 'Male' 'Other']\n",
      "ethnicity:\n",
      "['Hispanic' 'White' 'Asian' 'Black' 'Other']\n",
      "education_level:\n",
      "['Highschool' 'Graduate' 'Postgraduate' 'No formal']\n",
      "income_level:\n",
      "['Lower-Middle' 'Upper-Middle' 'Low' 'Middle' 'High']\n",
      "smoking_status:\n",
      "['Current' 'Never' 'Former']\n",
      "employment_status:\n",
      "['Employed' 'Retired' 'Student' 'Unemployed']\n",
      "\n",
      "##### Test data categorical cols unique values #####\n",
      "gender:\n",
      "['Female' 'Male' 'Other']\n",
      "ethnicity:\n",
      "['White' 'Hispanic' 'Black' 'Asian' 'Other']\n",
      "education_level:\n",
      "['Highschool' 'Graduate' 'Postgraduate' 'No formal']\n",
      "income_level:\n",
      "['Middle' 'Low' 'Lower-Middle' 'Upper-Middle' 'High']\n",
      "smoking_status:\n",
      "['Former' 'Never' 'Current']\n",
      "employment_status:\n",
      "['Employed' 'Unemployed' 'Retired' 'Student']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for (dataset_name, dataset) in [('Train data', orig_train_data), ('Test data', orig_test_data)]:\n",
    "    print(f\"##### {dataset_name} categorical cols unique values #####\")\n",
    "    for categorical_col_name in categorical_col_names:\n",
    "        print(f\"{categorical_col_name}:\")\n",
    "        print(dataset[categorical_col_name].unique())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c28b510b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-26T16:31:51.863117Z",
     "iopub.status.busy": "2025-12-26T16:31:51.862572Z",
     "iopub.status.idle": "2025-12-26T16:31:51.866884Z",
     "shell.execute_reply": "2025-12-26T16:31:51.866182Z"
    },
    "papermill": {
     "duration": 0.016029,
     "end_time": "2025-12-26T16:31:51.868032",
     "exception": false,
     "start_time": "2025-12-26T16:31:51.852003",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# KDE plots of target variable and numerical features\n",
    "if not SKIP_PLOTS:\n",
    "    plt.figure(figsize=(12, 24))\n",
    "    kdeplot_col_names = [target_col]\n",
    "    kdeplot_col_names.extend(numeric_col_names)\n",
    "    for i, col in enumerate(kdeplot_col_names, start=1):\n",
    "        plt.subplot(10, 2, i)\n",
    "        sns.kdeplot(data=orig_train_data, x=col, fill=True)\n",
    "        plt.tight_layout()\n",
    "        plt.title(f\"KDE plot of {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1f7725c2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-26T16:31:51.888564Z",
     "iopub.status.busy": "2025-12-26T16:31:51.888360Z",
     "iopub.status.idle": "2025-12-26T16:31:51.892332Z",
     "shell.execute_reply": "2025-12-26T16:31:51.891658Z"
    },
    "papermill": {
     "duration": 0.015382,
     "end_time": "2025-12-26T16:31:51.893441",
     "exception": false,
     "start_time": "2025-12-26T16:31:51.878059",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not SKIP_PLOTS:\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.heatmap(\n",
    "        orig_train_data[numeric_col_names].corr(),\n",
    "        cmap='Reds',\n",
    "        annot=True,\n",
    "        linewidths=2,\n",
    "        fmt='.2f',\n",
    "        vmin=-1,\n",
    "        vmax=1\n",
    "    )\n",
    "    plt.title('Correlation Matrix of Numerical Features', fontsize=18, pad=20)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d876867",
   "metadata": {
    "papermill": {
     "duration": 0.009826,
     "end_time": "2025-12-26T16:31:51.913205",
     "exception": false,
     "start_time": "2025-12-26T16:31:51.903379",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 4. Data Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "80116828",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-26T16:31:51.933990Z",
     "iopub.status.busy": "2025-12-26T16:31:51.933492Z",
     "iopub.status.idle": "2025-12-26T16:31:52.091556Z",
     "shell.execute_reply": "2025-12-26T16:31:52.090696Z"
    },
    "papermill": {
     "duration": 0.170053,
     "end_time": "2025-12-26T16:31:52.093073",
     "exception": false,
     "start_time": "2025-12-26T16:31:51.923020",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data = orig_train_data.copy()\n",
    "test_data = orig_test_data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3175ded8",
   "metadata": {
    "papermill": {
     "duration": 0.009777,
     "end_time": "2025-12-26T16:31:52.113282",
     "exception": false,
     "start_time": "2025-12-26T16:31:52.103505",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 4.1 Ordinal Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "558a1f12",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-26T16:31:52.133874Z",
     "iopub.status.busy": "2025-12-26T16:31:52.133616Z",
     "iopub.status.idle": "2025-12-26T16:31:52.997562Z",
     "shell.execute_reply": "2025-12-26T16:31:52.996659Z"
    },
    "papermill": {
     "duration": 0.875665,
     "end_time": "2025-12-26T16:31:52.998702",
     "exception": false,
     "start_time": "2025-12-26T16:31:52.123037",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "education_level_encoded:\n",
      "{'No formal': 0, 'Highschool': 1, 'Graduate': 2, 'Postgraduate': 3}\n",
      "income_level_encoded:\n",
      "{'Low': 0, 'Lower-Middle': 1, 'Middle': 2, 'Upper-Middle': 3, 'High': 4}\n",
      "smoking_status_encoded:\n",
      "{'Never': 0, 'Former': 1, 'Current': 2}\n"
     ]
    }
   ],
   "source": [
    "# education level\n",
    "education_level_encoder = OrdinalEncoder(categories=[['No formal', 'Highschool', 'Graduate', 'Postgraduate']])\n",
    "train_data['education_level_encoded'] = education_level_encoder.fit_transform(train_data[['education_level']])\n",
    "test_data['education_level_encoded'] = education_level_encoder.fit_transform(test_data[['education_level']])\n",
    "\n",
    "# income level\n",
    "income_level_encoder = OrdinalEncoder(categories=[['Low', 'Lower-Middle','Middle', 'Upper-Middle', 'High']])\n",
    "train_data['income_level_encoded'] = income_level_encoder.fit_transform(train_data[['income_level']])\n",
    "test_data['income_level_encoded'] = income_level_encoder.fit_transform(test_data[['income_level']])\n",
    "\n",
    "# smoking status\n",
    "smoking_status_encoder = OrdinalEncoder(categories=[['Never', 'Former', 'Current']])\n",
    "train_data['smoking_status_encoded'] = smoking_status_encoder.fit_transform(train_data[['smoking_status']])\n",
    "test_data['smoking_status_encoded'] = smoking_status_encoder.fit_transform(test_data[['smoking_status']])\n",
    "\n",
    "# drop original cols\n",
    "for col in ['income_level', 'education_level', 'smoking_status']:\n",
    "    train_data.drop(col, axis=1, inplace=True)\n",
    "    test_data.drop(col, axis=1, inplace=True)\n",
    "\n",
    "# print out value maps to check assigned values are as expected\n",
    "for (encoded_col_name, encoder) in [\n",
    "    ('education_level_encoded', education_level_encoder),\n",
    "    ('income_level_encoded', income_level_encoder),\n",
    "    ('smoking_status_encoded', smoking_status_encoder),\n",
    "]:\n",
    "    categories = encoder.categories_[0]\n",
    "    value_map = { category: i for i, category in enumerate(categories) }\n",
    "    print(f\"{encoded_col_name}:\\n{value_map}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c4065e",
   "metadata": {
    "papermill": {
     "duration": 0.011507,
     "end_time": "2025-12-26T16:31:53.020962",
     "exception": false,
     "start_time": "2025-12-26T16:31:53.009455",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 4.2 Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e93dfd84",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-26T16:31:53.042022Z",
     "iopub.status.busy": "2025-12-26T16:31:53.041746Z",
     "iopub.status.idle": "2025-12-26T16:31:53.057664Z",
     "shell.execute_reply": "2025-12-26T16:31:53.056938Z"
    },
    "papermill": {
     "duration": 0.027793,
     "end_time": "2025-12-26T16:31:53.058863",
     "exception": false,
     "start_time": "2025-12-26T16:31:53.031070",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fix_blood_pressure(df):\n",
    "    mask = df['diastolic_bp'] > df['systolic_bp']\n",
    "    df.loc[mask, ['systolic_bp', 'diastolic_bp']] = (\n",
    "        df.loc[mask, ['diastolic_bp', 'systolic_bp']].values\n",
    "    )\n",
    "    return df\n",
    "\n",
    "train_data = fix_blood_pressure(train_data)\n",
    "test_data = fix_blood_pressure(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2b4bcd",
   "metadata": {
    "papermill": {
     "duration": 0.010452,
     "end_time": "2025-12-26T16:31:53.079829",
     "exception": false,
     "start_time": "2025-12-26T16:31:53.069377",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 4.3 Feature Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "97bd82c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-26T16:31:53.101867Z",
     "iopub.status.busy": "2025-12-26T16:31:53.101594Z",
     "iopub.status.idle": "2025-12-26T16:31:53.112487Z",
     "shell.execute_reply": "2025-12-26T16:31:53.111824Z"
    },
    "papermill": {
     "duration": 0.023617,
     "end_time": "2025-12-26T16:31:53.113590",
     "exception": false,
     "start_time": "2025-12-26T16:31:53.089973",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_generated_features(df):\n",
    "    # log transforms for skewed data\n",
    "    for col in ['triglycerides', 'ldl_cholesterol', 'cholesterol_total']:\n",
    "        df[f'log_{col}'] = np.log1p(df[col])\n",
    "\n",
    "    # medical ratios & interactions\n",
    "    df['cholesterol_ratio'] = df['cholesterol_total'] / (df['hdl_cholesterol'] + 1e-5)\n",
    "    df['ldl_hdl_ratio'] = df['ldl_cholesterol'] / (df['hdl_cholesterol'] + 1e-5)\n",
    "    df['pulse_pressure'] = df['systolic_bp'] - df['diastolic_bp']\n",
    "    df['mean_arterial_pressure'] = (df['systolic_bp'] + 2 * df['diastolic_bp']) / 3\n",
    "    df['age_x_bmi'] = df['age'] * df['bmi']\n",
    "    df['waist_x_bmi'] = df['waist_to_hip_ratio'] * df['bmi']\n",
    "    df['family_history_diabetes_x_log_triglycerides'] = df['family_history_diabetes'] * df['log_triglycerides']\n",
    "    df['hypertension_history_x_systolic_bp'] = df['hypertension_history'] * df['systolic_bp']\n",
    "    df['activity_x_diet'] = df['physical_activity_minutes_per_week'] * df['diet_score']\n",
    "    df['atherogenic_index'] = np.log((df['triglycerides'] / (df['hdl_cholesterol'] + 1e-5)) + 1e-5)\n",
    "    df['non_hdl_cholesterol'] = df['cholesterol_total'] - df['hdl_cholesterol']\n",
    "    df['map_x_bmi'] = df['mean_arterial_pressure'] * df['bmi']\n",
    "    df['lipid_accumulation_proxy'] = df['waist_to_hip_ratio'] * df['log_triglycerides']\n",
    "    df['visceral_adiposity_proxy'] = (df['bmi'] * df['triglycerides']) / (df['hdl_cholesterol'] + 1e-5)\n",
    "\n",
    "    # squared\n",
    "    df['age_sq'] = df['age'] ** 2\n",
    "    df['bmi_sq'] = df['bmi'] ** 2\n",
    "    df['waist_to_hip_ratio_sq'] = df['waist_to_hip_ratio'] ** 2\n",
    "    df['systolic_bp_sq'] = df['systolic_bp'] ** 2\n",
    "\n",
    "    # risk grouping\n",
    "    df['comorbidity_count'] = (\n",
    "        df['hypertension_history'] + df['cardiovascular_history'] + df['family_history_diabetes']\n",
    "    )\n",
    "\n",
    "    # binning\n",
    "    df['bmi_cat'] = pd.cut(df['bmi'], bins=[-1, 25, 30, 100], labels=[0, 1, 2]).astype(int)\n",
    "    df['bp_cat'] = pd.cut(\n",
    "        df['systolic_bp'], \n",
    "        bins=[-1, 120, 130, 140, 300], # AHA Guidelines: Normal < 120, Elevated 120-129, Stage 1 130-139, Stage 2 >= 140\n",
    "        labels=[0, 1, 2, 3] # Normal, Elevated, Stage 1, Stage 2\n",
    "    ).astype(int)\n",
    "    df['cholesterol_cat'] = pd.cut(\n",
    "        df['cholesterol_total'], \n",
    "        bins=[-1, 200, 240, 1000], # ATP III Guidelines: Optimal < 200, Borderline 200-239, High >= 240\n",
    "        labels=[0, 1, 2]\n",
    "    ).astype(int)\n",
    "    df['hdl_cat'] = pd.cut(\n",
    "        df['hdl_cholesterol'], \n",
    "        bins=[-1, 40, 60, 200], # Inverted Risk: Risk < 40, Normal 40-60, Protective > 60\n",
    "        labels=[2, 1, 0] # 2 is worst (Low HDL)\n",
    "    ).astype(int)\n",
    "    df['ldl_cat'] = pd.cut(\n",
    "        df['ldl_cholesterol'], \n",
    "        bins=[-1, 100, 130, 160, 190, 1000], # ATP III Guidelines: Optimal < 100, Near Optimal 100-129, Borderline High 130-159, High 160-189, Very High >= 190\n",
    "        labels=[0, 1, 2, 3, 4] # 0 is best (Optimal), 4 is worst (Very High)\n",
    "    ).astype(int)\n",
    "\n",
    "    # quantile binning\n",
    "    quantile_cols = [\n",
    "        'triglycerides', 'waist_to_hip_ratio', 'bmi', 'mean_arterial_pressure'\n",
    "    ]\n",
    "    for col in quantile_cols:\n",
    "        df[f'{col}_decile'] = pd.qcut(df[col], q=10, labels=False, duplicates='drop').astype(int)\n",
    "\n",
    "    # relative BMI, BP and cholesterol\n",
    "    df['age_decade'] = (df['age'] // 10).astype(int)\n",
    "    for group_col in ['age_decade', 'hypertension_history']:\n",
    "        for num_col in ['bmi', 'systolic_bp', 'cholesterol_total']:\n",
    "            group_means = df.groupby(group_col)[num_col].transform('mean')\n",
    "            df[f'{num_col}_relative_to_{group_col}'] = df[num_col] - group_means\n",
    "\n",
    "    # bin interactions\n",
    "    df['age_bp_interaction'] = df['age_decade'].astype(str) + '_' + df['bp_cat'].astype(str)\n",
    "    df['age_bp_interaction'] = df['age_bp_interaction'].astype('category').cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cf7843b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-26T16:31:53.134806Z",
     "iopub.status.busy": "2025-12-26T16:31:53.134279Z",
     "iopub.status.idle": "2025-12-26T16:31:53.139488Z",
     "shell.execute_reply": "2025-12-26T16:31:53.138809Z"
    },
    "papermill": {
     "duration": 0.016816,
     "end_time": "2025-12-26T16:31:53.140485",
     "exception": false,
     "start_time": "2025-12-26T16:31:53.123669",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_kmeans_features(train_df, test_df, n_clusters):\n",
    "    features_to_cluster = [\n",
    "        'age', 'bmi', 'mean_arterial_pressure', 'cholesterol_ratio', 'log_triglycerides'\n",
    "    ]\n",
    "    \n",
    "    combined = pd.concat([train_df[features_to_cluster], test_df[features_to_cluster]], axis=0)\n",
    "    scaler = StandardScaler()\n",
    "    scaled_data = scaler.fit_transform(combined)\n",
    "    \n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=RANDOM_SEEDS[0], n_init=10)\n",
    "    clusters = kmeans.fit_predict(scaled_data)\n",
    "\n",
    "    train_dists = kmeans.transform(scaled_data[:len(train_df)])\n",
    "    test_dists = kmeans.transform(scaled_data[len(train_df):])\n",
    "\n",
    "    train_df['cluster_label'] = clusters[:len(train_df)].astype(object)\n",
    "    test_df['cluster_label'] = clusters[len(train_df):].astype(object)\n",
    "\n",
    "    for i in range(n_clusters):\n",
    "        train_df[f'dist_to_cluster_{i}'] = train_dists[:, i]\n",
    "        test_df[f'dist_to_cluster_{i}'] = test_dists[:, i]\n",
    "\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "66108043",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-26T16:31:53.161651Z",
     "iopub.status.busy": "2025-12-26T16:31:53.161218Z",
     "iopub.status.idle": "2025-12-26T16:32:08.026150Z",
     "shell.execute_reply": "2025-12-26T16:32:08.025520Z"
    },
    "papermill": {
     "duration": 14.876974,
     "end_time": "2025-12-26T16:32:08.027519",
     "exception": false,
     "start_time": "2025-12-26T16:31:53.150545",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# add generated features\n",
    "add_generated_features(train_data)\n",
    "add_generated_features(test_data)\n",
    "\n",
    "# apply clustering\n",
    "train_data, test_data = add_kmeans_features(train_data, test_data, n_clusters=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ab277a6e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-26T16:32:08.049373Z",
     "iopub.status.busy": "2025-12-26T16:32:08.048966Z",
     "iopub.status.idle": "2025-12-26T16:32:08.054046Z",
     "shell.execute_reply": "2025-12-26T16:32:08.053331Z"
    },
    "papermill": {
     "duration": 0.017059,
     "end_time": "2025-12-26T16:32:08.055146",
     "exception": false,
     "start_time": "2025-12-26T16:32:08.038087",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['age', 'alcohol_consumption_per_week', 'physical_activity_minutes_per_week', 'diet_score', 'sleep_hours_per_day', 'screen_time_hours_per_day', 'bmi', 'waist_to_hip_ratio', 'systolic_bp', 'diastolic_bp', 'heart_rate', 'cholesterol_total', 'hdl_cholesterol', 'ldl_cholesterol', 'triglycerides', 'gender', 'ethnicity', 'employment_status', 'family_history_diabetes', 'hypertension_history', 'cardiovascular_history', 'diagnosed_diabetes', 'education_level_encoded', 'income_level_encoded', 'smoking_status_encoded', 'log_triglycerides', 'log_ldl_cholesterol', 'log_cholesterol_total', 'cholesterol_ratio', 'ldl_hdl_ratio', 'pulse_pressure', 'mean_arterial_pressure', 'age_x_bmi', 'waist_x_bmi', 'family_history_diabetes_x_log_triglycerides', 'hypertension_history_x_systolic_bp', 'activity_x_diet', 'atherogenic_index', 'non_hdl_cholesterol', 'map_x_bmi', 'lipid_accumulation_proxy', 'visceral_adiposity_proxy', 'age_sq', 'bmi_sq', 'waist_to_hip_ratio_sq', 'systolic_bp_sq', 'comorbidity_count',\n",
       "       'bmi_cat', 'bp_cat', 'cholesterol_cat', 'hdl_cat', 'ldl_cat', 'triglycerides_decile', 'waist_to_hip_ratio_decile', 'bmi_decile', 'mean_arterial_pressure_decile', 'age_decade', 'bmi_relative_to_age_decade', 'systolic_bp_relative_to_age_decade', 'cholesterol_total_relative_to_age_decade', 'bmi_relative_to_hypertension_history', 'systolic_bp_relative_to_hypertension_history', 'cholesterol_total_relative_to_hypertension_history', 'age_bp_interaction', 'cluster_label', 'dist_to_cluster_0', 'dist_to_cluster_1', 'dist_to_cluster_2', 'dist_to_cluster_3', 'dist_to_cluster_4', 'dist_to_cluster_5', 'dist_to_cluster_6'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d4e2c8",
   "metadata": {
    "papermill": {
     "duration": 0.009989,
     "end_time": "2025-12-26T16:32:08.075229",
     "exception": false,
     "start_time": "2025-12-26T16:32:08.065240",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 4.4 Remaining Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "78dddd9f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-26T16:32:08.096411Z",
     "iopub.status.busy": "2025-12-26T16:32:08.096003Z",
     "iopub.status.idle": "2025-12-26T16:32:08.635577Z",
     "shell.execute_reply": "2025-12-26T16:32:08.634739Z"
    },
    "papermill": {
     "duration": 0.551862,
     "end_time": "2025-12-26T16:32:08.636973",
     "exception": false,
     "start_time": "2025-12-26T16:32:08.085111",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cat_features = train_data.drop(target_col, axis=1).select_dtypes(include='object').columns.to_list()\n",
    "if len(cat_features) > 0:\n",
    "    for col in cat_features:\n",
    "        train_data[col] = train_data[col].astype('category')\n",
    "        test_data[col] = test_data[col].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3bc8b9f",
   "metadata": {
    "papermill": {
     "duration": 0.01011,
     "end_time": "2025-12-26T16:32:08.657608",
     "exception": false,
     "start_time": "2025-12-26T16:32:08.647498",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 5. Stacking Initial Setup\n",
    "\n",
    "We'll use stacking, an [ensemble learning](https://en.wikipedia.org/wiki/Ensemble_learning) strategy, to generate the predictions. As we'll need to gather predictions from various base models (a.k.a. level-0 models) to feed as input features to a meta model (a.k.a. level-1 model), in order to streamline the process of experimenting with different combinations of base models, some helper classes will be defined in this section. These classes can also be found [here](https://github.com/chuo-v/machine-learning-utils/blob/master/ensemble-learning/stacking/stacking_predictions_retriever.py) at one of my GitHub repositories used to organize some utilities I implemented for machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "84b15f26",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-26T16:32:08.679335Z",
     "iopub.status.busy": "2025-12-26T16:32:08.678879Z",
     "iopub.status.idle": "2025-12-26T16:32:08.704067Z",
     "shell.execute_reply": "2025-12-26T16:32:08.703485Z"
    },
    "papermill": {
     "duration": 0.037547,
     "end_time": "2025-12-26T16:32:08.705122",
     "exception": false,
     "start_time": "2025-12-26T16:32:08.667575",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class StackingEstimator:\n",
    "    \"\"\"\n",
    "    A class representing an estimator that will be used for stacking, an ensemble learning strategy.\n",
    "\n",
    "    Intended to be used in conjunction with the `StackingPredictionsRetriever` class, which helps\n",
    "    retrieve predictions for multiple instances of `StackingEstimator`; as the predictions are saved\n",
    "    in files, on subsequent requests to retrieve predictions, even as the set of estimators has been\n",
    "    modified, the `StackingPredictionsRetriever` class can determine the predictions of estimators\n",
    "    that are non-stale and available (if any) by using the `get_hash` method of the `StackingEstimator`\n",
    "    class to determine the relevance and staleness of any saved predictions.\n",
    "\n",
    "    Proper usage of this class requires one important condition to be satisfied: the predictions made\n",
    "    using the estimator are determinstic, i.e. they are exactly the same everytime the estimator is\n",
    "    run with the same inputs (`name`, `params_dict`, `feature_names`, `get_predictions`).\n",
    "    \"\"\"\n",
    "    name = \"\"\n",
    "    params_dict = {}\n",
    "    feature_names = []\n",
    "    get_predictions = lambda: None\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        name: str,\n",
    "        feature_names: [str],\n",
    "        params_dict: {},\n",
    "        get_preds: FunctionType\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializes a new instance of `StackingEstimator`.\n",
    "\n",
    "        :param name:\n",
    "            A string representing a name for the estimator. It is used for the column names of\n",
    "            the training and test predictions for each estimator, and is also used as an input\n",
    "            to calculate a hash value for the estimator. It is recommended to use a different\n",
    "            name from the names used for other estimators passed to `StackingPredictionsRetriever`.\n",
    "        :param feature_names:\n",
    "            A list of strings representing the names of the features that will be used for the\n",
    "            estimator. It will be passed as an argument to `get_preds`. Internally, it is only\n",
    "            used as an input to calculate a hash value for the estimator.\n",
    "        :param params_dict:\n",
    "            A dictionary of parameters that will be specified for the estimator. It will be\n",
    "            passed as an argument to `get_preds`. Internally, it is only used as an input\n",
    "            to calculate a hash value for the estimator.\n",
    "        :param get_preds:\n",
    "            A function for getting the predictions for the estimator. It should only take two\n",
    "            arguments: 'params_dict' and 'feature_names', and should return predictions for\n",
    "            the training and test data (in that order) as a tuple of two `pandas.Series`.\n",
    "        \"\"\"\n",
    "        # parameter check\n",
    "        if not isinstance(name, str):\n",
    "            raise ValueError(\"`name` argument should be of type `str`\")\n",
    "        if not isinstance(feature_names, list):\n",
    "            raise ValueError(f\"`feature_names` argument for estimator \\\"{name}\\\" should be of type `list`\")\n",
    "        elif not all(isinstance(feature_name, str) for feature_name in feature_names):\n",
    "            raise ValueError(f\"`feature_names` argument for estimator \\\"{name}\\\" should only contain instances of `str`\")\n",
    "        if not isinstance(params_dict, dict):\n",
    "            raise ValueError(f\"`params_dict` argument for estimator \\\"{name}\\\" should be of type `dict`\")\n",
    "        get_preds_params = inspect.signature(get_preds).parameters.values()\n",
    "        get_preds_param_names = [param.name for param in get_preds_params]\n",
    "        if len(get_preds_param_names) != 2:\n",
    "            raise ValueError(f\"`get_preds` function for estimator \\\"{name}\\\" should take two arguments\")\n",
    "        elif \"params_dict\" not in get_preds_param_names:\n",
    "            raise ValueError(f\"`get_preds` function for estimator \\\"{name}\\\" should take a \\\"params_dict\\\" argument\")\n",
    "        elif \"feature_names\" not in get_preds_param_names:\n",
    "            raise ValueError(f\"`get_preds` function for estimator \\\"{name}\\\" should take a \\\"feature_names\\\" argument\")\n",
    "\n",
    "        self.name = name\n",
    "        self.feature_names = feature_names\n",
    "        self.params_dict = params_dict\n",
    "        self.get_preds = get_preds\n",
    "\n",
    "    def get_hash_value(self):\n",
    "        \"\"\"\n",
    "        Calculates and returns a hash value for the estimator using\n",
    "        `name`, `feature_names` and `params_dict` as inputs.\n",
    "        \"\"\"\n",
    "        feature_names_str = \"_\".join(sorted(self.feature_names))\n",
    "        params_dict_str = \"_\".join(f\"{key}-{value}\" for (key, value) in sorted(self.params_dict.items()))\n",
    "        hash_input_str = \"_\".join([self.name, feature_names_str, params_dict_str])\n",
    "        md5_hash = hl.md5(hash_input_str.encode('utf-8')).hexdigest()\n",
    "        return md5_hash\n",
    "\n",
    "class StackingPredictionsRetriever:\n",
    "    \"\"\"\n",
    "    A class for streamlining stacking (an ensemble learning strategy) that saves predictions\n",
    "    from estimators to file so that when trying out different combinations of (base) estimators,\n",
    "    the predictions that are not stale can be reused, saving the time of having the estimators\n",
    "    make predictions again.\n",
    "\n",
    "    Intended to be used in conjunction with the `StackingEstimator` class. The `hash_value` of\n",
    "    `StackingEstimator` is used to determine the staleness and relevance of the predictions for\n",
    "    an estimator. The implementation for making predictions using an estimator needs to be\n",
    "    provided as a function to `get_preds` for `StackingEstimator`; when predictions need to be\n",
    "    made using an estimator, this class will call `get_preds` for the `StackingEstimator` instance.\n",
    "\n",
    "    Proper usage of this class requires one important condition to be satisfied: the predictions made\n",
    "    using the estimators are determinstic, i.e. they are exactly the same everytime a\n",
    "    `StackingEstimator` instance is run with the same inputs.\n",
    "    \"\"\"\n",
    "    estimators = []\n",
    "    working_dir_path = \"\"\n",
    "    train_preds_filename = \"\"\n",
    "    test_preds_filename = \"\"\n",
    "    preds_save_interval = 0\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        estimators: [StackingEstimator],\n",
    "        working_dir_path: str,\n",
    "        train_preds_filename: str = \"train_preds\",\n",
    "        test_preds_filename: str = \"test_preds\",\n",
    "        preds_save_interval: int = 5,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializes a new instance of `StackingPredictionsRetriever`.\n",
    "\n",
    "        :param estimators:\n",
    "            A list of `StackingEstimator` instances for which the class will retrieve predictions.\n",
    "        :param working_dir_path:\n",
    "            The path for the working directory where the files with predictions will be saved.\n",
    "        :param train_preds_filename:\n",
    "            The name of the file in which predictions for the training set will be stored.\n",
    "        :param test_preds_filename:\n",
    "            The name of the file in which predictions for the test set will be stored.\n",
    "        :param preds_save_interval:\n",
    "            An integer which specifies the interval at which predictions will be saved when\n",
    "            `get_preds` is called, corresponding to the number of estimators whose predictions\n",
    "            have been retrieved since the predictions were previously saved. Any estimators\n",
    "            whose predictions are not stale and therefore were not required to make predictions\n",
    "            again are not included in this number.\n",
    "        \"\"\"\n",
    "        # parameter check\n",
    "        if not isinstance(estimators, list):\n",
    "            raise ValueError(\"`estimators` must be passed as a list\")\n",
    "        if not all(isinstance(e, StackingEstimator) for e in estimators):\n",
    "            raise ValueError(\"`estimators` should only contain instances of `StackingEstimator`\")\n",
    "        if not isinstance(working_dir_path, str):\n",
    "            raise ValueError(\"`working_dir_path` argument should be of type `str`\")\n",
    "        if not isinstance(preds_save_interval, int):\n",
    "            raise ValueError(\"`preds_save_interval` argument should be of type `int`\")\n",
    "\n",
    "        self.estimators = estimators\n",
    "        self.working_dir_path = working_dir_path\n",
    "        self.train_preds_filename = train_preds_filename\n",
    "        self.test_preds_filename = test_preds_filename\n",
    "        self.preds_save_interval = preds_save_interval\n",
    "\n",
    "    def get_train_preds_file_path(self):\n",
    "        \"\"\"\n",
    "        Returns the file path for storing predictions for training data.\n",
    "        \"\"\"\n",
    "        return Path(f\"{self.working_dir_path}/{self.train_preds_filename}.csv\")\n",
    "\n",
    "    def get_test_preds_file_path(self):\n",
    "        \"\"\"\n",
    "        Returns the file path for storing predictions for test data.\n",
    "        \"\"\"\n",
    "        return Path(f\"{self.working_dir_path}/{self.test_preds_filename}.csv\")\n",
    "\n",
    "    def get_current_train_and_test_preds(self):\n",
    "        \"\"\"\n",
    "        Returns the current predictions for training and test data (in that order)\n",
    "        as a tuple of two `pandas.DataFrame`.\n",
    "\n",
    "        The predictions are attempted to be retrieved from the file paths returned\n",
    "        by `get_train_preds_file_path` and `get_test_preds_file_path`; if there are\n",
    "        any issues with doing so (e.g. file does not exist, dataframe is empty),\n",
    "        empty dataframes will be returned instead.\n",
    "        In the case an `pandas.errors.EmptyDataError` exception is raised when\n",
    "        reading from a file, the corresponding file will be removed.\n",
    "        \"\"\"\n",
    "        curr_train_preds = pd.DataFrame()\n",
    "        curr_test_preds = pd.DataFrame()\n",
    "        train_preds_file_path = self.get_train_preds_file_path()\n",
    "        test_preds_file_path = self.get_test_preds_file_path()\n",
    "\n",
    "        if train_preds_file_path.is_file():\n",
    "            try:\n",
    "                curr_train_preds = pd.read_csv(train_preds_file_path)\n",
    "            except pd.errors.EmptyDataError:\n",
    "                train_preds_file_path.unlink()\n",
    "        if test_preds_file_path.is_file():\n",
    "            try:\n",
    "                curr_test_preds = pd.read_csv(test_preds_file_path)\n",
    "            except pd.errors.EmptyDataError:\n",
    "                test_preds_file_path.unlink()\n",
    "\n",
    "        return curr_train_preds, curr_test_preds\n",
    "\n",
    "    def get_preds(self):\n",
    "        \"\"\"\n",
    "        Retrieves predictions from all estimators in `estimators`, storing them in\n",
    "        two files at the file paths specified by `working_dir_path`,\n",
    "        `train_preds_filename` and `test_preds_filename`.\n",
    "\n",
    "        If non-stale (relevant) predictions are found for an estimator, retrieval\n",
    "        of predictions by calling `get_preds` on the estimator will be skipped,\n",
    "        and the existing predictions for the estimator will be kept.\n",
    "        \"\"\"\n",
    "        print(\"[INFO] Getting predictions..\")\n",
    "        curr_train_preds, curr_test_preds = self.get_current_train_and_test_preds()\n",
    "\n",
    "        preds_retrieved_count = 0\n",
    "        num_preds_retrieved_but_not_yet_saved = 0\n",
    "        estimators_skipped = []\n",
    "\n",
    "        for estimator in self.estimators:\n",
    "            estimator_hash_value = estimator.get_hash_value()\n",
    "            estimator_name = f\"{estimator.name} ({estimator_hash_value})\"\n",
    "\n",
    "            # skip retrieving predictions for estimator if non-stale predictions are already available\n",
    "            train_preds_available = any(estimator_hash_value in col_name for col_name in curr_train_preds.columns)\n",
    "            test_preds_available = any(estimator_hash_value in col_name for col_name in curr_test_preds.columns)\n",
    "            if train_preds_available and test_preds_available:\n",
    "                estimators_skipped += [estimator_name]\n",
    "                continue\n",
    "\n",
    "            print(f\"[INFO] Getting predictions for estimator {estimator_name}\")\n",
    "            train_preds, test_preds = estimator.get_preds(estimator.params_dict, estimator.feature_names)\n",
    "            if not isinstance(train_preds, pd.core.series.Series):\n",
    "                raise ValueError(\"`train_preds` should be of type `pandas.Series`\")\n",
    "            if not isinstance(test_preds, pd.core.series.Series):\n",
    "                raise ValueError(\"`test_preds` should be of type `pandas.Series`\")\n",
    "            curr_train_preds[estimator_name] = train_preds\n",
    "            curr_test_preds[estimator_name] = test_preds\n",
    "            preds_retrieved_count += 1\n",
    "\n",
    "            # save predictions at an interval of `preds_save_interval`\n",
    "            if preds_retrieved_count % self.preds_save_interval == 0:\n",
    "                curr_train_preds.sort_index(axis=1).to_csv(self.get_train_preds_file_path(), index=False)\n",
    "                curr_test_preds.sort_index(axis=1).to_csv(self.get_test_preds_file_path(), index=False)\n",
    "                num_preds_retrieved_but_not_yet_saved = 0\n",
    "                print(\"[INFO] Saved predictions\")\n",
    "            else:\n",
    "                num_preds_retrieved_but_not_yet_saved += 1\n",
    "\n",
    "        if estimators_skipped:\n",
    "            estimators_skipped.sort()\n",
    "            formatted_estimators = \", \".join(estimators_skipped)\n",
    "            print(f\"[INFO] Skipped retrieving predictions for following estimators as their current ones are not stale:\\n{formatted_estimators}\")\n",
    "\n",
    "        if num_preds_retrieved_but_not_yet_saved != 0:\n",
    "            curr_train_preds.sort_index(axis=1).to_csv(self.get_train_preds_file_path(), index=False)\n",
    "            curr_test_preds.sort_index(axis=1).to_csv(self.get_test_preds_file_path(), index=False)\n",
    "            print(\"[INFO] Saved predictions\")\n",
    "\n",
    "        print(\"[INFO] Finished getting all predictions\")\n",
    "\n",
    "    def sync_preds(self):\n",
    "        \"\"\"\n",
    "        Syncs the predictions stored at the two file paths specified by\n",
    "        `working_dir_path`, `train_preds_filename` and `test_preds_filename` by\n",
    "        removing predictions for any estimator that is not currently in `estimators`.\n",
    "\n",
    "        Note that new predictions for estimators that do not currently have predictions\n",
    "        in the files will not be added; `get_preds` should be used for this purpose\n",
    "        instead.\n",
    "        \"\"\"\n",
    "        print(\"[INFO] Syncing predictions..\")\n",
    "        estimator_hash_values = [estimator.get_hash_value() for estimator in self.estimators]\n",
    "        should_remove_col = lambda col_name: not any(hash_value in col_name for hash_value in estimator_hash_values)\n",
    "\n",
    "        curr_train_preds, curr_test_preds = self.get_current_train_and_test_preds()\n",
    "\n",
    "        if not curr_train_preds.empty:\n",
    "            col_names_to_remove = [col_name for col_name in curr_train_preds.columns if should_remove_col(col_name)]\n",
    "            if col_names_to_remove:\n",
    "                print(f\"[INFO] Dropping columns for following estimators from training predictions:\\n{col_names_to_remove}\")\n",
    "                curr_train_preds.drop(columns=col_names_to_remove, inplace=True)\n",
    "                curr_train_preds.to_csv(self.get_train_preds_file_path(), index=False)\n",
    "            else:\n",
    "                print(f\"[INFO] No columns for training predictions were dropped\")\n",
    "        if not curr_test_preds.empty:\n",
    "            col_names_to_remove = [col_name for col_name in curr_test_preds.columns if should_remove_col(col_name)]\n",
    "            if col_names_to_remove:\n",
    "                print(f\"[INFO] Dropping columns for following estimators from test predictions:\\n{col_names_to_remove}\")\n",
    "                curr_test_preds.drop(columns=col_names_to_remove, inplace=True)\n",
    "                curr_test_preds.to_csv(self.get_test_preds_file_path(), index=False)\n",
    "            else:\n",
    "                print(f\"[INFO] No columns for test predictions were dropped\")\n",
    "\n",
    "        print(\"[INFO] Finished syncing predictions\")\n",
    "\n",
    "    def import_preds(self, input_dir_path):\n",
    "        \"\"\"\n",
    "        Imports predictions stored at the two file paths at `input_dir_path` with\n",
    "        `train_preds_filename` and `test_preds_filename` as their filenames. If no\n",
    "        such files are found, no predictions will be imported.\n",
    "\n",
    "        Only predictions for estimators specified in `estimators` will be imported.\n",
    "        Any predictions for estimators that were already available will be overwritten\n",
    "        with predictions for the same estimators found in the files at `input_dir_path`.\n",
    "\n",
    "        :param input_dir_path:\n",
    "            The path to the directory for the training and test predictions files.\n",
    "            The file names are expected to be the same as `train_preds_filename`\n",
    "            and `test_preds_filename`\n",
    "        \"\"\"\n",
    "        print(\"[INFO] Importing predictions..\")\n",
    "        curr_train_preds, curr_test_preds = self.get_current_train_and_test_preds()\n",
    "        input_train_preds = pd.DataFrame()\n",
    "        input_test_preds = pd.DataFrame()\n",
    "\n",
    "        input_train_preds_path = Path(f\"{input_dir_path}/{self.train_preds_filename}.csv\")\n",
    "        input_test_preds_path = Path(f\"{input_dir_path}/{self.test_preds_filename}.csv\")\n",
    "        if input_train_preds_path.is_file():\n",
    "            try:\n",
    "                input_train_preds = pd.read_csv(input_train_preds_path)\n",
    "            except: pass\n",
    "        if input_test_preds_path.is_file():\n",
    "            try:\n",
    "                input_test_preds = pd.read_csv(input_test_preds_path)\n",
    "            except: pass\n",
    "\n",
    "        estimators_with_imported_train_preds = []\n",
    "        estimators_with_imported_test_preds = []\n",
    "        for estimator in self.estimators:\n",
    "            estimator_hash_value = estimator.get_hash_value()\n",
    "            estimator_name = f\"{estimator.name} ({estimator_hash_value})\"\n",
    "            train_preds_available = any(estimator_hash_value in col_name for col_name in input_train_preds.columns)\n",
    "            test_preds_available = any(estimator_hash_value in col_name for col_name in input_test_preds.columns)\n",
    "\n",
    "            if train_preds_available:\n",
    "                curr_train_preds[estimator_name] = input_train_preds[estimator_name]\n",
    "                estimators_with_imported_train_preds += [estimator_name]\n",
    "            if test_preds_available:\n",
    "                curr_test_preds[estimator_name] = input_test_preds[estimator_name]\n",
    "                estimators_with_imported_test_preds += [estimator_name]\n",
    "\n",
    "        if not estimators_with_imported_train_preds:\n",
    "            print(\"[INFO] No train predictions were imported\")\n",
    "        else:\n",
    "            curr_train_preds.sort_index(axis=1).to_csv(self.get_train_preds_file_path(), index=False)\n",
    "            formatted_estimators = \", \".join(estimators_with_imported_train_preds)\n",
    "            print(f\"[INFO] {len(estimators_with_imported_train_preds)} train predictions were imported:\\n{formatted_estimators}\")\n",
    "        if not estimators_with_imported_test_preds:\n",
    "            print(\"[INFO] No test predictions were imported\")\n",
    "        else:\n",
    "            curr_test_preds.sort_index(axis=1).to_csv(self.get_test_preds_file_path(), index=False)\n",
    "            formatted_estimators = \", \".join(estimators_with_imported_test_preds)\n",
    "            print(f\"[INFO] {len(estimators_with_imported_test_preds)} test predictions were imported:\\n{formatted_estimators}\")\n",
    "        \n",
    "        print(\"[INFO] Finished importing predictions\")\n",
    "\n",
    "    def clear_preds(self):\n",
    "        \"\"\"\n",
    "        Removes all stored predictions by deleting the two files at filepaths specified\n",
    "        by `working_dir_path`, `train_preds_filename` and `test_preds_filename`.\n",
    "        \"\"\"\n",
    "        train_preds_file_path = self.get_train_preds_file_path()\n",
    "        test_preds_file_path = self.get_test_preds_file_path()\n",
    "\n",
    "        if train_preds_file_path.is_file():\n",
    "            train_preds_file_path.unlink()\n",
    "        if test_preds_file_path.is_file():\n",
    "            test_preds_file_path.unlink()\n",
    "\n",
    "        print(\"[INFO] Finished clearing predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca05316",
   "metadata": {
    "papermill": {
     "duration": 0.01114,
     "end_time": "2025-12-26T16:32:08.726310",
     "exception": false,
     "start_time": "2025-12-26T16:32:08.715170",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Next, we'll simply create a variable for storing the estimators (`StackingEstimator` instances) that we'll pass to the `StackingPredictionsRetriever` class for getting all the predictions from our base models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0d4ddb14",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-26T16:32:08.747245Z",
     "iopub.status.busy": "2025-12-26T16:32:08.747023Z",
     "iopub.status.idle": "2025-12-26T16:32:08.750156Z",
     "shell.execute_reply": "2025-12-26T16:32:08.749608Z"
    },
    "papermill": {
     "duration": 0.01479,
     "end_time": "2025-12-26T16:32:08.751169",
     "exception": false,
     "start_time": "2025-12-26T16:32:08.736379",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "estimators = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707a63ad",
   "metadata": {
    "papermill": {
     "duration": 0.010058,
     "end_time": "2025-12-26T16:32:08.771134",
     "exception": false,
     "start_time": "2025-12-26T16:32:08.761076",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 6. Feature Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "85d3b50f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-26T16:32:08.792963Z",
     "iopub.status.busy": "2025-12-26T16:32:08.792735Z",
     "iopub.status.idle": "2025-12-26T16:32:08.799588Z",
     "shell.execute_reply": "2025-12-26T16:32:08.799043Z"
    },
    "papermill": {
     "duration": 0.019318,
     "end_time": "2025-12-26T16:32:08.800552",
     "exception": false,
     "start_time": "2025-12-26T16:32:08.781234",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# base feature set\n",
    "FEATURE_SET_1 = [\n",
    "    'age', 'alcohol_consumption_per_week', 'physical_activity_minutes_per_week',\n",
    "    'diet_score', 'sleep_hours_per_day', 'screen_time_hours_per_day',\n",
    "    'bmi', 'waist_to_hip_ratio', 'systolic_bp',\n",
    "    'diastolic_bp', 'heart_rate', 'cholesterol_total',\n",
    "    'hdl_cholesterol', 'ldl_cholesterol', 'triglycerides',\n",
    "    'gender', 'ethnicity', 'employment_status',\n",
    "    'family_history_diabetes', 'hypertension_history', 'cardiovascular_history',\n",
    "    'education_level_encoded', 'income_level_encoded', 'smoking_status_encoded',\n",
    "    'log_triglycerides', 'log_ldl_cholesterol', 'log_cholesterol_total',\n",
    "    'cholesterol_ratio', 'ldl_hdl_ratio', 'pulse_pressure',\n",
    "    'mean_arterial_pressure', 'age_x_bmi', 'waist_x_bmi',\n",
    "    'family_history_diabetes_x_log_triglycerides', 'hypertension_history_x_systolic_bp', 'activity_x_diet',\n",
    "    'age_sq', 'bmi_sq', 'waist_to_hip_ratio_sq',\n",
    "    'systolic_bp_sq', 'comorbidity_count', 'bmi_cat',\n",
    "    'cluster_label',\n",
    "]\n",
    "\n",
    "# FEATURE_SET_2 = FEATURE_SET_1 + proxies + binned/relative features + clusters\n",
    "FEATURE_SET_2 = [\n",
    "    'age', 'alcohol_consumption_per_week', 'physical_activity_minutes_per_week',\n",
    "    'diet_score', 'sleep_hours_per_day', 'screen_time_hours_per_day',\n",
    "    'bmi', 'waist_to_hip_ratio', 'systolic_bp',\n",
    "    'diastolic_bp', 'heart_rate', 'cholesterol_total',\n",
    "    'hdl_cholesterol', 'ldl_cholesterol', 'triglycerides',\n",
    "    'gender', 'ethnicity', 'employment_status',\n",
    "    'family_history_diabetes', 'hypertension_history', 'cardiovascular_history',\n",
    "    'education_level_encoded', 'income_level_encoded', 'smoking_status_encoded',\n",
    "    'log_triglycerides', 'log_ldl_cholesterol', 'log_cholesterol_total',\n",
    "    'cholesterol_ratio', 'ldl_hdl_ratio', 'pulse_pressure',\n",
    "    'mean_arterial_pressure', 'age_x_bmi', 'waist_x_bmi',\n",
    "    'family_history_diabetes_x_log_triglycerides', 'hypertension_history_x_systolic_bp', 'activity_x_diet',\n",
    "    'atherogenic_index', 'non_hdl_cholesterol', 'map_x_bmi',\n",
    "    'lipid_accumulation_proxy', 'visceral_adiposity_proxy', 'age_sq',\n",
    "    'bmi_sq', 'waist_to_hip_ratio_sq', 'systolic_bp_sq',\n",
    "    'comorbidity_count', 'bmi_cat', 'bmi_relative_to_age_decade',\n",
    "    'systolic_bp_relative_to_age_decade', 'cholesterol_total_relative_to_age_decade', 'bmi_relative_to_hypertension_history',\n",
    "    'systolic_bp_relative_to_hypertension_history', 'cholesterol_total_relative_to_hypertension_history', 'cluster_label',\n",
    "    'dist_to_cluster_0', 'dist_to_cluster_1', 'dist_to_cluster_2',\n",
    "    'dist_to_cluster_3', 'dist_to_cluster_4', 'dist_to_cluster_5',\n",
    "    'dist_to_cluster_6',\n",
    "]\n",
    "\n",
    "# FEATURE_SET_3 = FEATURE_SET_2 + additional binned features\n",
    "FEATURE_SET_3 = [\n",
    "    'age', 'alcohol_consumption_per_week', 'physical_activity_minutes_per_week',\n",
    "    'diet_score', 'sleep_hours_per_day', 'screen_time_hours_per_day',\n",
    "    'bmi', 'waist_to_hip_ratio', 'systolic_bp',\n",
    "    'diastolic_bp', 'heart_rate', 'cholesterol_total',\n",
    "    'hdl_cholesterol', 'ldl_cholesterol', 'triglycerides',\n",
    "    'gender', 'ethnicity', 'employment_status',\n",
    "    'family_history_diabetes', 'hypertension_history', 'cardiovascular_history',\n",
    "    'education_level_encoded', 'income_level_encoded', 'smoking_status_encoded',\n",
    "    'log_triglycerides', 'log_ldl_cholesterol', 'log_cholesterol_total',\n",
    "    'cholesterol_ratio', 'ldl_hdl_ratio', 'pulse_pressure',\n",
    "    'mean_arterial_pressure', 'age_x_bmi', 'waist_x_bmi',\n",
    "    'family_history_diabetes_x_log_triglycerides', 'hypertension_history_x_systolic_bp', 'activity_x_diet',\n",
    "    'atherogenic_index', 'non_hdl_cholesterol', 'map_x_bmi',\n",
    "    'lipid_accumulation_proxy', 'visceral_adiposity_proxy', 'age_sq',\n",
    "    'bmi_sq', 'waist_to_hip_ratio_sq', 'systolic_bp_sq',\n",
    "    'comorbidity_count', 'bmi_cat', 'bp_cat',\n",
    "    'cholesterol_cat', 'hdl_cat', 'ldl_cat',\n",
    "    'triglycerides_decile', 'waist_to_hip_ratio_decile', 'bmi_decile',\n",
    "    'mean_arterial_pressure_decile', 'age_decade', 'bmi_relative_to_age_decade',\n",
    "    'systolic_bp_relative_to_age_decade', 'cholesterol_total_relative_to_age_decade', 'bmi_relative_to_hypertension_history',\n",
    "    'systolic_bp_relative_to_hypertension_history', 'cholesterol_total_relative_to_hypertension_history', 'age_bp_interaction',\n",
    "    'cluster_label', 'dist_to_cluster_0', 'dist_to_cluster_1',\n",
    "    'dist_to_cluster_2', 'dist_to_cluster_3', 'dist_to_cluster_4',\n",
    "    'dist_to_cluster_5', 'dist_to_cluster_6',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3499877",
   "metadata": {
    "papermill": {
     "duration": 0.010004,
     "end_time": "2025-12-26T16:32:08.820566",
     "exception": false,
     "start_time": "2025-12-26T16:32:08.810562",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 7. Base Model Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4fa1c274",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-26T16:32:08.841404Z",
     "iopub.status.busy": "2025-12-26T16:32:08.841200Z",
     "iopub.status.idle": "2025-12-26T16:32:08.844231Z",
     "shell.execute_reply": "2025-12-26T16:32:08.843725Z"
    },
    "papermill": {
     "duration": 0.014782,
     "end_time": "2025-12-26T16:32:08.845251",
     "exception": false,
     "start_time": "2025-12-26T16:32:08.830469",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# to skip hyperparameter tuning when it's not needed; set to `False` to do the tuning\n",
    "SKIP_BASE_MODEL_HYPERPARAMETER_TUNING = True\n",
    "\n",
    "# value set for early stopping for base models that support it; this value will be used for actual model training as well\n",
    "BASE_MODEL_EARLY_STOPPING_ROUNDS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "336f19f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-26T16:32:08.866328Z",
     "iopub.status.busy": "2025-12-26T16:32:08.865918Z",
     "iopub.status.idle": "2025-12-26T16:32:08.869319Z",
     "shell.execute_reply": "2025-12-26T16:32:08.868819Z"
    },
    "papermill": {
     "duration": 0.015117,
     "end_time": "2025-12-26T16:32:08.870334",
     "exception": false,
     "start_time": "2025-12-26T16:32:08.855217",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BaseModelOptunaStudyEstimator(Enum):\n",
    "    CATBOOSTCLASSIFIER = \"CatBoostClassifier\"\n",
    "    LGBMCLASSIFIER = \"LGBMClassifier\"\n",
    "    XGBCLASSIFIER = \"XGBClassifier\"\n",
    "    XGBCLASSIFIER_DART = \"XGBClassifier_DART\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e350c09",
   "metadata": {
    "papermill": {
     "duration": 0.010075,
     "end_time": "2025-12-26T16:32:08.890445",
     "exception": false,
     "start_time": "2025-12-26T16:32:08.880370",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Manually configure the values for the following variables for different studies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7bea4deb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-26T16:32:08.911142Z",
     "iopub.status.busy": "2025-12-26T16:32:08.910962Z",
     "iopub.status.idle": "2025-12-26T16:32:08.914271Z",
     "shell.execute_reply": "2025-12-26T16:32:08.913706Z"
    },
    "papermill": {
     "duration": 0.015038,
     "end_time": "2025-12-26T16:32:08.915438",
     "exception": false,
     "start_time": "2025-12-26T16:32:08.900400",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# estimator to use for Optuna study\n",
    "BASE_MODEL_OPTUNA_STUDY_ESTIMATOR = BaseModelOptunaStudyEstimator.CATBOOSTCLASSIFIER\n",
    "\n",
    "# feature set to use for Optuna study\n",
    "BASE_MODEL_OPTUNA_STUDY_FEATURE_SET = FEATURE_SET_3\n",
    "\n",
    "# maximum number of trials Optuna will conduct for the optimization\n",
    "BASE_MODEL_OPTUNA_STUDY_NUM_TRIALS = 60\n",
    "\n",
    "# number of splits to use for Stratified K-Fold Cross-Validation for Optuna study\n",
    "BASE_MODEL_OPTUNA_STUDY_KFOLD_NUM_SPLITS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "da910e9a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-26T16:32:08.937350Z",
     "iopub.status.busy": "2025-12-26T16:32:08.936909Z",
     "iopub.status.idle": "2025-12-26T16:32:08.962343Z",
     "shell.execute_reply": "2025-12-26T16:32:08.961810Z"
    },
    "papermill": {
     "duration": 0.037212,
     "end_time": "2025-12-26T16:32:08.963321",
     "exception": false,
     "start_time": "2025-12-26T16:32:08.926109",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_base_model_optuna_params(trial, study_estimator):\n",
    "    if study_estimator == BaseModelOptunaStudyEstimator.CATBOOSTCLASSIFIER:\n",
    "        if BASE_MODEL_OPTUNA_STUDY_FEATURE_SET == FEATURE_SET_1:\n",
    "            return {\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 0.015, 0.04),\n",
    "                'depth': trial.suggest_int('depth', 5, 7),\n",
    "                'border_count': trial.suggest_int('border_count', 200, 254),\n",
    "                'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 3.0, 50.0, log=True),\n",
    "                'bagging_temperature': trial.suggest_float('bagging_temperature', 0.0, 0.5),\n",
    "                'random_strength': trial.suggest_float('random_strength', 5.0, 20.0),\n",
    "                'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 50, 100),\n",
    "            }\n",
    "        elif BASE_MODEL_OPTUNA_STUDY_FEATURE_SET == FEATURE_SET_2:\n",
    "            return {\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 0.03, 0.10),\n",
    "                'depth': trial.suggest_int('depth', 5, 8),\n",
    "                'border_count': trial.suggest_int('border_count', 128, 254),\n",
    "                'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 10.0, 100.0, log=True),\n",
    "                'bagging_temperature': trial.suggest_float('bagging_temperature', 0.0, 1.0),\n",
    "                'random_strength': trial.suggest_float('random_strength', 1.0, 10.0, log=True),\n",
    "                'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 10, 100),\n",
    "            }\n",
    "        elif BASE_MODEL_OPTUNA_STUDY_FEATURE_SET == FEATURE_SET_3:\n",
    "            return {\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 0.08, 0.20),\n",
    "                'depth': trial.suggest_int('depth', 3, 4),\n",
    "                'border_count': trial.suggest_int('border_count', 128, 254),\n",
    "                'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 20.0, 150.0, log=True),\n",
    "                'bagging_temperature': trial.suggest_float('bagging_temperature', 0.0, 1.0),\n",
    "                'random_strength': trial.suggest_float('random_strength', 1.0, 10.0, log=True),\n",
    "                'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 10, 100),\n",
    "            }\n",
    "        else:\n",
    "            raise ValueError(f\"Search space for feature set for Optuna study not yet specified for CatBoostClassifier.\")\n",
    "    elif study_estimator == BaseModelOptunaStudyEstimator.LGBMCLASSIFIER:\n",
    "        if BASE_MODEL_OPTUNA_STUDY_FEATURE_SET == FEATURE_SET_1:\n",
    "            return {\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.03),\n",
    "                'num_leaves': trial.suggest_int('num_leaves', 20, 35),\n",
    "                'min_child_samples': trial.suggest_int('min_child_samples', 350, 550),\n",
    "                'min_split_gain': trial.suggest_float('min_split_gain', 0.001, 0.1, log=True),\n",
    "                'subsample': trial.suggest_float('subsample', 0.6, 0.9),\n",
    "                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.25, 0.45),\n",
    "                'reg_alpha': trial.suggest_float('reg_alpha', 1.0, 50.0, log=True),\n",
    "                'reg_lambda': trial.suggest_float('reg_lambda', 1.0, 50.0, log=True),\n",
    "            }\n",
    "        elif BASE_MODEL_OPTUNA_STUDY_FEATURE_SET == FEATURE_SET_2:\n",
    "            return {\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 0.015, 0.045),\n",
    "                'num_leaves': trial.suggest_int('num_leaves', 30, 45),\n",
    "                'min_child_samples': trial.suggest_int('min_child_samples', 300, 500),\n",
    "                'min_split_gain': trial.suggest_float('min_split_gain', 1e-4, 0.2, log=True),\n",
    "                'subsample': trial.suggest_float('subsample', 0.6, 0.9),\n",
    "                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.35, 0.55),\n",
    "                'reg_alpha': trial.suggest_float('reg_alpha', 1.0, 20.0, log=True),\n",
    "                'reg_lambda': trial.suggest_float('reg_lambda', 20.0, 100.0, log=True),\n",
    "            }\n",
    "        elif BASE_MODEL_OPTUNA_STUDY_FEATURE_SET == FEATURE_SET_3:\n",
    "            return {\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.04),\n",
    "                'num_leaves': trial.suggest_int('num_leaves', 15, 30),\n",
    "                'min_child_samples': trial.suggest_int('min_child_samples', 100, 300),\n",
    "                'min_split_gain': trial.suggest_float('min_split_gain', 1e-4, 0.1, log=True),\n",
    "                'subsample': trial.suggest_float('subsample', 0.6, 0.95),\n",
    "                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.40, 0.65),\n",
    "                'reg_alpha': trial.suggest_float('reg_alpha', 1.0, 50.0, log=True),\n",
    "                'reg_lambda': trial.suggest_float('reg_lambda', 1.0, 50.0, log=True),\n",
    "            }\n",
    "        else:\n",
    "            raise ValueError(f\"Search space for feature set for Optuna study not yet specified for LGBMClassifier.\")\n",
    "    elif study_estimator == BaseModelOptunaStudyEstimator.XGBCLASSIFIER:\n",
    "        if BASE_MODEL_OPTUNA_STUDY_FEATURE_SET == FEATURE_SET_1:\n",
    "            return {\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 0.008, 0.05),\n",
    "                'max_depth': trial.suggest_int('max_depth', 2, 5),\n",
    "                'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.8, 1.0),\n",
    "                'alpha': trial.suggest_float('alpha', 0.1, 10.0, log=True),\n",
    "                'gamma': trial.suggest_float('gamma', 1.0, 3.0),\n",
    "                'lambda': trial.suggest_float('lambda', 0.1, 10.0, log=True),\n",
    "                'min_child_weight': trial.suggest_int('min_child_weight', 50, 200),\n",
    "            }\n",
    "        elif BASE_MODEL_OPTUNA_STUDY_FEATURE_SET == FEATURE_SET_2:\n",
    "            return {\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.008),\n",
    "                'max_depth': trial.suggest_int('max_depth', 8, 11),\n",
    "                'subsample': trial.suggest_float('subsample', 0.6, 0.9),\n",
    "                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.1, 0.25),\n",
    "                'alpha': trial.suggest_float('alpha', 0.1, 10.0, log=True),\n",
    "                'gamma': trial.suggest_float('gamma', 1.0, 5.0),\n",
    "                'lambda': trial.suggest_float('lambda', 1.0, 10.0, log=True),\n",
    "                'min_child_weight': trial.suggest_int('min_child_weight', 40, 100),\n",
    "            }\n",
    "        elif BASE_MODEL_OPTUNA_STUDY_FEATURE_SET == FEATURE_SET_3:\n",
    "            return {\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 0.03, 0.08),\n",
    "                'max_depth': trial.suggest_int('max_depth', 3, 5),\n",
    "                'subsample': trial.suggest_float('subsample', 0.75, 0.95),\n",
    "                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.10, 0.25),\n",
    "                'alpha': trial.suggest_float('alpha', 0.1, 5.0, log=True),\n",
    "                'gamma': trial.suggest_float('gamma', 0.1, 1.0),\n",
    "                'lambda': trial.suggest_float('lambda', 1.0, 10.0, log=True),\n",
    "                'min_child_weight': trial.suggest_int('min_child_weight', 5, 25),\n",
    "            }\n",
    "        else:\n",
    "            raise ValueError(f\"Search space for feature set for Optuna study not yet specified for XGBClassifier.\")\n",
    "    elif study_estimator == BaseModelOptunaStudyEstimator.XGBCLASSIFIER_DART:\n",
    "        if BASE_MODEL_OPTUNA_STUDY_FEATURE_SET == FEATURE_SET_1:\n",
    "            return {\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 0.015, 0.035),\n",
    "                'max_depth': trial.suggest_int('max_depth', 7, 9),\n",
    "                'subsample': trial.suggest_float('subsample', 0.6, 0.9),\n",
    "                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.3, 0.5),\n",
    "                'alpha': trial.suggest_float('alpha', 0.01, 2.0, log=True),\n",
    "                'gamma': trial.suggest_float('gamma', 1.0, 3.0),\n",
    "                'lambda': trial.suggest_float('lambda', 0.1, 5.0, log=True),\n",
    "                'min_child_weight': trial.suggest_int('min_child_weight', 30, 70),\n",
    "                'rate_drop': trial.suggest_float('rate_drop', 0.05, 0.15),\n",
    "                'skip_drop': trial.suggest_float('skip_drop', 0.3, 0.6),\n",
    "                'one_drop': trial.suggest_categorical('one_drop', [0, 1]),\n",
    "            }\n",
    "        elif BASE_MODEL_OPTUNA_STUDY_FEATURE_SET == FEATURE_SET_2:\n",
    "            return {\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 0.015, 0.035),\n",
    "                'max_depth': trial.suggest_int('max_depth', 7, 9),\n",
    "                'subsample': trial.suggest_float('subsample', 0.6, 0.9),\n",
    "                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.3, 0.5),\n",
    "                'alpha': trial.suggest_float('alpha', 0.01, 2.0, log=True),\n",
    "                'gamma': trial.suggest_float('gamma', 1.0, 3.0),\n",
    "                'lambda': trial.suggest_float('lambda', 0.1, 5.0, log=True),\n",
    "                'min_child_weight': trial.suggest_int('min_child_weight', 30, 70),\n",
    "                'rate_drop': trial.suggest_float('rate_drop', 0.05, 0.15),\n",
    "                'skip_drop': trial.suggest_float('skip_drop', 0.3, 0.6),\n",
    "                'one_drop': trial.suggest_categorical('one_drop', [0, 1]),\n",
    "            }\n",
    "        else:\n",
    "            raise ValueError(f\"Search space for feature set for Optuna study not yet specified for XGBClassifier (DART).\")\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported optuna study estimator\")\n",
    "\n",
    "def get_base_model_predictions(study_estimator, trial_params, X_train_fold, y_train_fold, X_validation_fold, y_validation_fold):\n",
    "    if study_estimator == BaseModelOptunaStudyEstimator.CATBOOSTCLASSIFIER:\n",
    "        model = CatBoostClassifier(\n",
    "            **trial_params,\n",
    "            iterations=30000,\n",
    "            use_best_model=True,\n",
    "            cat_features=cat_features,\n",
    "            loss_function='Logloss',\n",
    "            eval_metric='AUC',\n",
    "            task_type='GPU' if torch.cuda.is_available() else 'CPU',\n",
    "            devices='0',\n",
    "            metric_period=5,\n",
    "            random_seed=RANDOM_SEEDS[0],\n",
    "            verbose=False,\n",
    "            allow_writing_files=False\n",
    "        )\n",
    "        model.fit(\n",
    "            X_train_fold, y_train_fold,\n",
    "            eval_set=(X_validation_fold, y_validation_fold),\n",
    "            early_stopping_rounds=BASE_MODEL_EARLY_STOPPING_ROUNDS\n",
    "        )\n",
    "        return model.predict_proba(X_validation_fold)[:, 1]\n",
    "    elif study_estimator == BaseModelOptunaStudyEstimator.LGBMCLASSIFIER:\n",
    "        model = lgb.LGBMClassifier(\n",
    "            **trial_params,\n",
    "            n_estimators=30000,\n",
    "            objective='binary',\n",
    "            metric='auc',\n",
    "            verbose=-1,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        model.fit(\n",
    "            X_train_fold, y_train_fold,\n",
    "            eval_set=(X_validation_fold, y_validation_fold),\n",
    "            callbacks=[lgb.early_stopping(stopping_rounds=BASE_MODEL_EARLY_STOPPING_ROUNDS, verbose=0)]\n",
    "        )\n",
    "        return model.predict_proba(X_validation_fold)[:, 1]\n",
    "    elif study_estimator == BaseModelOptunaStudyEstimator.XGBCLASSIFIER:\n",
    "        model = XGBClassifier(\n",
    "            **trial_params,\n",
    "            n_estimators=30000,\n",
    "            tree_method='hist' if torch.cuda.is_available() else 'auto',\n",
    "            device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "            enable_categorical=True,\n",
    "            objective='binary:logistic',\n",
    "            eval_metric='auc',\n",
    "            early_stopping_rounds=BASE_MODEL_EARLY_STOPPING_ROUNDS,\n",
    "            n_jobs=-1,\n",
    "            random_state=RANDOM_SEEDS[0],\n",
    "            verbosity=0\n",
    "        )\n",
    "        model.fit(\n",
    "            X_train_fold, y_train_fold,\n",
    "            eval_set=[(X_validation_fold, y_validation_fold)],\n",
    "            verbose=False\n",
    "        )\n",
    "        return model.predict_proba(X_validation_fold)[:, 1]\n",
    "    elif study_estimator == BaseModelOptunaStudyEstimator.XGBCLASSIFIER_DART:\n",
    "        model = XGBClassifier(\n",
    "            **trial_params,\n",
    "            n_estimators=2500,\n",
    "            booster='dart',\n",
    "            tree_method='hist' if torch.cuda.is_available() else 'auto',\n",
    "            device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "            enable_categorical=True,\n",
    "            objective='binary:logistic',\n",
    "            eval_metric='auc',\n",
    "            n_jobs=-1,\n",
    "            random_state=RANDOM_SEEDS[0],\n",
    "            verbosity=0\n",
    "        )\n",
    "        model.fit(\n",
    "            X_train_fold, y_train_fold,\n",
    "            eval_set=[(X_validation_fold, y_validation_fold)],\n",
    "            verbose=False\n",
    "        )\n",
    "        return model.predict_proba(X_validation_fold)[:, 1]\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported optuna study estimator\")\n",
    "\n",
    "def base_model_optuna_study_objective(trial):\n",
    "    base_model_params = get_base_model_optuna_params(trial, BASE_MODEL_OPTUNA_STUDY_ESTIMATOR)\n",
    "\n",
    "    X_train = train_data[BASE_MODEL_OPTUNA_STUDY_FEATURE_SET]\n",
    "    y_train = train_data[target_col]\n",
    "\n",
    "    base_model_optuna_study_skf = StratifiedKFold(n_splits=BASE_MODEL_OPTUNA_STUDY_KFOLD_NUM_SPLITS, shuffle=True, random_state=RANDOM_SEEDS[0])\n",
    "    base_model_optuna_study_skf_splits = base_model_optuna_study_skf.split(X_train, y_train)\n",
    "    base_model_optuna_study_skf_enumeration = enumerate(base_model_optuna_study_skf_splits)\n",
    "\n",
    "    total_roc_auc = 0\n",
    "\n",
    "    for fold, (train_indices, validation_indices) in base_model_optuna_study_skf_enumeration:\n",
    "        X_train_fold = X_train.iloc[train_indices]\n",
    "        X_validation_fold = X_train.iloc[validation_indices]\n",
    "        y_train_fold = y_train.iloc[train_indices]\n",
    "        y_validation_fold = y_train.iloc[validation_indices]\n",
    "\n",
    "        y_validation_pred_proba = get_base_model_predictions(\n",
    "            BASE_MODEL_OPTUNA_STUDY_ESTIMATOR,\n",
    "            base_model_params,\n",
    "            X_train_fold, y_train_fold,\n",
    "            X_validation_fold, y_validation_fold\n",
    "        )\n",
    "        roc_auc_fold = roc_auc_score(y_validation_fold, y_validation_pred_proba)\n",
    "        total_roc_auc += roc_auc_fold\n",
    "\n",
    "        trial.report(roc_auc_fold, step=fold)\n",
    "        if trial.should_prune():\n",
    "            raise optuna.TrialPruned()\n",
    "\n",
    "    average_roc_auc = total_roc_auc / BASE_MODEL_OPTUNA_STUDY_KFOLD_NUM_SPLITS\n",
    "    return average_roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2a4e7d82",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-26T16:32:08.984141Z",
     "iopub.status.busy": "2025-12-26T16:32:08.983947Z",
     "iopub.status.idle": "2025-12-26T16:32:08.988616Z",
     "shell.execute_reply": "2025-12-26T16:32:08.987836Z"
    },
    "papermill": {
     "duration": 0.016214,
     "end_time": "2025-12-26T16:32:08.989630",
     "exception": false,
     "start_time": "2025-12-26T16:32:08.973416",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped base model hyperparameter tuning\n"
     ]
    }
   ],
   "source": [
    "if SKIP_BASE_MODEL_HYPERPARAMETER_TUNING:\n",
    "    print(\"Skipped base model hyperparameter tuning\")\n",
    "else:\n",
    "    print(f\"Started base model hyperparameter tuning for {BASE_MODEL_OPTUNA_STUDY_ESTIMATOR.value}\")\n",
    "    sampler = optuna.samplers.TPESampler(n_ei_candidates=48, multivariate=True)\n",
    "    study = optuna.create_study(sampler=sampler, direction=\"maximize\")\n",
    "    study.optimize(base_model_optuna_study_objective, n_trials=BASE_MODEL_OPTUNA_STUDY_NUM_TRIALS)\n",
    "    \n",
    "    print(f\"# trials finished: {len(study.trials)}\")\n",
    "    trial = study.best_trial\n",
    "    print(f\"Best trial AUC: {trial.value}\")\n",
    "    print(f\"Best trial params:\")\n",
    "    for param_key, param_value in trial.params.items():\n",
    "        print(f\"- {param_key}: {param_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d98cc12",
   "metadata": {
    "papermill": {
     "duration": 0.009822,
     "end_time": "2025-12-26T16:32:09.009659",
     "exception": false,
     "start_time": "2025-12-26T16:32:08.999837",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 8. Base Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4e68a428",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-26T16:32:09.030690Z",
     "iopub.status.busy": "2025-12-26T16:32:09.030182Z",
     "iopub.status.idle": "2025-12-26T16:32:09.033059Z",
     "shell.execute_reply": "2025-12-26T16:32:09.032532Z"
    },
    "papermill": {
     "duration": 0.014465,
     "end_time": "2025-12-26T16:32:09.034078",
     "exception": false,
     "start_time": "2025-12-26T16:32:09.019613",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# number of splits to use for Stratified K-Fold Cross-Validation for base models\n",
    "BASE_MODEL_KFOLD_NUM_SPLITS = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9340a941",
   "metadata": {
    "papermill": {
     "duration": 0.009974,
     "end_time": "2025-12-26T16:32:09.054236",
     "exception": false,
     "start_time": "2025-12-26T16:32:09.044262",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 8.1 CatBoostClassifier\n",
    "\n",
    "### 8.1.1 Helper Methods (CatBoostClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7c0102cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-26T16:32:09.075055Z",
     "iopub.status.busy": "2025-12-26T16:32:09.074848Z",
     "iopub.status.idle": "2025-12-26T16:32:09.081498Z",
     "shell.execute_reply": "2025-12-26T16:32:09.080813Z"
    },
    "papermill": {
     "duration": 0.018409,
     "end_time": "2025-12-26T16:32:09.082539",
     "exception": false,
     "start_time": "2025-12-26T16:32:09.064130",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_catboostclassifier_preds(params_dict, feature_names):\n",
    "    oof_preds_accumulator = np.zeros(len(train_data))\n",
    "    test_preds_accumulator = np.zeros(len(test_data))\n",
    "\n",
    "    X_train = train_data[feature_names]\n",
    "    y_train = train_data[target_col]\n",
    "\n",
    "    for random_seed in RANDOM_SEEDS:\n",
    "        skf = StratifiedKFold(n_splits=BASE_MODEL_KFOLD_NUM_SPLITS, shuffle=True, random_state=random_seed)\n",
    "        skf_splits = skf.split(train_data[feature_names], train_data[target_col])\n",
    "        skf_enumeration = enumerate(skf_splits)\n",
    "\n",
    "        seed_oof_preds = np.zeros(len(train_data))\n",
    "    \n",
    "        for fold, (train_indices, validation_indices) in skf_enumeration:\n",
    "            X_train_fold = X_train.iloc[train_indices]\n",
    "            X_validation_fold = X_train.iloc[validation_indices]\n",
    "            y_train_fold = y_train.iloc[train_indices]\n",
    "            y_validation_fold = y_train.iloc[validation_indices]\n",
    "        \n",
    "            model = CatBoostClassifier(\n",
    "                **params_dict,\n",
    "                use_best_model=True,\n",
    "                cat_features=cat_features,\n",
    "                loss_function='Logloss',\n",
    "                eval_metric='AUC',\n",
    "                task_type='GPU' if torch.cuda.is_available() else 'CPU',\n",
    "                devices='0',\n",
    "                metric_period=5,\n",
    "                random_seed=random_seed,\n",
    "                verbose=False,\n",
    "                allow_writing_files=False\n",
    "            )\n",
    "            model.fit(\n",
    "                X_train_fold, y_train_fold,\n",
    "                eval_set=(X_validation_fold, y_validation_fold),\n",
    "                early_stopping_rounds=BASE_MODEL_EARLY_STOPPING_ROUNDS\n",
    "            )\n",
    "\n",
    "            y_validation_pred_proba = model.predict_proba(X_validation_fold)[:, 1]\n",
    "            y_test_pred_proba = model.predict_proba(test_data[feature_names])[:, 1]\n",
    "            seed_oof_preds[validation_indices] = np.array(y_validation_pred_proba)\n",
    "            test_preds_accumulator += np.array(y_test_pred_proba)\n",
    "\n",
    "        oof_preds_accumulator += seed_oof_preds\n",
    "\n",
    "    final_oof_preds = oof_preds_accumulator / len(RANDOM_SEEDS)\n",
    "    final_test_preds = test_preds_accumulator / (BASE_MODEL_KFOLD_NUM_SPLITS * len(RANDOM_SEEDS))\n",
    "    return pd.Series(final_oof_preds), pd.Series(final_test_preds)\n",
    "\n",
    "def get_catboostclassifier_stacking_estimator(index, params_dict, feature_names):\n",
    "    return StackingEstimator(\n",
    "        name=f\"CatBoostClassifier_{index}\",\n",
    "        params_dict=params_dict,\n",
    "        feature_names=feature_names,\n",
    "        get_preds=get_catboostclassifier_preds\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa24eb6",
   "metadata": {
    "papermill": {
     "duration": 0.010078,
     "end_time": "2025-12-26T16:32:09.103007",
     "exception": false,
     "start_time": "2025-12-26T16:32:09.092929",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 8.1.2 Add Estimators (CatBoostClassifier)\n",
    "\n",
    "Add CatBoostClassifier estimators to list that StackingPredictionsRetriever will process. Hyperparameters were found using Optuna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3f09c3d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-26T16:32:09.124317Z",
     "iopub.status.busy": "2025-12-26T16:32:09.124090Z",
     "iopub.status.idle": "2025-12-26T16:32:09.135480Z",
     "shell.execute_reply": "2025-12-26T16:32:09.134792Z"
    },
    "papermill": {
     "duration": 0.023534,
     "end_time": "2025-12-26T16:32:09.136532",
     "exception": false,
     "start_time": "2025-12-26T16:32:09.112998",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# CatBoostClassifier base models using FEATURE_SET_1\n",
    "estimators += [\n",
    "    get_catboostclassifier_stacking_estimator(\n",
    "        index=1,\n",
    "        params_dict={ # Optuna study AUC: 0.7261767336235222\n",
    "            'iterations': 30000,\n",
    "            'learning_rate': 0.03933473509871599,\n",
    "            'depth': 3,\n",
    "            'l2_leaf_reg': 14.932109771039046,\n",
    "            'bagging_temperature': 0.13345806085697987,\n",
    "            'random_strength': 7.486374538597635,\n",
    "            'min_data_in_leaf': 2,\n",
    "        },\n",
    "        feature_names=FEATURE_SET_1\n",
    "    ),\n",
    "    get_catboostclassifier_stacking_estimator(\n",
    "        index=2,\n",
    "        params_dict={ # Optuna study AUC: 0.725842155230371\n",
    "            'iterations': 30000,\n",
    "            'learning_rate': 0.041779205681346576,\n",
    "            'depth': 4,\n",
    "            'l2_leaf_reg': 3.628892496718331,\n",
    "            'bagging_temperature': 0.1922242909320177,\n",
    "            'random_strength': 8.464699585881778,\n",
    "            'min_data_in_leaf': 5,\n",
    "        },\n",
    "        feature_names=FEATURE_SET_1\n",
    "    ),\n",
    "    get_catboostclassifier_stacking_estimator(\n",
    "        index=3,\n",
    "        params_dict={ # Optuna study AUC: 0.7257614687804782\n",
    "            'iterations': 30000,\n",
    "            'learning_rate': 0.08955773312600926,\n",
    "            'depth': 4,\n",
    "            'l2_leaf_reg': 8.952470035979275,\n",
    "            'bagging_temperature': 0.21150772067613666,\n",
    "            'random_strength': 14.741499198080962,\n",
    "            'min_data_in_leaf': 1,\n",
    "        },\n",
    "        feature_names=FEATURE_SET_1\n",
    "    ),\n",
    "    # get_catboostclassifier_stacking_estimator(\n",
    "    #     index=4,\n",
    "    #     params_dict={ # Optuna study AUC: 0.7268829540444225\n",
    "    #         'iterations': 30000,\n",
    "    #         'learning_rate': 0.020565190920085484,\n",
    "    #         'depth': 5,\n",
    "    #         'border_count': 244,\n",
    "    #         'l2_leaf_reg': 5.425795328172981,\n",
    "    #         'bagging_temperature': 0.40465921452779574,\n",
    "    #         'random_strength': 16.633797525335456,\n",
    "    #         'min_data_in_leaf': 81,\n",
    "    #     },\n",
    "    #     feature_names=FEATURE_SET_1\n",
    "    # ),\n",
    "    # get_catboostclassifier_stacking_estimator(\n",
    "    #     index=5,\n",
    "    #     params_dict={ # Optuna study AUC: 0.7270256119357182\n",
    "    #         'iterations': 30000,\n",
    "    #         'learning_rate': 0.018463696564527392,\n",
    "    #         'depth': 5,\n",
    "    #         'border_count': 227,\n",
    "    #         'l2_leaf_reg': 38.86122949114052,\n",
    "    #         'bagging_temperature': 0.3006204146898439,\n",
    "    #         'random_strength': 15.551270889153717,\n",
    "    #         'min_data_in_leaf': 74,\n",
    "    #     },\n",
    "    #     feature_names=FEATURE_SET_1\n",
    "    # ),\n",
    "]\n",
    "\n",
    "# CatBoostClassifier base models using FEATURE_SET_2\n",
    "estimators += [\n",
    "    get_catboostclassifier_stacking_estimator(\n",
    "        index=100,\n",
    "        params_dict={ # Optuna study AUC: 0.7262278647675813\n",
    "            'iterations': 30000,\n",
    "            'learning_rate': 0.009903259488412045,\n",
    "            'depth': 5,\n",
    "            'border_count': 229,\n",
    "            'l2_leaf_reg': 27.552992645787203,\n",
    "            'bagging_temperature': 0.29260010262333336,\n",
    "            'random_strength': 6.378831014965193,\n",
    "            'min_data_in_leaf': 79,\n",
    "        },\n",
    "        feature_names=FEATURE_SET_2\n",
    "    ),\n",
    "    get_catboostclassifier_stacking_estimator(\n",
    "        index=101,\n",
    "        params_dict={ # Optuna study AUC: 0.7261694357436603\n",
    "            'iterations': 30000,\n",
    "            'learning_rate': 0.027995785656753255,\n",
    "            'depth': 5,\n",
    "            'border_count': 227,\n",
    "            'l2_leaf_reg': 37.089033893184045,\n",
    "            'bagging_temperature': 0.14510428700751374,\n",
    "            'random_strength': 16.707054253585802,\n",
    "            'min_data_in_leaf': 74,\n",
    "        },\n",
    "        feature_names=FEATURE_SET_2\n",
    "    ),\n",
    "    # get_catboostclassifier_stacking_estimator(\n",
    "    #     index=102,\n",
    "    #     params_dict={ # Optuna study AUC: 0.7261505685174567\n",
    "    #         'iterations': 30000,\n",
    "    #         'learning_rate': 0.0144075596799417,\n",
    "    #         'depth': 5,\n",
    "    #         'border_count': 246,\n",
    "    #         'l2_leaf_reg': 12.579095618077156,\n",
    "    #         'bagging_temperature': 0.29812637308837886,\n",
    "    #         'random_strength': 17.813848681334914,\n",
    "    #         'min_data_in_leaf': 98,\n",
    "    #     },\n",
    "    #     feature_names=FEATURE_SET_2\n",
    "    # ),\n",
    "    get_catboostclassifier_stacking_estimator(\n",
    "        index=103,\n",
    "        params_dict={ # Optuna study AUC: 0.726110986436845\n",
    "            'iterations': 30000,\n",
    "            'learning_rate': 0.018075722182856198,\n",
    "            'depth': 4,\n",
    "            'border_count': 238,\n",
    "            'l2_leaf_reg': 3.703935032484741,\n",
    "            'bagging_temperature': 0.4113827574385829,\n",
    "            'random_strength': 45.932408424577005,\n",
    "            'min_data_in_leaf': 33,\n",
    "        },\n",
    "        feature_names=FEATURE_SET_2\n",
    "    ),\n",
    "    get_catboostclassifier_stacking_estimator(\n",
    "        index=104,\n",
    "        params_dict={ # Optuna study AUC: 0.7262024541725216\n",
    "            'iterations': 30000,\n",
    "            'learning_rate': 0.06682939989294684,\n",
    "            'depth': 5,\n",
    "            'border_count': 247,\n",
    "            'l2_leaf_reg': 87.98761404785203,\n",
    "            'bagging_temperature': 0.18097158882519226,\n",
    "            'random_strength': 2.1764370649792166,\n",
    "            'min_data_in_leaf': 96,\n",
    "        },\n",
    "        feature_names=FEATURE_SET_2\n",
    "    ),\n",
    "    get_catboostclassifier_stacking_estimator(\n",
    "        index=105,\n",
    "        params_dict={ # Optuna study AUC: 0.7261804551734349\n",
    "            'iterations': 30000,\n",
    "            'learning_rate': 0.041371759542327295,\n",
    "            'depth': 5,\n",
    "            'border_count': 233,\n",
    "            'l2_leaf_reg': 79.72030402752588,\n",
    "            'bagging_temperature': 0.1745366308337077,\n",
    "            'random_strength': 3.247152923456543,\n",
    "            'min_data_in_leaf': 91,\n",
    "        },\n",
    "        feature_names=FEATURE_SET_2\n",
    "    ),\n",
    "]\n",
    "\n",
    "# CatBoostClassifier base models using FEATURE_SET_3\n",
    "estimators += [\n",
    "    get_catboostclassifier_stacking_estimator(\n",
    "        index=200,\n",
    "        params_dict={ # Optuna study AUC: 0.7261085382272944\n",
    "            'iterations': 30000,\n",
    "            'learning_rate': 0.033098694832699,\n",
    "            'depth': 5,\n",
    "            'border_count': 252,\n",
    "            'l2_leaf_reg': 75.49841384168623,\n",
    "            'bagging_temperature': 0.3157931590124562,\n",
    "            'random_strength': 0.3883253343704605,\n",
    "            'min_data_in_leaf': 27,\n",
    "        },\n",
    "        feature_names=FEATURE_SET_3\n",
    "    ),\n",
    "    get_catboostclassifier_stacking_estimator(\n",
    "        index=201,\n",
    "        params_dict={ # Optuna study AUC: 0.726058121823289\n",
    "            'iterations': 30000,\n",
    "            'learning_rate': 0.030129689158590805,\n",
    "            'depth': 6,\n",
    "            'border_count': 249,\n",
    "            'l2_leaf_reg': 68.34193455262071,\n",
    "            'bagging_temperature': 0.04614353821535644,\n",
    "            'random_strength': 0.1091487165354567,\n",
    "            'min_data_in_leaf': 66,\n",
    "        },\n",
    "        feature_names=FEATURE_SET_3\n",
    "    ),\n",
    "    get_catboostclassifier_stacking_estimator(\n",
    "        index=202,\n",
    "        params_dict={ # Optuna study AUC: 0.7263177506115929\n",
    "            'iterations': 30000,\n",
    "            'learning_rate': 0.13239545573836314,\n",
    "            'depth': 3,\n",
    "            'border_count': 204,\n",
    "            'l2_leaf_reg': 139.14047722098894,\n",
    "            'bagging_temperature': 0.16284158872170643,\n",
    "            'random_strength': 2.9876361959550803,\n",
    "            'min_data_in_leaf': 26,\n",
    "        },\n",
    "        feature_names=FEATURE_SET_3\n",
    "    ),\n",
    "    get_catboostclassifier_stacking_estimator(\n",
    "        index=203,\n",
    "        params_dict={ # Optuna study AUC: 0.7262124228299923\n",
    "            'iterations': 30000,\n",
    "            'learning_rate': 0.14321977040789668,\n",
    "            'depth': 3,\n",
    "            'border_count': 206,\n",
    "            'l2_leaf_reg': 148.36720184486228,\n",
    "            'bagging_temperature': 0.20006682099634276,\n",
    "            'random_strength': 3.851450877580644,\n",
    "            'min_data_in_leaf': 28,\n",
    "        },\n",
    "        feature_names=FEATURE_SET_3\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d359ac64",
   "metadata": {
    "papermill": {
     "duration": 0.009938,
     "end_time": "2025-12-26T16:32:09.156464",
     "exception": false,
     "start_time": "2025-12-26T16:32:09.146526",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 8.2 LGBMClassifier\n",
    "\n",
    "### 8.2.1 Helper Methods (LGBMClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2e59ddec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-26T16:32:09.177409Z",
     "iopub.status.busy": "2025-12-26T16:32:09.177221Z",
     "iopub.status.idle": "2025-12-26T16:32:09.183694Z",
     "shell.execute_reply": "2025-12-26T16:32:09.183215Z"
    },
    "papermill": {
     "duration": 0.018237,
     "end_time": "2025-12-26T16:32:09.184641",
     "exception": false,
     "start_time": "2025-12-26T16:32:09.166404",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_lgbmclassifier_preds(params_dict, feature_names):\n",
    "    oof_preds_accumulator = np.zeros(len(train_data))\n",
    "    test_preds_accumulator = np.zeros(len(test_data))\n",
    "\n",
    "    X_train = train_data[feature_names]\n",
    "    y_train = train_data[target_col]\n",
    "\n",
    "    for random_seed in RANDOM_SEEDS:\n",
    "        skf = StratifiedKFold(n_splits=BASE_MODEL_KFOLD_NUM_SPLITS, shuffle=True, random_state=random_seed)\n",
    "        skf_splits = skf.split(train_data[feature_names], train_data[target_col])\n",
    "        skf_enumeration = enumerate(skf_splits)\n",
    "\n",
    "        seed_oof_preds = np.zeros(len(train_data))\n",
    "\n",
    "        for fold, (train_indices, validation_indices) in skf_enumeration:\n",
    "            X_train_fold = X_train.iloc[train_indices]\n",
    "            X_validation_fold = X_train.iloc[validation_indices]\n",
    "            y_train_fold = y_train.iloc[train_indices]\n",
    "            y_validation_fold = y_train.iloc[validation_indices]\n",
    "\n",
    "            model = lgb.LGBMClassifier(\n",
    "                **params_dict,\n",
    "                n_estimators=30000,\n",
    "                objective='binary',\n",
    "                metric='auc',\n",
    "                verbose=-1,\n",
    "                n_jobs=-1\n",
    "            )\n",
    "            model.fit(\n",
    "                X_train_fold, y_train_fold,\n",
    "                eval_set=(X_validation_fold, y_validation_fold),\n",
    "                callbacks=[lgb.early_stopping(stopping_rounds=BASE_MODEL_EARLY_STOPPING_ROUNDS, verbose=0)]\n",
    "            )\n",
    "\n",
    "            y_validation_pred_proba = model.predict_proba(X_validation_fold)[:, 1]\n",
    "            y_test_pred_proba = model.predict_proba(test_data[feature_names])[:, 1]\n",
    "            seed_oof_preds[validation_indices] = np.array(y_validation_pred_proba)\n",
    "            test_preds_accumulator += np.array(y_test_pred_proba)\n",
    "\n",
    "        oof_preds_accumulator += seed_oof_preds\n",
    "\n",
    "    final_oof_preds = oof_preds_accumulator / len(RANDOM_SEEDS)\n",
    "    final_test_preds = test_preds_accumulator / (BASE_MODEL_KFOLD_NUM_SPLITS * len(RANDOM_SEEDS))\n",
    "    return pd.Series(final_oof_preds), pd.Series(final_test_preds)\n",
    "\n",
    "def get_lgbmclassifier_stacking_estimator(index, params_dict, feature_names):\n",
    "    return StackingEstimator(\n",
    "        name=f\"LGBMClassifier_{index}\",\n",
    "        params_dict=params_dict,\n",
    "        feature_names=feature_names,\n",
    "        get_preds=get_lgbmclassifier_preds\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16fc7a6f",
   "metadata": {
    "papermill": {
     "duration": 0.01009,
     "end_time": "2025-12-26T16:32:09.204718",
     "exception": false,
     "start_time": "2025-12-26T16:32:09.194628",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 8.2.2 Add Estimators (LGBMClassifier)\n",
    "\n",
    "Add XGBClassifier estimators to list that StackingPredictionsRetriever will process. Hyperparameters were found using Optuna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "58d6e851",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-26T16:32:09.225988Z",
     "iopub.status.busy": "2025-12-26T16:32:09.225771Z",
     "iopub.status.idle": "2025-12-26T16:32:09.244758Z",
     "shell.execute_reply": "2025-12-26T16:32:09.244103Z"
    },
    "papermill": {
     "duration": 0.031273,
     "end_time": "2025-12-26T16:32:09.245920",
     "exception": false,
     "start_time": "2025-12-26T16:32:09.214647",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# LGBMClassifier base models using FEATURE_SET_1\n",
    "estimators += [\n",
    "    # get_lgbmclassifier_stacking_estimator(\n",
    "    #     index=1,\n",
    "    #     params_dict={ # Optuna study AUC: 0.7276280474276193\n",
    "    #         'learning_rate': 0.01125492919087445,\n",
    "    #         'num_leaves': 18,\n",
    "    #         'min_child_samples': 11,\n",
    "    #         'subsample': 0.999676392356712,\n",
    "    #         'colsample_bytree': 0.5319864181594173,\n",
    "    #         'reg_alpha': 9.584221562909311,\n",
    "    #         'reg_lambda': 3.3831986318550724,\n",
    "    #     },\n",
    "    #     feature_names=FEATURE_SET_1\n",
    "    # ),\n",
    "    get_lgbmclassifier_stacking_estimator(\n",
    "        index=2,\n",
    "        params_dict={ # Optuna study AUC: 0.7277311734147552\n",
    "            'learning_rate': 0.060449260107967834,\n",
    "            'num_leaves': 7,\n",
    "            'min_child_samples': 55,\n",
    "            'subsample': 0.923934087842396,\n",
    "            'colsample_bytree': 0.5313004056194756,\n",
    "            'reg_alpha': 9.481239127684901,\n",
    "            'reg_lambda': 0.001336263986782526,\n",
    "        },\n",
    "        feature_names=FEATURE_SET_1\n",
    "    ),\n",
    "    get_lgbmclassifier_stacking_estimator(\n",
    "        index=3,\n",
    "        params_dict={ # Optuna study AUC: 0.7277063286096981\n",
    "            'learning_rate': 0.028103753111304447,\n",
    "            'num_leaves': 8,\n",
    "            'min_child_samples': 50,\n",
    "            'subsample': 0.5917000582350134,\n",
    "            'colsample_bytree': 0.4222659398825859,\n",
    "            'reg_alpha': 17.056836702128017,\n",
    "            'reg_lambda': 0.005161430844595434,\n",
    "        },\n",
    "        feature_names=FEATURE_SET_1\n",
    "    ),\n",
    "    # get_lgbmclassifier_stacking_estimator(\n",
    "    #     index=4,\n",
    "    #     params_dict={ # Optuna study AUC: 0.7277637392447089\n",
    "    #         'learning_rate': 0.021776422228844104,\n",
    "    #         'num_leaves': 34,\n",
    "    #         'min_child_samples': 124,\n",
    "    #         'subsample': 0.7672542347544175,\n",
    "    #         'colsample_bytree': 0.40540262525095094,\n",
    "    #         'reg_alpha': 13.154165547218854,\n",
    "    #         'reg_lambda': 1.6421360904189628,\n",
    "    #     },\n",
    "    #     feature_names=FEATURE_SET_1\n",
    "    # ),\n",
    "    # get_lgbmclassifier_stacking_estimator(\n",
    "    #     index=5,\n",
    "    #     params_dict={ # Optuna study AUC: 0.7280265610367231\n",
    "    #         'learning_rate': 0.005372538919315431,\n",
    "    #         'num_leaves': 40,\n",
    "    #         'min_child_samples': 494,\n",
    "    #         'subsample': 0.9625276144224274,\n",
    "    #         'colsample_bytree': 0.3279562120965377,\n",
    "    #         'reg_alpha': 1.5142522813882282,\n",
    "    #         'reg_lambda': 4.088524578916788,\n",
    "    #     },\n",
    "    #     feature_names=FEATURE_SET_1\n",
    "    # ),\n",
    "    get_lgbmclassifier_stacking_estimator(\n",
    "        index=6,\n",
    "        params_dict={ # Optuna study AUC: 0.7285206901236\n",
    "            'learning_rate': 0.010514959295597044,\n",
    "            'num_leaves': 59,\n",
    "            'min_child_samples': 459,\n",
    "            'subsample': 0.9801456440779619,\n",
    "            'colsample_bytree': 0.21788689229472508,\n",
    "            'reg_alpha': 1.0879584848361499,\n",
    "            'reg_lambda': 2.824035430134392,\n",
    "        },\n",
    "        feature_names=FEATURE_SET_1\n",
    "    ),\n",
    "    get_lgbmclassifier_stacking_estimator(\n",
    "        index=7,\n",
    "        params_dict={ # Optuna study AUC: 0.7284107120755644\n",
    "            'learning_rate': 0.005337402645739864,\n",
    "            'num_leaves': 63,\n",
    "            'min_child_samples': 164,\n",
    "            'subsample': 0.8344370152901462,\n",
    "            'colsample_bytree': 0.24907941694655586,\n",
    "            'reg_alpha': 11.033723247954274,\n",
    "            'reg_lambda': 7.232987300526449,\n",
    "        },\n",
    "        feature_names=FEATURE_SET_1\n",
    "    ),\n",
    "    get_lgbmclassifier_stacking_estimator(\n",
    "        index=8,\n",
    "        params_dict={ # Optuna study AUC: 0.728549293987892\n",
    "            'learning_rate': 0.004891609003233232,\n",
    "            'num_leaves': 68,\n",
    "            'min_child_samples': 353,\n",
    "            'subsample': 0.7702836237568972,\n",
    "            'colsample_bytree': 0.21758736308082194,\n",
    "            'reg_alpha': 9.248200173575938,\n",
    "            'reg_lambda': 4.933963326601708,\n",
    "        },\n",
    "        feature_names=FEATURE_SET_1\n",
    "    ),\n",
    "    get_lgbmclassifier_stacking_estimator(\n",
    "        index=9,\n",
    "        params_dict={ # Optuna study AUC: 0.7283907091185537\n",
    "            'learning_rate': 0.030612552463613538,\n",
    "            'num_leaves': 43,\n",
    "            'min_child_samples': 386,\n",
    "            'subsample': 0.8154301174001749,\n",
    "            'colsample_bytree': 0.21385466094527275,\n",
    "            'reg_alpha': 10.72316200951412,\n",
    "            'reg_lambda': 7.779911584913586,\n",
    "        },\n",
    "        feature_names=FEATURE_SET_1\n",
    "    ),\n",
    "    get_lgbmclassifier_stacking_estimator(\n",
    "        index=10,\n",
    "        params_dict={ # Optuna study AUC: 0.7279906712847776\n",
    "            'learning_rate': 0.03919138962200246,\n",
    "            'num_leaves': 35,\n",
    "            'min_child_samples': 225,\n",
    "            'min_split_gain': 0.05023146411991609,\n",
    "            'subsample': 0.5961539507977773,\n",
    "            'colsample_bytree': 0.10669519745358953,\n",
    "            'reg_alpha': 0.3254954147190218,\n",
    "            'reg_lambda': 0.6390811014828014,\n",
    "        },\n",
    "        feature_names=FEATURE_SET_1\n",
    "    ),\n",
    "    get_lgbmclassifier_stacking_estimator(\n",
    "        index=11,\n",
    "        params_dict={ # Optuna study AUC: 0.7283403568881219\n",
    "            'learning_rate': 0.03014041766182314,\n",
    "            'num_leaves': 43,\n",
    "            'min_child_samples': 196,\n",
    "            'min_split_gain': 0.07870382373940327,\n",
    "            'subsample': 0.5710456562570112,\n",
    "            'colsample_bytree': 0.10539998953309211,\n",
    "            'reg_alpha': 1.8812331750385778,\n",
    "            'reg_lambda': 2.805371927939983,\n",
    "        },\n",
    "        feature_names=FEATURE_SET_1\n",
    "    ),\n",
    "    # get_lgbmclassifier_stacking_estimator(\n",
    "    #     index=12,\n",
    "    #     params_dict={ # Optuna study AUC: 0.7280725479607385\n",
    "    #         'learning_rate': 0.013909787667698213,\n",
    "    #         'num_leaves': 34,\n",
    "    #         'min_child_samples': 518,\n",
    "    #         'min_split_gain': 0.005739067126630592,\n",
    "    #         'subsample': 0.8126258628224535,\n",
    "    #         'colsample_bytree': 0.3068383256714443,\n",
    "    #         'reg_alpha': 9.463557129813784,\n",
    "    #         'reg_lambda': 1.3632810164682128,\n",
    "    #     },\n",
    "    #     feature_names=FEATURE_SET_1\n",
    "    # ),\n",
    "    get_lgbmclassifier_stacking_estimator(\n",
    "        index=13,\n",
    "        params_dict={ # Optuna study AUC: 0.728229455142913\n",
    "            'learning_rate': 0.023180932970380642,\n",
    "            'num_leaves': 33,\n",
    "            'min_child_samples': 495,\n",
    "            'min_split_gain': 0.0018314783202541777,\n",
    "            'subsample': 0.7808188761780435,\n",
    "            'colsample_bytree': 0.26435056293492076,\n",
    "            'reg_alpha': 12.727574938499398,\n",
    "            'reg_lambda': 5.9249848233276365,\n",
    "        },\n",
    "        feature_names=FEATURE_SET_1\n",
    "    ),\n",
    "    get_lgbmclassifier_stacking_estimator(\n",
    "        index=14,\n",
    "        params_dict={ # Optuna study AUC: 0.7281215093811463\n",
    "            'learning_rate': 0.019683073930799457,\n",
    "            'num_leaves': 29,\n",
    "            'min_child_samples': 355,\n",
    "            'min_split_gain': 0.029562222032764107,\n",
    "            'subsample': 0.639641578887935,\n",
    "            'colsample_bytree': 0.27722076729613665,\n",
    "            'reg_alpha': 9.121114441023241,\n",
    "            'reg_lambda': 1.801694790272159,\n",
    "        },\n",
    "        feature_names=FEATURE_SET_1\n",
    "    ),\n",
    "]\n",
    "\n",
    "# LGBMClassifier base models using FEATURE_SET_2\n",
    "estimators += [\n",
    "    get_lgbmclassifier_stacking_estimator(\n",
    "        index=100,\n",
    "        params_dict={ # Optuna study AUC: 0.7267113594654058\n",
    "            'learning_rate': 0.08030213631902122,\n",
    "            'num_leaves': 10,\n",
    "            'min_child_samples': 36,\n",
    "            'subsample': 0.9736572187439243,\n",
    "            'colsample_bytree': 0.6862436315985779,\n",
    "            'reg_alpha': 19.485115558852872,\n",
    "            'reg_lambda': 0.11379340962435365,\n",
    "        },\n",
    "        feature_names=FEATURE_SET_2\n",
    "    ),\n",
    "    get_lgbmclassifier_stacking_estimator(\n",
    "        index=101,\n",
    "        params_dict={ # Optuna study AUC: 0.7267412352672019\n",
    "            'learning_rate': 0.04180757514768784,\n",
    "            'num_leaves': 13,\n",
    "            'min_child_samples': 15,\n",
    "            'subsample': 0.7602307843844437,\n",
    "            'colsample_bytree': 0.6064219998341494,\n",
    "            'reg_alpha': 11.137420503168325,\n",
    "            'reg_lambda': 0.1206423560374285,\n",
    "        },\n",
    "        feature_names=FEATURE_SET_2\n",
    "    ),\n",
    "    get_lgbmclassifier_stacking_estimator(\n",
    "        index=102,\n",
    "        params_dict={ # Optuna study AUC: 0.7278304844725403\n",
    "            'learning_rate': 0.019123325048105728,\n",
    "            'num_leaves': 59,\n",
    "            'min_child_samples': 359,\n",
    "            'subsample': 0.606454771687896,\n",
    "            'colsample_bytree': 0.1444776504941246,\n",
    "            'reg_alpha': 10.581402527437783,\n",
    "            'reg_lambda': 7.154631032638869,\n",
    "        },\n",
    "        feature_names=FEATURE_SET_2\n",
    "    ),\n",
    "    get_lgbmclassifier_stacking_estimator(\n",
    "        index=103,\n",
    "        params_dict={ # Optuna study AUC: 0.7277193627577382\n",
    "            'learning_rate': 0.027309419421707757,\n",
    "            'num_leaves': 55,\n",
    "            'min_child_samples': 295,\n",
    "            'subsample': 0.475071029536569,\n",
    "            'colsample_bytree': 0.1810707161175214,\n",
    "            'reg_alpha': 8.15812166880756,\n",
    "            'reg_lambda': 9.046841588127366,\n",
    "        },\n",
    "        feature_names=FEATURE_SET_2\n",
    "    ),\n",
    "    get_lgbmclassifier_stacking_estimator(\n",
    "        index=104,\n",
    "        params_dict={ # Optuna study AUC: 0.7279646885679995\n",
    "            'learning_rate': 0.010480214785661916,\n",
    "            'num_leaves': 93,\n",
    "            'min_child_samples': 272,\n",
    "            'subsample': 0.5425209884935407,\n",
    "            'colsample_bytree': 0.14762561375602756,\n",
    "            'reg_alpha': 5.201985796455309,\n",
    "            'reg_lambda': 2.8864194162653183,\n",
    "        },\n",
    "        feature_names=FEATURE_SET_2\n",
    "    ),\n",
    "    get_lgbmclassifier_stacking_estimator(\n",
    "        index=105,\n",
    "        params_dict={ # Optuna study AUC: 0.72798144497887\n",
    "            'learning_rate': 0.01479512136381228,\n",
    "            'num_leaves': 79,\n",
    "            'min_child_samples': 429,\n",
    "            'min_split_gain': 0.03969422372541601,\n",
    "            'subsample': 0.6634754944047546,\n",
    "            'colsample_bytree': 0.14034794303042006,\n",
    "            'reg_alpha': 1.1940667970743344,\n",
    "            'reg_lambda': 18.626636485550414,\n",
    "        },\n",
    "        feature_names=FEATURE_SET_2\n",
    "    ),\n",
    "    # get_lgbmclassifier_stacking_estimator(\n",
    "    #     index=106,\n",
    "    #     params_dict={ # Optuna study AUC: 0.7274190398812063\n",
    "    #         'learning_rate': 0.021284907294550612,\n",
    "    #         'num_leaves': 43,\n",
    "    #         'min_child_samples': 189,\n",
    "    #         'min_split_gain': 0.08941374139898667,\n",
    "    #         'subsample': 0.6066732253347482,\n",
    "    #         'colsample_bytree': 0.2954477929296213,\n",
    "    #         'reg_alpha': 12.836889013195314,\n",
    "    #         'reg_lambda': 3.9894464387126454,      \n",
    "    #     },\n",
    "    #     feature_names=FEATURE_SET_2\n",
    "    # ),\n",
    "    get_lgbmclassifier_stacking_estimator(\n",
    "        index=107,\n",
    "        params_dict={ # Optuna study AUC: 0.7272209204742971\n",
    "            'learning_rate': 0.024870110418668975,\n",
    "            'num_leaves': 32,\n",
    "            'min_child_samples': 494,\n",
    "            'min_split_gain': 0.0005913639234297586,\n",
    "            'subsample': 0.7587055420676849,\n",
    "            'colsample_bytree': 0.37000848536901887,\n",
    "            'reg_alpha': 2.160187945675459,\n",
    "            'reg_lambda': 28.862050376184836,\n",
    "        },\n",
    "        feature_names=FEATURE_SET_2\n",
    "    ),\n",
    "]\n",
    "\n",
    "# LGBMClassifier base models using FEATURE_SET_3\n",
    "estimators += [\n",
    "    get_lgbmclassifier_stacking_estimator(\n",
    "        index=200,\n",
    "        params_dict={ # Optuna study AUC: 0.7276949434259791\n",
    "            'learning_rate': 0.02440401367741379,\n",
    "            'num_leaves': 47,\n",
    "            'min_child_samples': 257,\n",
    "            'min_split_gain': 0.0032121199918099664,\n",
    "            'subsample': 0.7936633846441812,\n",
    "            'colsample_bytree': 0.20767719549306815,\n",
    "            'reg_alpha': 6.0582293684829365,\n",
    "            'reg_lambda': 2.4407051521707515,\n",
    "        },\n",
    "        feature_names=FEATURE_SET_3\n",
    "    ),\n",
    "    get_lgbmclassifier_stacking_estimator(\n",
    "        index=201,\n",
    "        params_dict={ # Optuna study AUC: 0.7275512006104322\n",
    "            'learning_rate': 0.028043335648358536,\n",
    "            'num_leaves': 55,\n",
    "            'min_child_samples': 468,\n",
    "            'min_split_gain': 0.0007136696565902894,\n",
    "            'subsample': 0.8806276921497183,\n",
    "            'colsample_bytree': 0.2182810816503329,\n",
    "            'reg_alpha': 6.672897403557166,\n",
    "            'reg_lambda': 4.315365922939719,\n",
    "        },\n",
    "        feature_names=FEATURE_SET_3\n",
    "    ),\n",
    "    get_lgbmclassifier_stacking_estimator(\n",
    "        index=202,\n",
    "        params_dict={ # Optuna study AUC: 0.7267595056022728\n",
    "            'learning_rate': 0.03139400478939389,\n",
    "            'num_leaves': 14,\n",
    "            'min_child_samples': 230,\n",
    "            'min_split_gain': 0.002362112652730475,\n",
    "            'subsample': 0.668240094229581,\n",
    "            'colsample_bytree': 0.35751393630320255,\n",
    "            'reg_alpha': 0.13810005003251896,\n",
    "            'reg_lambda': 0.7113206310887704,\n",
    "        },\n",
    "        feature_names=FEATURE_SET_3\n",
    "    ),\n",
    "    get_lgbmclassifier_stacking_estimator(\n",
    "        index=203,\n",
    "        params_dict={ # Optuna study AUC: 0.7272005583249114\n",
    "            'learning_rate': 0.023919133050357203,\n",
    "            'num_leaves': 23,\n",
    "            'min_child_samples': 270,\n",
    "            'min_split_gain': 0.01757643438315908,\n",
    "            'subsample': 0.9216319828366681,\n",
    "            'colsample_bytree': 0.25307535834543227,\n",
    "            'reg_alpha': 0.15840694806695113,\n",
    "            'reg_lambda': 3.9473500486818325,\n",
    "        },\n",
    "        feature_names=FEATURE_SET_3\n",
    "    ),\n",
    "    get_lgbmclassifier_stacking_estimator(\n",
    "        index=204,\n",
    "        params_dict={ # Optuna study AUC: 0.726895431577648\n",
    "            'learning_rate': 0.0297665473868245,\n",
    "            'num_leaves': 23,\n",
    "            'min_child_samples': 223,\n",
    "            'min_split_gain': 0.08777450218531875,\n",
    "            'subsample': 0.8001257513311284,\n",
    "            'colsample_bytree': 0.4063046200565479,\n",
    "            'reg_alpha': 8.753771452700681,\n",
    "            'reg_lambda': 16.828130088381638,\n",
    "        },\n",
    "        feature_names=FEATURE_SET_3\n",
    "    ),\n",
    "    get_lgbmclassifier_stacking_estimator(\n",
    "        index=205,\n",
    "        params_dict={ # Optuna study AUC: 0.726806215190524\n",
    "            'learning_rate': 0.0226239202805405,\n",
    "            'num_leaves': 16,\n",
    "            'min_child_samples': 187,\n",
    "            'min_split_gain': 0.0008562657172401912,\n",
    "            'subsample': 0.6675455599332893,\n",
    "            'colsample_bytree': 0.527771427312258,\n",
    "            'reg_alpha': 5.554497850709915,\n",
    "            'reg_lambda': 28.82487683301403,\n",
    "        },\n",
    "        feature_names=FEATURE_SET_3\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a235765",
   "metadata": {
    "papermill": {
     "duration": 0.009933,
     "end_time": "2025-12-26T16:32:09.265934",
     "exception": false,
     "start_time": "2025-12-26T16:32:09.256001",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 8.3 XGBClassifier\n",
    "\n",
    "### 8.3.1 Helper Methods (XGBClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "24df0777",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-26T16:32:09.286986Z",
     "iopub.status.busy": "2025-12-26T16:32:09.286735Z",
     "iopub.status.idle": "2025-12-26T16:32:09.293865Z",
     "shell.execute_reply": "2025-12-26T16:32:09.293333Z"
    },
    "papermill": {
     "duration": 0.019058,
     "end_time": "2025-12-26T16:32:09.294946",
     "exception": false,
     "start_time": "2025-12-26T16:32:09.275888",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_xgbclassifier_preds(params_dict, feature_names):\n",
    "    oof_preds_accumulator = np.zeros(len(train_data))\n",
    "    test_preds_accumulator = np.zeros(len(test_data))\n",
    "\n",
    "    X_train = train_data[feature_names]\n",
    "    y_train = train_data[target_col]\n",
    "\n",
    "    for random_seed in RANDOM_SEEDS:\n",
    "        skf = StratifiedKFold(n_splits=BASE_MODEL_KFOLD_NUM_SPLITS, shuffle=True, random_state=random_seed)\n",
    "        skf_splits = skf.split(X_train, y_train)\n",
    "        skf_enumeration = enumerate(skf_splits)\n",
    "\n",
    "        seed_oof_preds = np.zeros(len(train_data))\n",
    "\n",
    "        for fold, (train_indices, validation_indices) in skf_enumeration:\n",
    "            X_train_fold = X_train.iloc[train_indices]\n",
    "            X_validation_fold = X_train.iloc[validation_indices]\n",
    "            y_train_fold = y_train.iloc[train_indices]\n",
    "            y_validation_fold = y_train.iloc[validation_indices]\n",
    "\n",
    "            model = XGBClassifier(\n",
    "                **params_dict,\n",
    "                tree_method='hist' if torch.cuda.is_available() else 'auto',\n",
    "                device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "                enable_categorical=True,\n",
    "                objective='binary:logistic',\n",
    "                eval_metric='auc',\n",
    "                early_stopping_rounds=BASE_MODEL_EARLY_STOPPING_ROUNDS,\n",
    "                n_jobs=-1,\n",
    "                random_state=random_seed,\n",
    "                verbosity=0\n",
    "            )\n",
    "            model.fit(\n",
    "                X_train_fold, y_train_fold,\n",
    "                eval_set=[(X_validation_fold, y_validation_fold)],\n",
    "                verbose=False\n",
    "            )\n",
    "\n",
    "            y_validation_pred_proba = model.predict_proba(X_validation_fold)[:, 1]\n",
    "            y_test_pred_proba = model.predict_proba(test_data[feature_names])[:, 1]\n",
    "            seed_oof_preds[validation_indices] = np.array(y_validation_pred_proba)\n",
    "            test_preds_accumulator += np.array(y_test_pred_proba)\n",
    "\n",
    "        oof_preds_accumulator += seed_oof_preds\n",
    "\n",
    "    final_oof_preds = oof_preds_accumulator / len(RANDOM_SEEDS)\n",
    "    final_test_preds = test_preds_accumulator / (BASE_MODEL_KFOLD_NUM_SPLITS * len(RANDOM_SEEDS))\n",
    "    return pd.Series(final_oof_preds), pd.Series(final_test_preds)\n",
    "\n",
    "def get_xgbclassifier_stacking_estimator(index, params_dict, feature_names):\n",
    "    return StackingEstimator(\n",
    "        name=f\"XGBClassifier_{index}\",\n",
    "        params_dict=params_dict,\n",
    "        feature_names=feature_names,\n",
    "        get_preds=get_xgbclassifier_preds\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e9c5fd",
   "metadata": {
    "papermill": {
     "duration": 0.010059,
     "end_time": "2025-12-26T16:32:09.315054",
     "exception": false,
     "start_time": "2025-12-26T16:32:09.304995",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 8.3.2 Add Estimators (XGBClassifier)\n",
    "\n",
    "Add XGBClassifier estimators to list that StackingPredictionsRetriever will process. Hyperparameters were found using Optuna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2897409e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-26T16:32:09.336866Z",
     "iopub.status.busy": "2025-12-26T16:32:09.336637Z",
     "iopub.status.idle": "2025-12-26T16:32:09.355204Z",
     "shell.execute_reply": "2025-12-26T16:32:09.354502Z"
    },
    "papermill": {
     "duration": 0.03088,
     "end_time": "2025-12-26T16:32:09.356226",
     "exception": false,
     "start_time": "2025-12-26T16:32:09.325346",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# XGBClassifier base models using FEATURE_SET_1\n",
    "estimators += [\n",
    "    # get_xgbclassifier_stacking_estimator(\n",
    "    #     index=1,\n",
    "    #     params_dict={ # Optuna study AUC: 0.7275219804910846\n",
    "    #         'n_estimators': 30000,\n",
    "    #         'learning_rate': 0.00985498815107458,\n",
    "    #         'max_depth': 3,\n",
    "    #         'subsample': 0.975836120137461,\n",
    "    #         'colsample_bytree': 0.5411854284303592,\n",
    "    #         'alpha': 9.940781978752474,\n",
    "    #         'gamma': 0.008422323405815038,\n",
    "    #         'lambda': 0.025214960531620187,\n",
    "    #         'min_child_weight': 12,\n",
    "    #     },\n",
    "    #     feature_names=FEATURE_SET_1\n",
    "    # ),\n",
    "    get_xgbclassifier_stacking_estimator(\n",
    "        index=2,\n",
    "        params_dict={ # Optuna study AUC: 0.7273817150393508\n",
    "            'n_estimators': 30000,\n",
    "            'learning_rate': 0.047179227853488916,\n",
    "            'max_depth': 3,\n",
    "            'subsample': 0.9561594029099818,\n",
    "            'colsample_bytree': 0.5200809916944509,\n",
    "            'alpha': 9.323686821094613,\n",
    "            'gamma': 0.06513704074541844,\n",
    "            'lambda': 0.07573405175712218,\n",
    "            'min_child_weight': 14,\n",
    "        },\n",
    "        feature_names=FEATURE_SET_1\n",
    "    ),\n",
    "    get_xgbclassifier_stacking_estimator(\n",
    "        index=3,\n",
    "        params_dict={ # Optuna study AUC: 0.7274144144696422\n",
    "            'n_estimators': 30000,\n",
    "            'learning_rate': 0.06778303256075534,\n",
    "            'max_depth': 3,\n",
    "            'subsample': 0.9750702612583769,\n",
    "            'colsample_bytree': 0.5164463777572837,\n",
    "            'alpha': 6.677223824702266,\n",
    "            'gamma': 0.06627215758548254,\n",
    "            'lambda': 0.10239210156952944,\n",
    "            'min_child_weight': 17,\n",
    "        },\n",
    "        feature_names=FEATURE_SET_1\n",
    "    ),\n",
    "    get_xgbclassifier_stacking_estimator(\n",
    "        index=4,\n",
    "        params_dict={ # Optuna study AUC: 0.7263868488191946\n",
    "            'n_estimators': 30000,\n",
    "            'learning_rate': 0.00992002978574334,\n",
    "            'max_depth': 6,\n",
    "            'subsample': 0.6885700003314461,\n",
    "            'colsample_bytree': 0.5082842329050175,\n",
    "            'alpha': 4.042835803115786,\n",
    "            'gamma': 0.19033575052721494,\n",
    "            'lambda': 1.4531584526994292,\n",
    "            'min_child_weight': 79,\n",
    "        },\n",
    "        feature_names=FEATURE_SET_1\n",
    "    ),\n",
    "    get_xgbclassifier_stacking_estimator(\n",
    "        index=5,\n",
    "        params_dict={ # Optuna study AUC: 0.7261995858097773\n",
    "            'n_estimators': 30000,\n",
    "            'learning_rate': 0.005092159244819224,\n",
    "            'max_depth': 8,\n",
    "            'subsample': 0.6985482460232558,\n",
    "            'colsample_bytree': 0.5002716122370332,\n",
    "            'alpha': 0.5442317401534714,\n",
    "            'gamma': 0.9101677712528158,\n",
    "            'lambda': 1.4849248721792976,\n",
    "            'min_child_weight': 86,\n",
    "        },\n",
    "        feature_names=FEATURE_SET_1\n",
    "    ),\n",
    "    get_xgbclassifier_stacking_estimator(\n",
    "        index=6,\n",
    "        params_dict={ # Optuna study AUC: 0.7276697068781471\n",
    "            'n_estimators': 30000,\n",
    "            'learning_rate': 0.02372054087355925,\n",
    "            'max_depth': 15,\n",
    "            'subsample': 0.926157066763106,\n",
    "            'colsample_bytree': 0.08129535635192518,\n",
    "            'alpha': 0.8089388335721612,\n",
    "            'gamma': 2.155286395452771,\n",
    "            'lambda': 0.0032987476048184643,\n",
    "            'min_child_weight': 106,\n",
    "        },\n",
    "        feature_names=FEATURE_SET_1\n",
    "    ),\n",
    "    get_xgbclassifier_stacking_estimator(\n",
    "        index=7,\n",
    "        params_dict={ # Optuna study AUC: 0.7275829624569639\n",
    "            'n_estimators': 30000,\n",
    "            'learning_rate': 0.03082838156810333,\n",
    "            'max_depth': 15,\n",
    "            'subsample': 0.9355182595718994,\n",
    "            'colsample_bytree': 0.04726638467138021,\n",
    "            'alpha': 0.0006789202250200641,\n",
    "            'gamma': 2.0423438455052727,\n",
    "            'lambda': 0.0011440505027664526,\n",
    "            'min_child_weight': 99,\n",
    "        },\n",
    "        feature_names=FEATURE_SET_1\n",
    "    ),\n",
    "    get_xgbclassifier_stacking_estimator(\n",
    "        index=8,\n",
    "        params_dict={ # Optuna study AUC: 0.726481873087293\n",
    "            'n_estimators': 30000,\n",
    "            'learning_rate': 0.016778981442240135,\n",
    "            'max_depth': 7,\n",
    "            'subsample': 0.8535639994326161,\n",
    "            'colsample_bytree': 0.3601691434581828,\n",
    "            'alpha': 0.11594863371568545,\n",
    "            'gamma': 1.7534917855981467,\n",
    "            'lambda': 90.18952430127791,\n",
    "            'min_child_weight': 232,\n",
    "        },\n",
    "        feature_names=FEATURE_SET_1\n",
    "    ),\n",
    "    get_xgbclassifier_stacking_estimator(\n",
    "        index=9,\n",
    "        params_dict={ # Optuna study AUC: 0.7269334752728761\n",
    "            'n_estimators': 30000,\n",
    "            'learning_rate': 0.006633818069700181,\n",
    "            'max_depth': 6,\n",
    "            'subsample': 0.8928869407116772,\n",
    "            'colsample_bytree': 0.302247524251857,\n",
    "            'alpha': 0.7271655035016035,\n",
    "            'gamma': 0.10791293819417,\n",
    "            'lambda': 31.358637398115444,\n",
    "            'min_child_weight': 211,\n",
    "        },\n",
    "        feature_names=FEATURE_SET_1\n",
    "    ),\n",
    "    get_xgbclassifier_stacking_estimator(\n",
    "        index=10,\n",
    "        params_dict={ # Optuna study AUC: 0.7267016243216274\n",
    "            'n_estimators': 30000,\n",
    "            'learning_rate': 0.01541504042919444,\n",
    "            'max_depth': 6,\n",
    "            'subsample': 0.898530669203575,\n",
    "            'colsample_bytree': 0.44350929512696624,\n",
    "            'alpha': 0.11291553156416392,\n",
    "            'gamma': 0.22741023384123787,\n",
    "            'lambda': 4.885615406166625,\n",
    "            'min_child_weight': 204,\n",
    "        },\n",
    "        feature_names=FEATURE_SET_1\n",
    "    ),\n",
    "    get_xgbclassifier_stacking_estimator(\n",
    "        index=11,\n",
    "        params_dict={ # Optuna study AUC: 0.7268725441663415\n",
    "            'n_estimators': 30000,\n",
    "            'learning_rate': 0.008722544820454547,\n",
    "            'max_depth': 3,\n",
    "            'subsample': 0.9322104305014642,\n",
    "            'colsample_bytree': 0.8603707435128638,\n",
    "            'alpha': 0.4706079902529737,\n",
    "            'gamma': 1.1134986358862557,\n",
    "            'lambda': 0.2571975355765451,\n",
    "            'min_child_weight': 178,\n",
    "        },\n",
    "        feature_names=FEATURE_SET_1\n",
    "    ),\n",
    "]\n",
    "\n",
    "# XGBClassifier base models using FEATURE_SET_2\n",
    "estimators += [\n",
    "    get_xgbclassifier_stacking_estimator(\n",
    "        index=100,\n",
    "        params_dict={ # Optuna study AUC: 0.7262159108318079\n",
    "            'n_estimators': 30000,\n",
    "            'learning_rate': 0.011859808032021718,\n",
    "            'max_depth': 3,\n",
    "            'subsample': 0.9372116555018073,\n",
    "            'colsample_bytree': 0.9755650095828481,\n",
    "            'alpha': 11.83079224267289,\n",
    "            'gamma': 0.47957759727475824,\n",
    "            'lambda': 1.5631226520724053,\n",
    "            'min_child_weight': 38,\n",
    "        },\n",
    "        feature_names=FEATURE_SET_2\n",
    "    ),\n",
    "    get_xgbclassifier_stacking_estimator(\n",
    "        index=101,\n",
    "        params_dict={ # Optuna study AUC: 0.7273837184769656\n",
    "            'n_estimators': 30000,\n",
    "            'learning_rate': 0.027980355641639105,\n",
    "            'max_depth': 22,\n",
    "            'subsample': 0.9470465534609164,\n",
    "            'colsample_bytree': 0.037050915293494434,\n",
    "            'alpha': 5.919291266337225e-07,\n",
    "            'gamma': 2.2947298746512588,\n",
    "            'lambda': 0.00937046969544808,\n",
    "            'min_child_weight': 109,\n",
    "        },\n",
    "        feature_names=FEATURE_SET_2\n",
    "    ),\n",
    "    get_xgbclassifier_stacking_estimator(\n",
    "        index=102,\n",
    "        params_dict={ # Optuna study AUC: 0.7273458047806104\n",
    "            'n_estimators': 30000,\n",
    "            'learning_rate': 0.046164174619963895,\n",
    "            'max_depth': 15,\n",
    "            'subsample': 0.990211202548381,\n",
    "            'colsample_bytree': 0.03512726983073455,\n",
    "            'alpha': 3.6758967225029957e-06,\n",
    "            'gamma': 2.367535362401481,\n",
    "            'lambda': 0.007800881812139096,\n",
    "            'min_child_weight': 59,\n",
    "        },\n",
    "        feature_names=FEATURE_SET_2\n",
    "    ),\n",
    "    get_xgbclassifier_stacking_estimator(\n",
    "        index=103,\n",
    "        params_dict={ # Optuna study AUC: 0.7271766457975763\n",
    "            'n_estimators': 30000,\n",
    "            'learning_rate': 0.029608261279916184,\n",
    "            'max_depth': 12,\n",
    "            'subsample': 0.8302439848584828,\n",
    "            'colsample_bytree': 0.03599141865024359,\n",
    "            'alpha': 2.370851147241027e-08,\n",
    "            'gamma': 2.488161026703388,\n",
    "            'lambda': 0.03579562780682341,\n",
    "            'min_child_weight': 79,\n",
    "        },\n",
    "        feature_names=FEATURE_SET_2\n",
    "    ),\n",
    "    # get_xgbclassifier_stacking_estimator(\n",
    "    #     index=104,\n",
    "    #     params_dict={ # Optuna study AUC: 0.727313377922011\n",
    "    #         'learning_rate': 0.018830984253997193,\n",
    "    #         'max_depth': 17,\n",
    "    #         'subsample': 0.9721977334617213,\n",
    "    #         'colsample_bytree': 0.04297481358590687,\n",
    "    #         'alpha': 1.0510194593417237,\n",
    "    #         'gamma': 1.9540524911604567,\n",
    "    #         'lambda': 0.20230023217673998,\n",
    "    #         'min_child_weight': 96,\n",
    "    #     },\n",
    "    #     feature_names=FEATURE_SET_2\n",
    "    # ),\n",
    "    # get_xgbclassifier_stacking_estimator(\n",
    "    #     index=105,\n",
    "    #     params_dict={ # Optuna study AUC: 0.7271054074166622\n",
    "    #         'learning_rate': 0.016139246094996982,\n",
    "    #         'max_depth': 18,\n",
    "    #         'subsample': 0.9564519772365436,\n",
    "    #         'colsample_bytree': 0.04057400543991785,\n",
    "    #         'alpha': 0.13886943367841298,\n",
    "    #         'gamma': 2.103458500140603,\n",
    "    #         'lambda': 0.271808754880666,\n",
    "    #         'min_child_weight': 189,\n",
    "    #     },\n",
    "    #     feature_names=FEATURE_SET_2\n",
    "    # ),\n",
    "    get_xgbclassifier_stacking_estimator(\n",
    "        index=106,\n",
    "        params_dict={ # Optuna study AUC: 0.7267722841676081\n",
    "            'n_estimators': 30000,\n",
    "            'learning_rate': 0.0065043250205180534,\n",
    "            'max_depth': 10,\n",
    "            'subsample': 0.8945044057959924,\n",
    "            'colsample_bytree': 0.10087167584713677,\n",
    "            'alpha': 1.8921170347131862,\n",
    "            'gamma': 2.865040204519563,\n",
    "            'lambda': 3.335272814695635,\n",
    "            'min_child_weight': 83,\n",
    "        },\n",
    "        feature_names=FEATURE_SET_2\n",
    "    ),\n",
    "    get_xgbclassifier_stacking_estimator(\n",
    "        index=107,\n",
    "        params_dict={ # Optuna study AUC: 0.7266176698378618\n",
    "            'n_estimators': 30000,\n",
    "            'learning_rate': 0.007924773077477808,\n",
    "            'max_depth': 11,\n",
    "            'subsample': 0.8016340992158001,\n",
    "            'colsample_bytree': 0.11042333005514733,\n",
    "            'alpha': 3.794091823169151,\n",
    "            'gamma': 2.0651797737669173,\n",
    "            'lambda': 9.274540244581045,\n",
    "            'min_child_weight': 100,\n",
    "        },\n",
    "        feature_names=FEATURE_SET_2\n",
    "    ),\n",
    "]\n",
    "\n",
    "# XGBClassifier base models using FEATURE_SET_3\n",
    "estimators += [\n",
    "    get_xgbclassifier_stacking_estimator(\n",
    "        index=200,\n",
    "        params_dict={ # Optuna study AUC: 0.72662484358974\n",
    "            'n_estimators': 30000,\n",
    "            'learning_rate': 0.004722170637610488,\n",
    "            'max_depth': 8,\n",
    "            'subsample': 0.9463619229520444,\n",
    "            'colsample_bytree': 0.10815535487923243,\n",
    "            'alpha': 0.2266410700990591,\n",
    "            'gamma': 1.5254201232797373,\n",
    "            'lambda': 1.1151246586327885,\n",
    "            'min_child_weight': 59,\n",
    "        },\n",
    "        feature_names=FEATURE_SET_3\n",
    "    ),\n",
    "    get_xgbclassifier_stacking_estimator(\n",
    "        index=201,\n",
    "        params_dict={ # Optuna study AUC: 0.7263789553843397\n",
    "            'n_estimators': 30000,\n",
    "            'learning_rate': 0.004111790393241001,\n",
    "            'max_depth': 5,\n",
    "            'subsample': 0.9300933146213577,\n",
    "            'colsample_bytree': 0.1493961853415991,\n",
    "            'alpha': 0.12456513604853313,\n",
    "            'gamma': 0.9403251116679338,\n",
    "            'lambda': 8.136098473974746,\n",
    "            'min_child_weight': 114,\n",
    "        },\n",
    "        feature_names=FEATURE_SET_3\n",
    "    ),\n",
    "    get_xgbclassifier_stacking_estimator(\n",
    "        index=202,\n",
    "        params_dict={ # Optuna study AUC: 0.7262290159966112\n",
    "            'n_estimators': 30000,\n",
    "            'learning_rate': 0.05550672575020678,\n",
    "            'max_depth': 3,\n",
    "            'subsample': 0.9477018401213056,\n",
    "            'colsample_bytree': 0.4493907534694529,\n",
    "            'alpha': 4.47848584018269,\n",
    "            'gamma': 0.9155407876651237,\n",
    "            'lambda': 9.972213918787583,\n",
    "            'min_child_weight': 56,\n",
    "        },\n",
    "        feature_names=FEATURE_SET_3\n",
    "    ),\n",
    "    get_xgbclassifier_stacking_estimator(\n",
    "        index=203,\n",
    "        params_dict={ # Optuna study AUC: 0.7262949541805783\n",
    "            'n_estimators': 30000,\n",
    "            'learning_rate': 0.06597548869194442,\n",
    "            'max_depth': 3,\n",
    "            'subsample': 0.9292415774505621,\n",
    "            'colsample_bytree': 0.2419008659034062,\n",
    "            'alpha': 4.150553881297006,\n",
    "            'gamma': 0.49205261469458217,\n",
    "            'lambda': 2.043007240380063,\n",
    "            'min_child_weight': 6,\n",
    "        },\n",
    "        feature_names=FEATURE_SET_3\n",
    "    ),\n",
    "    get_xgbclassifier_stacking_estimator(\n",
    "        index=204,\n",
    "        params_dict={ # Optuna study AUC: 0.726241045292214\n",
    "            'n_estimators': 30000,\n",
    "            'learning_rate': 0.031040864135625165,\n",
    "            'max_depth': 4,\n",
    "            'subsample': 0.9380337531391458,\n",
    "            'colsample_bytree': 0.10194279727241388,\n",
    "            'alpha': 4.329119623946634,\n",
    "            'gamma': 0.9675602501164753,\n",
    "            'lambda': 1.0345146162650225,\n",
    "            'min_child_weight': 22,\n",
    "        },\n",
    "        feature_names=FEATURE_SET_3\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74369bc",
   "metadata": {
    "papermill": {
     "duration": 0.010118,
     "end_time": "2025-12-26T16:32:09.376271",
     "exception": false,
     "start_time": "2025-12-26T16:32:09.366153",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 8.4 Number of Base Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "12f394ed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-26T16:32:09.397088Z",
     "iopub.status.busy": "2025-12-26T16:32:09.396905Z",
     "iopub.status.idle": "2025-12-26T16:32:09.400505Z",
     "shell.execute_reply": "2025-12-26T16:32:09.399821Z"
    },
    "papermill": {
     "duration": 0.01532,
     "end_time": "2025-12-26T16:32:09.401512",
     "exception": false,
     "start_time": "2025-12-26T16:32:09.386192",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of base models: 56\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total number of base models: {len(estimators)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1cfa1b8",
   "metadata": {
    "papermill": {
     "duration": 0.01036,
     "end_time": "2025-12-26T16:32:09.421954",
     "exception": false,
     "start_time": "2025-12-26T16:32:09.411594",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 9. Base Model Predictions\n",
    "\n",
    "## 9.1 Get Base Model Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f4bdaf7e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-26T16:32:09.443552Z",
     "iopub.status.busy": "2025-12-26T16:32:09.443030Z",
     "iopub.status.idle": "2025-12-26T16:54:12.814191Z",
     "shell.execute_reply": "2025-12-26T16:54:12.813578Z"
    },
    "papermill": {
     "duration": 1323.383629,
     "end_time": "2025-12-26T16:54:12.815558",
     "exception": false,
     "start_time": "2025-12-26T16:32:09.431929",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Importing predictions..\n",
      "[INFO] 54 train predictions were imported:\n",
      "CatBoostClassifier_1 (5b3133609cb7932f4c272f1494d1f6b0), CatBoostClassifier_2 (c5054ebf7585fcc187c40442e3cbbcff), CatBoostClassifier_3 (a2e94c97067190eb52e25c8f0d0b1b81), CatBoostClassifier_100 (8438003294d3bfe9b6020e7cbcc8f250), CatBoostClassifier_101 (939b20ef655c1c22f3cc5749a9dc0de7), CatBoostClassifier_103 (985c7cd22a70cbfa293f37e23c80da7c), CatBoostClassifier_104 (9db0b212db19e590b51a1af7fe65b7a0), CatBoostClassifier_105 (faa5d67c16c906b11264ade5016c0635), CatBoostClassifier_200 (193b4e2a59c2eac5f73a997fb968592b), CatBoostClassifier_201 (0aabd4de7e66e705cd06f7a5af29de6d), LGBMClassifier_2 (756b644eceb4b410ad34d055d7a2a7ad), LGBMClassifier_3 (12e279a3f79f03854fcc391dd7dbd023), LGBMClassifier_6 (a0c02e5dec4bd4467961cd96901740a9), LGBMClassifier_7 (0d5c160d0c75a01607ee65e1309f0f3e), LGBMClassifier_8 (74451732f18860140bc6e208f17167ec), LGBMClassifier_9 (535b190c7fd27867f25b36c846af5749), LGBMClassifier_10 (d768bcfaea71d6315f8a3aec15264fd3), LGBMClassifier_11 (434b9911e9f067fa638a7372ae9de3b7), LGBMClassifier_13 (57b6180bb789e56517729d29e1113e9f), LGBMClassifier_14 (e9cd9321a6617423d09c86651868827b), LGBMClassifier_100 (e9308d5ef0fe3d2e21b23b76c24abf10), LGBMClassifier_101 (4184d68f723388204df6096a194619e3), LGBMClassifier_102 (15ce46227658fcf66d9fa61aa8770bd6), LGBMClassifier_103 (0f77b5c92200f4372a73a16c28e66a0c), LGBMClassifier_104 (03f3c6e632f5fffd70ec8a4c916a9b3a), LGBMClassifier_105 (b69a0d12e5e9a0dea6e97fd93d6e83ca), LGBMClassifier_107 (209378e46f2c00fc2bab71cb2a4b2806), LGBMClassifier_200 (595825024cf6728eede6526c34137bea), LGBMClassifier_201 (6fc3b3d774e94599d833c0f91c5a3bd9), LGBMClassifier_202 (be93bc9c94b117d579fef02264e6d81f), LGBMClassifier_203 (d42c5caf1073a9a9e7440490645eea6f), LGBMClassifier_204 (508d4bd009535bc725e1412c3fccd89b), LGBMClassifier_205 (58b38247cf5e26b3ca8dcd12b838e5fd), XGBClassifier_2 (f9455e9875a464c78cd90b60584d3f1c), XGBClassifier_3 (4a02ebc078c643d903a00ddcfb0f3301), XGBClassifier_4 (41fcdf694bf252b540f6c6d28d63eb45), XGBClassifier_5 (04de438dafcf1a1b67a0dbfa0e2bc0d1), XGBClassifier_6 (dd58a49196a10b1dfc83dd004b5cae1b), XGBClassifier_7 (159c6c407450077591a665dea6ecacfe), XGBClassifier_8 (020a786ac38ddae2dcd3a381a705107e), XGBClassifier_9 (f04e3fec7fdc9e5bdcfce6e8ccafda3d), XGBClassifier_10 (3eba9b3facf87fe1c9ba57c1d875c799), XGBClassifier_11 (a13eb68990690e9fc4bef373e814ae80), XGBClassifier_100 (a0e17bbe4a8e31c8ed28c4ff78fd8568), XGBClassifier_101 (34a22ef42d0624dee9c89abc9971c23f), XGBClassifier_102 (3a16502492397945fe3feec5f3ce94f2), XGBClassifier_103 (ea8d1ee90caf97bacd865adb7e87423d), XGBClassifier_106 (99eada29cafaf2aedd156545890e7235), XGBClassifier_107 (d3abb2fc7bb37f99aed2d658a0f5bee3), XGBClassifier_200 (0fd8d373da5ad5bf5148772e49bf2d3c), XGBClassifier_201 (4a18eb80c90bfe9108b6bedaecacec3a), XGBClassifier_202 (29c6bbceec36b1810be94900531114e9), XGBClassifier_203 (b8dc34df04edf0b3ea42293d5436bea7), XGBClassifier_204 (1e5bc330e406cb4a18b5c76cb9907cbb)\n",
      "[INFO] 54 test predictions were imported:\n",
      "CatBoostClassifier_1 (5b3133609cb7932f4c272f1494d1f6b0), CatBoostClassifier_2 (c5054ebf7585fcc187c40442e3cbbcff), CatBoostClassifier_3 (a2e94c97067190eb52e25c8f0d0b1b81), CatBoostClassifier_100 (8438003294d3bfe9b6020e7cbcc8f250), CatBoostClassifier_101 (939b20ef655c1c22f3cc5749a9dc0de7), CatBoostClassifier_103 (985c7cd22a70cbfa293f37e23c80da7c), CatBoostClassifier_104 (9db0b212db19e590b51a1af7fe65b7a0), CatBoostClassifier_105 (faa5d67c16c906b11264ade5016c0635), CatBoostClassifier_200 (193b4e2a59c2eac5f73a997fb968592b), CatBoostClassifier_201 (0aabd4de7e66e705cd06f7a5af29de6d), LGBMClassifier_2 (756b644eceb4b410ad34d055d7a2a7ad), LGBMClassifier_3 (12e279a3f79f03854fcc391dd7dbd023), LGBMClassifier_6 (a0c02e5dec4bd4467961cd96901740a9), LGBMClassifier_7 (0d5c160d0c75a01607ee65e1309f0f3e), LGBMClassifier_8 (74451732f18860140bc6e208f17167ec), LGBMClassifier_9 (535b190c7fd27867f25b36c846af5749), LGBMClassifier_10 (d768bcfaea71d6315f8a3aec15264fd3), LGBMClassifier_11 (434b9911e9f067fa638a7372ae9de3b7), LGBMClassifier_13 (57b6180bb789e56517729d29e1113e9f), LGBMClassifier_14 (e9cd9321a6617423d09c86651868827b), LGBMClassifier_100 (e9308d5ef0fe3d2e21b23b76c24abf10), LGBMClassifier_101 (4184d68f723388204df6096a194619e3), LGBMClassifier_102 (15ce46227658fcf66d9fa61aa8770bd6), LGBMClassifier_103 (0f77b5c92200f4372a73a16c28e66a0c), LGBMClassifier_104 (03f3c6e632f5fffd70ec8a4c916a9b3a), LGBMClassifier_105 (b69a0d12e5e9a0dea6e97fd93d6e83ca), LGBMClassifier_107 (209378e46f2c00fc2bab71cb2a4b2806), LGBMClassifier_200 (595825024cf6728eede6526c34137bea), LGBMClassifier_201 (6fc3b3d774e94599d833c0f91c5a3bd9), LGBMClassifier_202 (be93bc9c94b117d579fef02264e6d81f), LGBMClassifier_203 (d42c5caf1073a9a9e7440490645eea6f), LGBMClassifier_204 (508d4bd009535bc725e1412c3fccd89b), LGBMClassifier_205 (58b38247cf5e26b3ca8dcd12b838e5fd), XGBClassifier_2 (f9455e9875a464c78cd90b60584d3f1c), XGBClassifier_3 (4a02ebc078c643d903a00ddcfb0f3301), XGBClassifier_4 (41fcdf694bf252b540f6c6d28d63eb45), XGBClassifier_5 (04de438dafcf1a1b67a0dbfa0e2bc0d1), XGBClassifier_6 (dd58a49196a10b1dfc83dd004b5cae1b), XGBClassifier_7 (159c6c407450077591a665dea6ecacfe), XGBClassifier_8 (020a786ac38ddae2dcd3a381a705107e), XGBClassifier_9 (f04e3fec7fdc9e5bdcfce6e8ccafda3d), XGBClassifier_10 (3eba9b3facf87fe1c9ba57c1d875c799), XGBClassifier_11 (a13eb68990690e9fc4bef373e814ae80), XGBClassifier_100 (a0e17bbe4a8e31c8ed28c4ff78fd8568), XGBClassifier_101 (34a22ef42d0624dee9c89abc9971c23f), XGBClassifier_102 (3a16502492397945fe3feec5f3ce94f2), XGBClassifier_103 (ea8d1ee90caf97bacd865adb7e87423d), XGBClassifier_106 (99eada29cafaf2aedd156545890e7235), XGBClassifier_107 (d3abb2fc7bb37f99aed2d658a0f5bee3), XGBClassifier_200 (0fd8d373da5ad5bf5148772e49bf2d3c), XGBClassifier_201 (4a18eb80c90bfe9108b6bedaecacec3a), XGBClassifier_202 (29c6bbceec36b1810be94900531114e9), XGBClassifier_203 (b8dc34df04edf0b3ea42293d5436bea7), XGBClassifier_204 (1e5bc330e406cb4a18b5c76cb9907cbb)\n",
      "[INFO] Finished importing predictions\n",
      "[INFO] Syncing predictions..\n",
      "[INFO] No columns for training predictions were dropped\n",
      "[INFO] No columns for test predictions were dropped\n",
      "[INFO] Finished syncing predictions\n",
      "[INFO] Getting predictions..\n",
      "[INFO] Getting predictions for estimator CatBoostClassifier_202 (44630aa57085636ff4b42ef84843dad4)\n",
      "[INFO] Saved predictions\n",
      "[INFO] Getting predictions for estimator CatBoostClassifier_203 (2b17c55ab32780d9a9f1a913c64ad718)\n",
      "[INFO] Saved predictions\n",
      "[INFO] Skipped retrieving predictions for following estimators as their current ones are not stale:\n",
      "CatBoostClassifier_1 (5b3133609cb7932f4c272f1494d1f6b0), CatBoostClassifier_100 (8438003294d3bfe9b6020e7cbcc8f250), CatBoostClassifier_101 (939b20ef655c1c22f3cc5749a9dc0de7), CatBoostClassifier_103 (985c7cd22a70cbfa293f37e23c80da7c), CatBoostClassifier_104 (9db0b212db19e590b51a1af7fe65b7a0), CatBoostClassifier_105 (faa5d67c16c906b11264ade5016c0635), CatBoostClassifier_2 (c5054ebf7585fcc187c40442e3cbbcff), CatBoostClassifier_200 (193b4e2a59c2eac5f73a997fb968592b), CatBoostClassifier_201 (0aabd4de7e66e705cd06f7a5af29de6d), CatBoostClassifier_3 (a2e94c97067190eb52e25c8f0d0b1b81), LGBMClassifier_10 (d768bcfaea71d6315f8a3aec15264fd3), LGBMClassifier_100 (e9308d5ef0fe3d2e21b23b76c24abf10), LGBMClassifier_101 (4184d68f723388204df6096a194619e3), LGBMClassifier_102 (15ce46227658fcf66d9fa61aa8770bd6), LGBMClassifier_103 (0f77b5c92200f4372a73a16c28e66a0c), LGBMClassifier_104 (03f3c6e632f5fffd70ec8a4c916a9b3a), LGBMClassifier_105 (b69a0d12e5e9a0dea6e97fd93d6e83ca), LGBMClassifier_107 (209378e46f2c00fc2bab71cb2a4b2806), LGBMClassifier_11 (434b9911e9f067fa638a7372ae9de3b7), LGBMClassifier_13 (57b6180bb789e56517729d29e1113e9f), LGBMClassifier_14 (e9cd9321a6617423d09c86651868827b), LGBMClassifier_2 (756b644eceb4b410ad34d055d7a2a7ad), LGBMClassifier_200 (595825024cf6728eede6526c34137bea), LGBMClassifier_201 (6fc3b3d774e94599d833c0f91c5a3bd9), LGBMClassifier_202 (be93bc9c94b117d579fef02264e6d81f), LGBMClassifier_203 (d42c5caf1073a9a9e7440490645eea6f), LGBMClassifier_204 (508d4bd009535bc725e1412c3fccd89b), LGBMClassifier_205 (58b38247cf5e26b3ca8dcd12b838e5fd), LGBMClassifier_3 (12e279a3f79f03854fcc391dd7dbd023), LGBMClassifier_6 (a0c02e5dec4bd4467961cd96901740a9), LGBMClassifier_7 (0d5c160d0c75a01607ee65e1309f0f3e), LGBMClassifier_8 (74451732f18860140bc6e208f17167ec), LGBMClassifier_9 (535b190c7fd27867f25b36c846af5749), XGBClassifier_10 (3eba9b3facf87fe1c9ba57c1d875c799), XGBClassifier_100 (a0e17bbe4a8e31c8ed28c4ff78fd8568), XGBClassifier_101 (34a22ef42d0624dee9c89abc9971c23f), XGBClassifier_102 (3a16502492397945fe3feec5f3ce94f2), XGBClassifier_103 (ea8d1ee90caf97bacd865adb7e87423d), XGBClassifier_106 (99eada29cafaf2aedd156545890e7235), XGBClassifier_107 (d3abb2fc7bb37f99aed2d658a0f5bee3), XGBClassifier_11 (a13eb68990690e9fc4bef373e814ae80), XGBClassifier_2 (f9455e9875a464c78cd90b60584d3f1c), XGBClassifier_200 (0fd8d373da5ad5bf5148772e49bf2d3c), XGBClassifier_201 (4a18eb80c90bfe9108b6bedaecacec3a), XGBClassifier_202 (29c6bbceec36b1810be94900531114e9), XGBClassifier_203 (b8dc34df04edf0b3ea42293d5436bea7), XGBClassifier_204 (1e5bc330e406cb4a18b5c76cb9907cbb), XGBClassifier_3 (4a02ebc078c643d903a00ddcfb0f3301), XGBClassifier_4 (41fcdf694bf252b540f6c6d28d63eb45), XGBClassifier_5 (04de438dafcf1a1b67a0dbfa0e2bc0d1), XGBClassifier_6 (dd58a49196a10b1dfc83dd004b5cae1b), XGBClassifier_7 (159c6c407450077591a665dea6ecacfe), XGBClassifier_8 (020a786ac38ddae2dcd3a381a705107e), XGBClassifier_9 (f04e3fec7fdc9e5bdcfce6e8ccafda3d)\n",
      "[INFO] Finished getting all predictions\n"
     ]
    }
   ],
   "source": [
    "stacking_preds_retriever = StackingPredictionsRetriever(\n",
    "    estimators=estimators,\n",
    "    working_dir_path=\"/kaggle/working/\",\n",
    "    train_preds_filename=\"base_models_train_preds\",\n",
    "    test_preds_filename=\"base_models_test_preds\",\n",
    "    preds_save_interval=1\n",
    ")\n",
    "stacking_preds_retriever.import_preds(\"/kaggle/input/diabetes-prediction-challenge-base-model-preds/\")\n",
    "stacking_preds_retriever.sync_preds()\n",
    "stacking_preds_retriever.get_preds()\n",
    "\n",
    "base_model_train_preds, base_model_test_preds = stacking_preds_retriever.get_current_train_and_test_preds()\n",
    "base_model_train_preds.sort_index(axis=1, inplace=True, key=lambda index: index.map(lambda col_name: (col_name.split(\"_\")[0], int(col_name.split()[0].split(\"_\")[-1]))))\n",
    "base_model_test_preds.sort_index(axis=1, inplace=True, key=lambda index: index.map(lambda col_name: (col_name.split(\"_\")[0], int(col_name.split()[0].split(\"_\")[-1]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7154f0",
   "metadata": {
    "papermill": {
     "duration": 0.010574,
     "end_time": "2025-12-26T16:54:12.837383",
     "exception": false,
     "start_time": "2025-12-26T16:54:12.826809",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 9.2 Base Models AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7a757344",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-26T16:54:12.860010Z",
     "iopub.status.busy": "2025-12-26T16:54:12.859733Z",
     "iopub.status.idle": "2025-12-26T16:54:21.977393Z",
     "shell.execute_reply": "2025-12-26T16:54:21.976665Z"
    },
    "papermill": {
     "duration": 9.130291,
     "end_time": "2025-12-26T16:54:21.978649",
     "exception": false,
     "start_time": "2025-12-26T16:54:12.848358",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMClassifier_6 (a0c02e5dec4bd4467961cd96901740a9)          0.729329\n",
       "LGBMClassifier_11 (434b9911e9f067fa638a7372ae9de3b7)         0.729276\n",
       "LGBMClassifier_7 (0d5c160d0c75a01607ee65e1309f0f3e)          0.729191\n",
       "LGBMClassifier_9 (535b190c7fd27867f25b36c846af5749)          0.729190\n",
       "LGBMClassifier_13 (57b6180bb789e56517729d29e1113e9f)         0.729107\n",
       "LGBMClassifier_10 (d768bcfaea71d6315f8a3aec15264fd3)         0.729059\n",
       "LGBMClassifier_14 (e9cd9321a6617423d09c86651868827b)         0.728990\n",
       "LGBMClassifier_104 (03f3c6e632f5fffd70ec8a4c916a9b3a)        0.728980\n",
       "LGBMClassifier_105 (b69a0d12e5e9a0dea6e97fd93d6e83ca)        0.728904\n",
       "LGBMClassifier_103 (0f77b5c92200f4372a73a16c28e66a0c)        0.728801\n",
       "LGBMClassifier_102 (15ce46227658fcf66d9fa61aa8770bd6)        0.728792\n",
       "LGBMClassifier_3 (12e279a3f79f03854fcc391dd7dbd023)          0.728647\n",
       "LGBMClassifier_201 (6fc3b3d774e94599d833c0f91c5a3bd9)        0.728564\n",
       "LGBMClassifier_2 (756b644eceb4b410ad34d055d7a2a7ad)          0.728547\n",
       "LGBMClassifier_200 (595825024cf6728eede6526c34137bea)        0.728528\n",
       "XGBClassifier_6 (dd58a49196a10b1dfc83dd004b5cae1b)           0.728361\n",
       "XGBClassifier_7 (159c6c407450077591a665dea6ecacfe)           0.728263\n",
       "LGBMClassifier_107 (209378e46f2c00fc2bab71cb2a4b2806)        0.728086\n",
       "LGBMClassifier_204 (508d4bd009535bc725e1412c3fccd89b)        0.727978\n",
       "LGBMClassifier_203 (d42c5caf1073a9a9e7440490645eea6f)        0.727955\n",
       "XGBClassifier_2 (f9455e9875a464c78cd90b60584d3f1c)           0.727895\n",
       "XGBClassifier_3 (4a02ebc078c643d903a00ddcfb0f3301)           0.727831\n",
       "XGBClassifier_101 (34a22ef42d0624dee9c89abc9971c23f)         0.727822\n",
       "XGBClassifier_102 (3a16502492397945fe3feec5f3ce94f2)         0.727786\n",
       "LGBMClassifier_205 (58b38247cf5e26b3ca8dcd12b838e5fd)        0.727755\n",
       "XGBClassifier_103 (ea8d1ee90caf97bacd865adb7e87423d)         0.727724\n",
       "LGBMClassifier_202 (be93bc9c94b117d579fef02264e6d81f)        0.727705\n",
       "LGBMClassifier_101 (4184d68f723388204df6096a194619e3)        0.727692\n",
       "XGBClassifier_9 (f04e3fec7fdc9e5bdcfce6e8ccafda3d)           0.727613\n",
       "LGBMClassifier_100 (e9308d5ef0fe3d2e21b23b76c24abf10)        0.727588\n",
       "XGBClassifier_10 (3eba9b3facf87fe1c9ba57c1d875c799)          0.727534\n",
       "XGBClassifier_11 (a13eb68990690e9fc4bef373e814ae80)          0.727481\n",
       "XGBClassifier_106 (99eada29cafaf2aedd156545890e7235)         0.727345\n",
       "XGBClassifier_107 (d3abb2fc7bb37f99aed2d658a0f5bee3)         0.727332\n",
       "CatBoostClassifier_203 (2b17c55ab32780d9a9f1a913c64ad718)    0.727320\n",
       "CatBoostClassifier_105 (faa5d67c16c906b11264ade5016c0635)    0.727305\n",
       "CatBoostClassifier_202 (44630aa57085636ff4b42ef84843dad4)    0.727295\n",
       "XGBClassifier_8 (020a786ac38ddae2dcd3a381a705107e)           0.727283\n",
       "CatBoostClassifier_104 (9db0b212db19e590b51a1af7fe65b7a0)    0.727244\n",
       "CatBoostClassifier_101 (939b20ef655c1c22f3cc5749a9dc0de7)    0.727232\n",
       "XGBClassifier_4 (41fcdf694bf252b540f6c6d28d63eb45)           0.727212\n",
       "CatBoostClassifier_200 (193b4e2a59c2eac5f73a997fb968592b)    0.727082\n",
       "XGBClassifier_5 (04de438dafcf1a1b67a0dbfa0e2bc0d1)           0.727053\n",
       "CatBoostClassifier_103 (985c7cd22a70cbfa293f37e23c80da7c)    0.727049\n",
       "CatBoostClassifier_201 (0aabd4de7e66e705cd06f7a5af29de6d)    0.727028\n",
       "XGBClassifier_203 (b8dc34df04edf0b3ea42293d5436bea7)         0.727010\n",
       "CatBoostClassifier_100 (8438003294d3bfe9b6020e7cbcc8f250)    0.727004\n",
       "XGBClassifier_202 (29c6bbceec36b1810be94900531114e9)         0.726951\n",
       "XGBClassifier_100 (a0e17bbe4a8e31c8ed28c4ff78fd8568)         0.726944\n",
       "XGBClassifier_200 (0fd8d373da5ad5bf5148772e49bf2d3c)         0.726836\n",
       "XGBClassifier_204 (1e5bc330e406cb4a18b5c76cb9907cbb)         0.726790\n",
       "CatBoostClassifier_3 (a2e94c97067190eb52e25c8f0d0b1b81)      0.726661\n",
       "XGBClassifier_201 (4a18eb80c90bfe9108b6bedaecacec3a)         0.726539\n",
       "CatBoostClassifier_1 (5b3133609cb7932f4c272f1494d1f6b0)      0.726533\n",
       "CatBoostClassifier_2 (c5054ebf7585fcc187c40442e3cbbcff)      0.726503\n",
       "LGBMClassifier_8 (74451732f18860140bc6e208f17167ec)          0.725616\n",
       "dtype: float64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model_auc = pd.Series()\n",
    "for estimator in base_model_train_preds.columns:\n",
    "    base_model_auc[estimator] = roc_auc_score(train_data[target_col], base_model_train_preds[estimator])\n",
    "base_model_auc.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517466ba",
   "metadata": {
    "papermill": {
     "duration": 0.011111,
     "end_time": "2025-12-26T16:54:22.001636",
     "exception": false,
     "start_time": "2025-12-26T16:54:21.990525",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 10. Meta-Model\n",
    "\n",
    "## 10.1 Meta-Model Hyperparameter Tuning\n",
    "\n",
    "### 10.1.1 Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "896808b8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-26T16:54:22.024617Z",
     "iopub.status.busy": "2025-12-26T16:54:22.023970Z",
     "iopub.status.idle": "2025-12-26T16:54:22.028013Z",
     "shell.execute_reply": "2025-12-26T16:54:22.027408Z"
    },
    "papermill": {
     "duration": 0.016779,
     "end_time": "2025-12-26T16:54:22.029159",
     "exception": false,
     "start_time": "2025-12-26T16:54:22.012380",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# to skip hyperparameter tuning when it's not needed; set to `False` to do the tuning & selection\n",
    "SKIP_META_MODEL_HYPERPARAMETER_TUNING = True\n",
    "\n",
    "# maximum number of trials Optuna will conduct for the optimization\n",
    "META_MODEL_OPTUNA_STUDY_NUM_TRIALS = 100\n",
    "\n",
    "# number of splits to use for K-Fold Cross-Validation\n",
    "META_MODEL_KFOLD_NUM_SPLITS = 5\n",
    "\n",
    "# use different random seeds from ones used to train base models to avoid\n",
    "# potential leakage or alignment artifacts from original splits\n",
    "META_MODEL_RANDOM_SEEDS = [77, 99]\n",
    "\n",
    "# optuna study best parameters for meta model\n",
    "meta_model_optuna_study_best_params = {}\n",
    "\n",
    "# parameters selected for meta model\n",
    "meta_model_params = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "596be395",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-26T16:54:22.052013Z",
     "iopub.status.busy": "2025-12-26T16:54:22.051796Z",
     "iopub.status.idle": "2025-12-26T16:54:22.059503Z",
     "shell.execute_reply": "2025-12-26T16:54:22.058973Z"
    },
    "papermill": {
     "duration": 0.020267,
     "end_time": "2025-12-26T16:54:22.060528",
     "exception": false,
     "start_time": "2025-12-26T16:54:22.040261",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_meta_model_optuna_params(trial):\n",
    "    return {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 400),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.05),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 5),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 10, 25),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 50, 150),\n",
    "        'min_split_gain': trial.suggest_float('min_split_gain', 0.01, 0.5, log=True),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 0.95),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.1, 0.6),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0.1, 10.0, log=True),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 5.0, 50.0, log=True),\n",
    "    }\n",
    "\n",
    "def meta_model_optuna_study_objective(trial):\n",
    "    meta_model_params = get_meta_model_optuna_params(trial)\n",
    "\n",
    "    meta_oof_preds_accumulator = np.zeros(len(train_data))\n",
    "\n",
    "    for random_seed in META_MODEL_RANDOM_SEEDS:\n",
    "        meta_skf = StratifiedKFold(n_splits=META_MODEL_KFOLD_NUM_SPLITS, shuffle=True, random_state=random_seed)\n",
    "        meta_skf_splits = meta_skf.split(base_model_train_preds, train_data[target_col])\n",
    "        meta_skf_enumeration = enumerate(meta_skf_splits)\n",
    "    \n",
    "        seed_oof_preds = np.zeros(len(train_data))\n",
    "    \n",
    "        for fold, (train_indices, validation_indices) in meta_skf_enumeration:\n",
    "            X_train_fold = base_model_train_preds.iloc[train_indices]\n",
    "            y_train_fold = train_data[target_col].iloc[train_indices]\n",
    "            X_validation_fold = base_model_train_preds.iloc[validation_indices]\n",
    "            y_validation_fold = train_data[target_col].iloc[validation_indices]\n",
    "    \n",
    "            model = lgb.LGBMClassifier(\n",
    "                **meta_model_params,\n",
    "                objective='binary',\n",
    "                metric='auc',\n",
    "                verbose=-1,\n",
    "                n_jobs=-1\n",
    "            )\n",
    "            model.fit(\n",
    "                X_train_fold, y_train_fold,\n",
    "                eval_set=(X_validation_fold, y_validation_fold)\n",
    "            )\n",
    "    \n",
    "            y_validation_pred_proba = model.predict_proba(X_validation_fold)[:, 1]\n",
    "            seed_oof_preds[validation_indices] = y_validation_pred_proba\n",
    "    \n",
    "        meta_oof_preds_accumulator += seed_oof_preds\n",
    "    \n",
    "    final_meta_oof_preds = meta_oof_preds_accumulator / len(META_MODEL_RANDOM_SEEDS)\n",
    "\n",
    "    return roc_auc_score(train_data[target_col], final_meta_oof_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "47ffe85d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-26T16:54:22.083145Z",
     "iopub.status.busy": "2025-12-26T16:54:22.082704Z",
     "iopub.status.idle": "2025-12-26T16:54:22.087257Z",
     "shell.execute_reply": "2025-12-26T16:54:22.086665Z"
    },
    "papermill": {
     "duration": 0.016907,
     "end_time": "2025-12-26T16:54:22.088276",
     "exception": false,
     "start_time": "2025-12-26T16:54:22.071369",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped hyperparameter tuning for meta model\n"
     ]
    }
   ],
   "source": [
    "if SKIP_META_MODEL_HYPERPARAMETER_TUNING:\n",
    "    print(\"Skipped hyperparameter tuning for meta model\")\n",
    "else:\n",
    "    print(\"Started hyperparameter tuning for meta model\")\n",
    "    sampler = optuna.samplers.TPESampler(n_ei_candidates=48, multivariate=True)\n",
    "    study = optuna.create_study(sampler=sampler, direction=\"maximize\")\n",
    "    study.optimize(meta_model_optuna_study_objective, n_trials=META_MODEL_OPTUNA_STUDY_NUM_TRIALS)\n",
    "    \n",
    "    print(f\"# trials finished: {len(study.trials)}\")\n",
    "    trial = study.best_trial\n",
    "    meta_model_optuna_study_best_params = study.best_params\n",
    "    print(f\"Best trial AUC: {trial.value}\")\n",
    "    print(f\"Best trial params:\")\n",
    "    for param_key, param_value in meta_model_optuna_study_best_params.items():\n",
    "        print(f\"- {param_key}: {param_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4954940",
   "metadata": {
    "papermill": {
     "duration": 0.010697,
     "end_time": "2025-12-26T16:54:22.109695",
     "exception": false,
     "start_time": "2025-12-26T16:54:22.098998",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 10.1.2 Tuned Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "88c837f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-26T16:54:22.133706Z",
     "iopub.status.busy": "2025-12-26T16:54:22.133474Z",
     "iopub.status.idle": "2025-12-26T16:54:22.139627Z",
     "shell.execute_reply": "2025-12-26T16:54:22.139081Z"
    },
    "papermill": {
     "duration": 0.019811,
     "end_time": "2025-12-26T16:54:22.140659",
     "exception": false,
     "start_time": "2025-12-26T16:54:22.120848",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following tuned parameters will be used for the meta model:\n",
      "- n_estimators: 389\n",
      "- learning_rate: 0.044442769245993996\n",
      "- max_depth: 5\n",
      "- num_leaves: 25\n",
      "- min_child_samples: 78\n",
      "- min_split_gain: 0.12135233306859451\n",
      "- subsample: 0.6968230266995306\n",
      "- colsample_bytree: 0.259028638069591\n",
      "- reg_alpha: 0.8528152084291172\n",
      "- reg_lambda: 15.210283085091218\n"
     ]
    }
   ],
   "source": [
    "# default values (most found from previous tuning/selection\n",
    "META_MODEL_DEFAULT_N_ESTIMATORS = 389\n",
    "META_MODEL_DEFAULT_LEARNING_RATE = 0.044442769245993996\n",
    "META_MODEL_DEFAULT_MAX_DEPTH = 5\n",
    "META_MODEL_DEFAULT_NUM_LEAVES = 25\n",
    "META_MODEL_DEFAULT_MIN_CHILD_SAMPLES = 78\n",
    "META_MODEL_DEFAULT_MIN_SPLIT_GAIN = 0.12135233306859451\n",
    "META_MODEL_DEFAULT_SUBSAMPLE = 0.6968230266995306\n",
    "META_MODEL_DEFAULT_COLSAMPLE_BY_TREE = 0.259028638069591\n",
    "META_MODEL_DEFAULT_REG_ALPHA = 0.8528152084291172\n",
    "META_MODEL_DEFAULT_REG_LAMBDA = 15.210283085091218\n",
    "\n",
    "# meta model parameters\n",
    "meta_model_params['n_estimators'] = meta_model_optuna_study_best_params.get('n_estimators', META_MODEL_DEFAULT_N_ESTIMATORS)\n",
    "meta_model_params['learning_rate'] = meta_model_optuna_study_best_params.get('learning_rate', META_MODEL_DEFAULT_LEARNING_RATE)\n",
    "meta_model_params['max_depth'] = meta_model_optuna_study_best_params.get('max_depth', META_MODEL_DEFAULT_MAX_DEPTH)\n",
    "meta_model_params['num_leaves'] = meta_model_optuna_study_best_params.get('num_leaves', META_MODEL_DEFAULT_NUM_LEAVES)\n",
    "meta_model_params['min_child_samples'] = meta_model_optuna_study_best_params.get('min_child_samples', META_MODEL_DEFAULT_MIN_CHILD_SAMPLES)\n",
    "meta_model_params['min_split_gain'] = meta_model_optuna_study_best_params.get('min_split_gain', META_MODEL_DEFAULT_MIN_SPLIT_GAIN)\n",
    "meta_model_params['subsample'] = meta_model_optuna_study_best_params.get('subsample', META_MODEL_DEFAULT_SUBSAMPLE)\n",
    "meta_model_params['colsample_bytree'] = meta_model_optuna_study_best_params.get('colsample_bytree', META_MODEL_DEFAULT_COLSAMPLE_BY_TREE)\n",
    "meta_model_params['reg_alpha'] = meta_model_optuna_study_best_params.get('reg_alpha', META_MODEL_DEFAULT_REG_ALPHA)\n",
    "meta_model_params['reg_lambda'] = meta_model_optuna_study_best_params.get('reg_lambda', META_MODEL_DEFAULT_REG_LAMBDA)\n",
    "print(f\"The following tuned parameters will be used for the meta model:\")\n",
    "for param_key, param_value in meta_model_params.items():\n",
    "        print(f\"- {param_key}: {param_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3ed34a",
   "metadata": {
    "papermill": {
     "duration": 0.011409,
     "end_time": "2025-12-26T16:54:22.162963",
     "exception": false,
     "start_time": "2025-12-26T16:54:22.151554",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 10.2 Meta-Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "802474b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-26T16:54:22.186223Z",
     "iopub.status.busy": "2025-12-26T16:54:22.185998Z",
     "iopub.status.idle": "2025-12-26T16:56:51.187244Z",
     "shell.execute_reply": "2025-12-26T16:56:51.186440Z"
    },
    "papermill": {
     "duration": 149.014438,
     "end_time": "2025-12-26T16:56:51.188831",
     "exception": false,
     "start_time": "2025-12-26T16:54:22.174393",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "meta_oof_preds_accumulator = np.zeros(len(train_data))\n",
    "meta_test_preds_accumulator = np.zeros(len(test_data))\n",
    "meta_train_feature_importances_accumulator = np.zeros(len(base_model_train_preds.columns))\n",
    "\n",
    "for random_seed in META_MODEL_RANDOM_SEEDS:\n",
    "    meta_skf = StratifiedKFold(n_splits=META_MODEL_KFOLD_NUM_SPLITS, shuffle=True, random_state=random_seed)\n",
    "    meta_skf_splits = meta_skf.split(base_model_train_preds, train_data[target_col])\n",
    "    meta_skf_enumeration = enumerate(meta_skf_splits)\n",
    "\n",
    "    seed_oof_preds = np.zeros(len(train_data))\n",
    "\n",
    "    for fold, (train_indices, validation_indices) in meta_skf_enumeration:\n",
    "        X_train_fold = base_model_train_preds.iloc[train_indices]\n",
    "        y_train_fold = train_data[target_col].iloc[train_indices]\n",
    "        X_validation_fold = base_model_train_preds.iloc[validation_indices]\n",
    "        y_validation_fold = train_data[target_col].iloc[validation_indices]\n",
    "\n",
    "        meta_model = lgb.LGBMClassifier(\n",
    "            **meta_model_params,\n",
    "            objective='binary',\n",
    "            metric='auc',\n",
    "            verbose=-1,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        meta_model.fit(\n",
    "            X_train_fold, y_train_fold,\n",
    "            eval_set=(X_validation_fold, y_validation_fold)\n",
    "        )\n",
    "\n",
    "        y_validation_pred_proba = meta_model.predict_proba(X_validation_fold)[:, 1]\n",
    "        y_test_pred_proba = meta_model.predict_proba(base_model_test_preds)[:, 1]\n",
    "        seed_oof_preds[validation_indices] = np.array(y_validation_pred_proba)\n",
    "        meta_test_preds_accumulator += np.array(y_test_pred_proba)\n",
    "        meta_train_feature_importances_accumulator += np.array(meta_model.feature_importances_)\n",
    "\n",
    "    meta_oof_preds_accumulator += seed_oof_preds\n",
    "\n",
    "final_meta_oof_preds = meta_oof_preds_accumulator / len(META_MODEL_RANDOM_SEEDS)\n",
    "final_meta_test_preds = meta_test_preds_accumulator / (META_MODEL_KFOLD_NUM_SPLITS * len(META_MODEL_RANDOM_SEEDS))\n",
    "meta_train_feature_importances = meta_train_feature_importances_accumulator / (META_MODEL_KFOLD_NUM_SPLITS * len(META_MODEL_RANDOM_SEEDS))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bc3dea",
   "metadata": {
    "papermill": {
     "duration": 0.011021,
     "end_time": "2025-12-26T16:56:51.212746",
     "exception": false,
     "start_time": "2025-12-26T16:56:51.201725",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 10.3 Meta-Model Feature Importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f8c9ad2d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-26T16:56:51.235222Z",
     "iopub.status.busy": "2025-12-26T16:56:51.234981Z",
     "iopub.status.idle": "2025-12-26T16:56:51.242141Z",
     "shell.execute_reply": "2025-12-26T16:56:51.241521Z"
    },
    "papermill": {
     "duration": 0.019797,
     "end_time": "2025-12-26T16:56:51.243174",
     "exception": false,
     "start_time": "2025-12-26T16:56:51.223377",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier_7 (159c6c407450077591a665dea6ecacfe)           279.0\n",
       "LGBMClassifier_11 (434b9911e9f067fa638a7372ae9de3b7)         254.8\n",
       "LGBMClassifier_10 (d768bcfaea71d6315f8a3aec15264fd3)         248.3\n",
       "XGBClassifier_102 (3a16502492397945fe3feec5f3ce94f2)         225.3\n",
       "XGBClassifier_201 (4a18eb80c90bfe9108b6bedaecacec3a)         223.3\n",
       "LGBMClassifier_8 (74451732f18860140bc6e208f17167ec)          219.3\n",
       "XGBClassifier_6 (dd58a49196a10b1dfc83dd004b5cae1b)           215.9\n",
       "LGBMClassifier_6 (a0c02e5dec4bd4467961cd96901740a9)          212.4\n",
       "LGBMClassifier_9 (535b190c7fd27867f25b36c846af5749)          203.3\n",
       "XGBClassifier_204 (1e5bc330e406cb4a18b5c76cb9907cbb)         201.6\n",
       "XGBClassifier_2 (f9455e9875a464c78cd90b60584d3f1c)           196.6\n",
       "XGBClassifier_101 (34a22ef42d0624dee9c89abc9971c23f)         189.8\n",
       "LGBMClassifier_104 (03f3c6e632f5fffd70ec8a4c916a9b3a)        179.4\n",
       "CatBoostClassifier_1 (5b3133609cb7932f4c272f1494d1f6b0)      179.4\n",
       "LGBMClassifier_105 (b69a0d12e5e9a0dea6e97fd93d6e83ca)        179.2\n",
       "LGBMClassifier_7 (0d5c160d0c75a01607ee65e1309f0f3e)          177.7\n",
       "XGBClassifier_3 (4a02ebc078c643d903a00ddcfb0f3301)           174.1\n",
       "XGBClassifier_103 (ea8d1ee90caf97bacd865adb7e87423d)         172.7\n",
       "XGBClassifier_200 (0fd8d373da5ad5bf5148772e49bf2d3c)         171.6\n",
       "XGBClassifier_8 (020a786ac38ddae2dcd3a381a705107e)           170.1\n",
       "XGBClassifier_106 (99eada29cafaf2aedd156545890e7235)         160.5\n",
       "LGBMClassifier_13 (57b6180bb789e56517729d29e1113e9f)         159.5\n",
       "CatBoostClassifier_2 (c5054ebf7585fcc187c40442e3cbbcff)      157.8\n",
       "XGBClassifier_107 (d3abb2fc7bb37f99aed2d658a0f5bee3)         154.2\n",
       "LGBMClassifier_202 (be93bc9c94b117d579fef02264e6d81f)        153.7\n",
       "LGBMClassifier_2 (756b644eceb4b410ad34d055d7a2a7ad)          153.3\n",
       "XGBClassifier_5 (04de438dafcf1a1b67a0dbfa0e2bc0d1)           150.8\n",
       "LGBMClassifier_100 (e9308d5ef0fe3d2e21b23b76c24abf10)        150.6\n",
       "XGBClassifier_10 (3eba9b3facf87fe1c9ba57c1d875c799)          149.0\n",
       "LGBMClassifier_14 (e9cd9321a6617423d09c86651868827b)         148.7\n",
       "CatBoostClassifier_202 (44630aa57085636ff4b42ef84843dad4)    143.3\n",
       "XGBClassifier_100 (a0e17bbe4a8e31c8ed28c4ff78fd8568)         142.7\n",
       "LGBMClassifier_3 (12e279a3f79f03854fcc391dd7dbd023)          139.8\n",
       "XGBClassifier_9 (f04e3fec7fdc9e5bdcfce6e8ccafda3d)           136.4\n",
       "CatBoostClassifier_3 (a2e94c97067190eb52e25c8f0d0b1b81)      136.3\n",
       "XGBClassifier_11 (a13eb68990690e9fc4bef373e814ae80)          136.0\n",
       "XGBClassifier_202 (29c6bbceec36b1810be94900531114e9)         134.4\n",
       "LGBMClassifier_101 (4184d68f723388204df6096a194619e3)        134.0\n",
       "CatBoostClassifier_201 (0aabd4de7e66e705cd06f7a5af29de6d)    133.0\n",
       "LGBMClassifier_203 (d42c5caf1073a9a9e7440490645eea6f)        131.3\n",
       "LGBMClassifier_103 (0f77b5c92200f4372a73a16c28e66a0c)        128.7\n",
       "CatBoostClassifier_203 (2b17c55ab32780d9a9f1a913c64ad718)    126.8\n",
       "CatBoostClassifier_105 (faa5d67c16c906b11264ade5016c0635)    125.2\n",
       "CatBoostClassifier_103 (985c7cd22a70cbfa293f37e23c80da7c)    125.2\n",
       "XGBClassifier_203 (b8dc34df04edf0b3ea42293d5436bea7)         123.1\n",
       "XGBClassifier_4 (41fcdf694bf252b540f6c6d28d63eb45)           122.5\n",
       "CatBoostClassifier_200 (193b4e2a59c2eac5f73a997fb968592b)    122.5\n",
       "CatBoostClassifier_101 (939b20ef655c1c22f3cc5749a9dc0de7)    119.9\n",
       "LGBMClassifier_201 (6fc3b3d774e94599d833c0f91c5a3bd9)        118.8\n",
       "LGBMClassifier_107 (209378e46f2c00fc2bab71cb2a4b2806)        117.9\n",
       "LGBMClassifier_200 (595825024cf6728eede6526c34137bea)        114.5\n",
       "LGBMClassifier_102 (15ce46227658fcf66d9fa61aa8770bd6)        112.6\n",
       "CatBoostClassifier_100 (8438003294d3bfe9b6020e7cbcc8f250)    107.9\n",
       "LGBMClassifier_205 (58b38247cf5e26b3ca8dcd12b838e5fd)        100.7\n",
       "LGBMClassifier_204 (508d4bd009535bc725e1412c3fccd89b)         99.8\n",
       "CatBoostClassifier_104 (9db0b212db19e590b51a1af7fe65b7a0)     96.1\n",
       "dtype: float64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_model_feature_importances = pd.Series(meta_train_feature_importances)\n",
    "meta_model_feature_importances.index = base_model_train_preds.columns\n",
    "meta_model_feature_importances.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954b7879",
   "metadata": {
    "papermill": {
     "duration": 0.010872,
     "end_time": "2025-12-26T16:56:51.264974",
     "exception": false,
     "start_time": "2025-12-26T16:56:51.254102",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 10.4 Final Adjustments to Test Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bde2b13d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-26T16:56:51.289238Z",
     "iopub.status.busy": "2025-12-26T16:56:51.289019Z",
     "iopub.status.idle": "2025-12-26T16:56:51.300383Z",
     "shell.execute_reply": "2025-12-26T16:56:51.299816Z"
    },
    "papermill": {
     "duration": 0.024573,
     "end_time": "2025-12-26T16:56:51.301384",
     "exception": false,
     "start_time": "2025-12-26T16:56:51.276811",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def min_max_scale(preds):\n",
    "    min_val = preds.min()\n",
    "    max_val = preds.max()\n",
    "    if max_val > min_val:\n",
    "        return (preds - min_val) / (max_val - min_val)\n",
    "    return preds\n",
    "\n",
    "# scale final meta oof/test preds\n",
    "scaled_final_meta_oof_preds = min_max_scale(final_meta_oof_preds)\n",
    "scaled_final_meta_test_preds = min_max_scale(final_meta_test_preds)\n",
    "\n",
    "# just in case floating point math leaves values very slightly below 0 or above 1\n",
    "scaled_final_meta_oof_preds = np.clip(scaled_final_meta_oof_preds, 0, 1)\n",
    "scaled_final_meta_test_preds = np.clip(scaled_final_meta_test_preds, 0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16cfead0",
   "metadata": {
    "papermill": {
     "duration": 0.011078,
     "end_time": "2025-12-26T16:56:51.323568",
     "exception": false,
     "start_time": "2025-12-26T16:56:51.312490",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 10.5 Meta-Model AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "62240e08",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-26T16:56:51.346703Z",
     "iopub.status.busy": "2025-12-26T16:56:51.346484Z",
     "iopub.status.idle": "2025-12-26T16:56:51.521062Z",
     "shell.execute_reply": "2025-12-26T16:56:51.520355Z"
    },
    "papermill": {
     "duration": 0.187849,
     "end_time": "2025-12-26T16:56:51.522274",
     "exception": false,
     "start_time": "2025-12-26T16:56:51.334425",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7300528965188284\n"
     ]
    }
   ],
   "source": [
    "meta_model_auc = roc_auc_score(train_data[target_col], scaled_final_meta_oof_preds)\n",
    "print(meta_model_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f09675",
   "metadata": {
    "papermill": {
     "duration": 0.011259,
     "end_time": "2025-12-26T16:56:51.544920",
     "exception": false,
     "start_time": "2025-12-26T16:56:51.533661",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 11. Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "de0a0fab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-26T16:56:51.568544Z",
     "iopub.status.busy": "2025-12-26T16:56:51.568293Z",
     "iopub.status.idle": "2025-12-26T16:56:52.124209Z",
     "shell.execute_reply": "2025-12-26T16:56:52.123360Z"
    },
    "papermill": {
     "duration": 0.569316,
     "end_time": "2025-12-26T16:56:52.125397",
     "exception": false,
     "start_time": "2025-12-26T16:56:51.556081",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission file prepared.\n"
     ]
    }
   ],
   "source": [
    "# prepare submission\n",
    "submission = pd.DataFrame({'id': test_data.index, target_col: scaled_final_meta_test_preds})\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print('Submission file prepared.')"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 14272474,
     "sourceId": 91723,
     "sourceType": "competition"
    },
    {
     "datasetId": 8925440,
     "sourceId": 14298931,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31192,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1521.758366,
   "end_time": "2025-12-26T16:56:54.606167",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-12-26T16:31:32.847801",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
